{
"chunks": [
"Billion-scale similarity search with GPUs\nJeff Johnson\nFacebook AI Research\nNew Y orkMatthijs Douze\nFacebook AI Research\nParisHerv ´e J´egou\nFacebook AI Research\nParis",
"Similarity search ﬁnds application in specialized database\nsystems handling complex data such as images or videos,\nwhich are typically represented by high-dimensional features\nand require speciﬁc indexing structures. This paper tackles\nthe problem of better utilizing GPUs for this task. While\nGPUs excel at data-parallel tasks, prior approaches are bot-\ntlenecked by algorithms that expose less parallelism, such as\nk-min selection, or make poor use of the memory hierarchy. We propose a design for k-selection that operates at up\nto 55% of theoretical peak performance, enabling a nearest\nneighbor implementation that is 8.5 ×faster than prior GPU\nstate of the art. We apply it in diﬀerent similarity search\nscenarios, by proposing optimized design for brute-force, ap-\nproximate and compressed-domain search based on product\nquantization. In all these setups, we outperform the state of\nthe art by large margins. Our implementation enables the\nconstruction of a high accuracy k-NN graph on 95 million\nimages from the Yfcc100M dataset in 35 minutes, and of\na graph connecting 1 billion vectors in less than 12 hours\non 4 Maxwell Titan X GPUs. We have open-sourced our\napproach1for the sake of comparison and reproducibility.",
"Images and videos constitute a new massive source of data\nfor indexing and search. Extensive metadata for this con-\ntent is often not available. Search and interpretation of this\nand other human-generated content, like text, is diﬃcult and\nimportant. A variety of machine learning and deep learn-\ning algorithms are being used to interpret and classify these\ncomplex, real-world entities. Popular examples include the\ntext representation known as word2vec [32], representations\nof images by convolutional neural networks [39, 19], and im-\nage descriptors for instance search [20]. Such representations\norembeddings are usually real-valued, high-dimensional vec-\ntors of 50 to 1000+ dimensions. Many of these vector repre-\nsentations can only eﬀectively be produced on GPU systems,\n1https://github.com/facebookresearch/faissas the underlying processes either have high arithmetic com-\nplexity and/or high data bandwidth demands [28], or cannot\nbe eﬀectively partitioned without failing due to communi-\ncation overhead or representation quality [38]. Once pro-\nduced, their manipulation is itself arithmetically intensive. However, how to utilize GPU assets is not straightforward. More generally, how to exploit new heterogeneous architec-\ntures is a key subject for the database community [9]. In this context, searching by numerical similarity rather\nthan via structured relations is more suitable. This could be\nto ﬁnd the most similar content to a picture, or to ﬁnd the\nvectors that have the highest response to a linear classiﬁer\non all vectors of a collection. One of the most expensive operations to be performed on\nlarge collections is to compute a k-NN graph. It is a directed\ngraph where each vector of the database is a node and each\nedge connects a node to its knearest neighbors. This is\nour ﬂagship application. Note, state of the art methods like\nNN-Descent [15] have a large memory overhead on top of\nthe dataset itself and cannot readily scale to the billion-sized\ndatabases we consider.",
"This is\nour ﬂagship application. Note, state of the art methods like\nNN-Descent [15] have a large memory overhead on top of\nthe dataset itself and cannot readily scale to the billion-sized\ndatabases we consider. Such applications must deal with the curse of dimension-\nality [46], rendering both exhaustive search or exact index-\ning for non-exhaustive search impractical on billion-scale\ndatabases. This is why there is a large body of work on\napproximate search and/or graph construction. To handle\nhuge datasets that do not ﬁt in RAM, several approaches\nemploy an internal compressed representation of the vec-\ntors using an encoding. This is especially convenient for\nmemory-limited devices like GPUs. It turns out that accept-\ning a minimal accuracy loss results in orders of magnitude\nof compression [21]. The most popular vector compression\nmethods can be classiﬁed into either binary codes [18, 22],\nor quantization methods [25, 37]. Both have the desirable\nproperty that searching neighbors does not require recon-\nstructing the vectors. Our paper focuses on methods based on product quanti-\nzation (PQ) codes, as these were shown to be more eﬀective\nthan binary codes [34]. In addition, binary codes incur im-\nportant overheads for non-exhaustive search methods [35]. Several improvements were proposed after the original prod-\nuct quantization proposal known as IVFADC [25]; most are\ndiﬃcult to implement eﬃciently on GPU. For instance, the\ninverted multi-index [4], useful for high-speed/low-quality\noperating points, depends on a complicated “multi-sequence”\nalgorithm. The optimized product quantization or OPQ [17]\nis a linear transformation on the input vectors that improves\nthe accuracy of the product quantization; it can be applied\n1arXiv:1702.08734v1  [cs.CV]  28 Feb 2017",
"as a pre-processing. The SIMD-optimized IVFADC imple-\nmentation from [2] operates only with sub-optimal parame-\nters (few coarse quantization centroids). Many other meth-\nods, like LOPQ and the Polysemous codes [27, 16] are too\ncomplex to be implemented eﬃciently on GPUs. There are many implementations of similarity search on\nGPUs, but mostly with binary codes [36], small datasets [44],\nor exhaustive search [14, 40, 41]. To the best of our knowl-\nedge, only the work by Wieschollek et al. [47] appears suit-\nable for billion-scale datasets with quantization codes. This\nis the prior state of the art on GPUs, which we compare\nagainst in Section 6.4. This paper makes the following contributions:\n•a GPUk-selection algorithm, operating in fast register\nmemory and ﬂexible enough to be fusable with other\nkernels, for which we provide a complexity analysis;\n•a near-optimal algorithmic layout for exact and ap-\nproximatek-nearest neighbor search on GPU;\n•a range of experiments that show that these improve-\nments outperform previous art by a large margin on\nmid- to large-scale nearest-neighbor search tasks, in\nsingle or multi-GPU conﬁgurations. The paper is organized as follows. Section 2 introduces\nthe context and notation. Section 3 reviews GPU archi-\ntecture and discusses problems appearing when using it for\nsimilarity search. Section 4 introduces one of our main con-\ntributions, i.e., our k-selection method for GPUs, while Sec-\ntion 5 provides details regarding the algorithm computation\nlayout. Finally, Section 6 provides extensive experiments for\nour approach, compares it to the state of the art, and shows\nconcrete use cases for image collections.",
"We are concerned with similarity search in vector collec-\ntions. Given the query vector x∈Rdand the collection2\n[yi]i=0:ℓ(yi∈Rd), we search:\nL=k-argmini=0:ℓ∥x−yi∥2, (1)\ni.e., we search the knearest neighbors of xin terms of L2\ndistance. The L2 distance is used most often, as it is op-\ntimized by design when learning several embeddings ( e.g.,\n[20]), due to its attractive linear algebra properties. The lowest distances are collected by k-selection. For an\narray [ai]i=0:ℓ,k-selection ﬁnds the klowest valued elements\n[asi]i=0:k,asi≤asi+1, along with the indices [ si]i=0:k, 0≤\nsi<ℓ, of those elements from the input array. The aiwill be\n32-bit ﬂoating point values; the siare 32- or 64-bit integers. Other comparators are sometimes desired; e.g., for cosine\nsimilarity we search for highest values. The order between\nequivalent keys asi=asjis not speciﬁed. Batching. Typically, searches are performed in batches\nofnqquery vectors [ xj]j=0:nq(xj∈Rd) in parallel, which\nallows for more ﬂexibility when executing on multiple CPU\nthreads or on GPU. Batching for k-selection entails selecting\nnq×kelements and indices from nqseparate arrays, where\neach array is of a potentially diﬀerent length ℓi≥k. 2To avoid clutter in 0-based indexing, we use the array no-\ntation 0 :ℓto denote the range {0,...,ℓ−1}inclusive.Exact search. The exact solution computes the full pair-\nwise distance matrix D= [∥xj−yi∥2\n2]j=0:nq,i=0:ℓ∈Rnq×ℓ. In practice, we use the decomposition\n∥xj−yi∥2\n2=∥xj∥2+∥yi∥2−2⟨xj,yi⟩. (2)\nThe two ﬁrst terms can be precomputed in one pass over\nthe matrices XandYwhose rows are the [ xj] and [yi]. The\nbottleneck is to evaluate ⟨xj,yi⟩, equivalent to the matrix\nmultiplication XY⊤. Thek-nearest neighbors for each of\nthenqqueries are k-selected along each row of D. Compressed-domain search. From now on, we focus on\napproximate nearest-neighbor search. We consider, in par-\nticular, the IVFADC indexing structure [25]. The IVFADC\nindex relies on two levels of quantization, and the database\nvectors are encoded.",
"Compressed-domain search. From now on, we focus on\napproximate nearest-neighbor search. We consider, in par-\nticular, the IVFADC indexing structure [25]. The IVFADC\nindex relies on two levels of quantization, and the database\nvectors are encoded. The database vector yis approximated\nas:\ny≈q(y) =q1(y) +q2(y−q1(y)) (3)\nwhereq1:Rd→C 1⊂Rdandq2:Rd→C 2⊂Rdare quan-\ntizers; i.e., functions that output an element from a ﬁnite\nset. Since the sets are ﬁnite, q(y) is encoded as the index of\nq1(y) and that of q2(y−q1(y)). The ﬁrst-level quantizer is a\ncoarse quantizer and the second level ﬁne quantizer encodes\nthe residual vector after the ﬁrst level. The Asymmetric Distance Computation (ADC) search\nmethod returns an approximate result:\nLADC=k-argmini=0:ℓ∥x−q(yi)∥2. (4)\nFor IVFADC the search is not exhaustive. Vectors for\nwhich the distance is computed are pre-selected depending\non the ﬁrst-level quantizer q1:\nLIVF=τ-argminc∈C1∥x−c∥2. (5)\nThe multi-probe parameter τis the number of coarse-level\ncentroids we consider. The quantizer operates a nearest-\nneighbor search with exact distances, in the set of reproduc-\ntion values. Then, the IVFADC search computes\nLIVFADC =k-argmin\ni=0:ℓs.t.q1(yi)∈LIVF∥x−q(yi)∥2. (6)\nHence, IVFADC relies on the same distance estimations as\nthe two-step quantization of ADC, but computes them only\non a subset of vectors. The corresponding data structure, the inverted ﬁle , groups\nthe vectors yiinto|C1|inverted listsI1,...,I|C1|with homo-\ngeneousq1(yi). Therefore, the most memory-intensive op-\neration is computing LIVFADC , and boils down to linearly\nscanningτinverted lists. The quantizers. The quantizers q1andq2have diﬀerent\nproperties. q1needs to have a relatively low number of repro-\nduction values so that the number of inverted lists does not\nexplode. We typically use |C1|≈√\nℓ, trained via k-means. Forq2, we can aﬀord to spend more memory for a more ex-\ntensive representation. The ID of the vector (a 4- or 8-byte\ninteger) is also stored in the inverted lists, so it makes no\nsense to have shorter codes than that; i.e., log2|C2|>4×8. Product quantizer.",
"Forq2, we can aﬀord to spend more memory for a more ex-\ntensive representation. The ID of the vector (a 4- or 8-byte\ninteger) is also stored in the inverted lists, so it makes no\nsense to have shorter codes than that; i.e., log2|C2|>4×8. Product quantizer. We use a product quantizer [25] forq2,\nwhich provides a large number of reproduction values with-\nout increasing the processing cost. It interprets the vector y\nasbsub-vectors y= [y0...yb−1], wherebis an even divisor of\n2",
"the dimension d. Each sub-vector is quantized with its own\nquantizer, yielding the tuple ( q0(y0),...,qb−1(yb−1)). The\nsub-quantizers typically have 256 reproduction values, to ﬁt\nin one byte. The quantization value of the product quantizer\nis thenq2(y) =q0(y0) + 256×q1(y1) +...+ 256b−1×qb−1,\nwhich from a storage point of view is just the concatena-\ntion of the bytes produced by each sub-quantizer. Thus, the\nproduct quantizer generates b-byte codes with |C2|= 256b\nreproduction values. The k-means dictionaries of the quan-\ntizers are small and quantization is computationally cheap.",
"This section reviews salient details of Nvidia’s general-\npurpose GPU architecture and programming model [30]. We\nthen focus on one of the less GPU-compliant parts involved\nin similarity search, namely the k-selection, and discuss the\nliterature and challenges. 3.1 Architecture\nGPU lanes and warps. The Nvidia GPU is a general-\npurpose computer that executes instruction streams using\na 32-wide vector of CUDA threads (the warp ); individual\nthreads in the warp are referred to as lanes , with a lane\nIDfrom 0 – 31. Despite the “thread” terminology, the best\nanalogy to modern vectorized multicore CPUs is that each\nwarp is a separate CPU hardware thread, as the warp shares\nan instruction counter. Warp lanes taking diﬀerent execu-\ntion paths results in warp divergence , reducing performance. Each lane has up to 255 32-bit registers in a shared register\nﬁle. The CPU analogy is that there are up to 255 vector\nregisters of width 32, with warp lanes as SIMD vector lanes. Collections of warps. A user-conﬁgurable collection of 1\nto 32 warps comprises a block or a co-operative thread ar-\nray(CTA). Each block has a high speed shared memory , up\nto 48 KiB in size. Individual CUDA threads have a block-\nrelative ID, called a thread id , which can be used to parti-\ntion and assign work. Each block is run on a single core of\nthe GPU called a streaming multiprocessor (SM). Each SM\nhas functional units , including ALUs, memory load/store\nunits, and various special instruction units. A GPU hides\nexecution latencies by having many operations in ﬂight on\nwarps across all SMs. Each individual warp lane instruction\nthroughput is low and latency is high, but the aggregate\narithmetic throughput of all SMs together is 5 – 10 ×higher\nthan typical CPUs. Grids and kernels. Blocks are organized in a gridof blocks\nin a kernel . Each block is assigned a grid relative ID. The\nkernel is the unit of work (instruction stream with argu-\nments) scheduled by the host CPU for the GPU to execute. After a block runs through to completion, new blocks can\nbe scheduled.",
"Each block is assigned a grid relative ID. The\nkernel is the unit of work (instruction stream with argu-\nments) scheduled by the host CPU for the GPU to execute. After a block runs through to completion, new blocks can\nbe scheduled. Blocks from diﬀerent kernels can run concur-\nrently. Ordering between kernels is controllable via ordering\nprimitives such as streams and events . Resources and occupancy. The number of blocks execut-\ning concurrently depends upon shared memory and register\nresources used by each block. Per-CUDA thread register us-\nage is determined at compilation time, while shared memory\nusage can be chosen at runtime. This usage aﬀects occu-\npancy on the GPU. If a block demands all 48 KiB of shared\nmemory for its private usage, or 128 registers per thread asopposed to 32, then only 1 – 2 other blocks can run concur-\nrently on the same SM, resulting in low occupancy. Under\nhigh occupancy more blocks will be present across all SMs,\nallowing more work to be in ﬂight at once. Memory types. Diﬀerent blocks and kernels communicate\nthrough global memory , typically 4 – 32 GB in size, with 5 –\n10×higher bandwidth than CPU main memory. Shared\nmemory is analogous to CPU L1 cache in terms of speed. GPU register ﬁle memory is the highest bandwidth memory. In order to maintain the high number of instructions in ﬂight\non a GPU, a vast register ﬁle is also required: 14 MB in the\nlatest Pascal P100, in contrast with a few tens of KB on\nCPU. A ratio of 250 : 6.25 : 1 for register to shared to global\nmemory aggregate cross-sectional bandwidth is typical on\nGPU, yielding 10 – 100s of TB/s for the register ﬁle [10]. 3.2 GPU register ﬁle usage\nStructured register data. Shared and register memory\nusage involves eﬃciency tradeoﬀs; they lower occupancy but\ncan increase overall performance by retaining a larger work-\ning set in a faster memory. Making heavy use of register-\nresident data at the expense of occupancy or instead of\nshared memory is often proﬁtable [43]. As the GPU register ﬁle is very large, storing structured\ndata (not just temporary operands) is useful.",
"Making heavy use of register-\nresident data at the expense of occupancy or instead of\nshared memory is often proﬁtable [43]. As the GPU register ﬁle is very large, storing structured\ndata (not just temporary operands) is useful. A single lane\ncan use its (scalar) registers to solve a local task, but with\nlimited parallelism and storage. Instead, lanes in a GPU\nwarp can instead exchange register data using the warp shuf-\nﬂeinstruction, enabling warp-wide parallelism and storage. Lane-stride register array. A common pattern to achieve\nthis is a lane-stride register array . That is, given elements\n[ai]i=0:ℓ, each successive value is held in a register by neigh-\nboring lanes. The array is stored in ℓ/32 registers per lane,\nwithℓa multiple of 32. Lane jstores{aj,a32+j,...,a ℓ−32+j},\nwhile register rholds{a32r,a32r+1,...,a 32r+31}. For manipulating the [ ai], the register in which aiis stored\n(i.e.,⌊i/32⌋) andℓmust be known at assembly time, while\nthe lane ( i.e.,imod 32) can be runtime knowledge. A wide\nvariety of access patterns (shift, any-to-any) are provided;\nwe use the butterﬂy permutation [29] extensively. 3.3 k-selection on CPU versus GPU\nk-selection algorithms, often for arbitrarily large ℓand\nk, can be translated to a GPU, including radix selection\nand bucket selection [1],probabilistic selection [33], quick-\nselect [14], and truncated sorts [40]. Their performance is\ndominated by multiple passes over the input in global mem-\nory. Sometimes for similarity search, the input distances are\ncomputed on-the-ﬂy or stored only in small blocks, not in\ntheir entirety. The full, explicit array might be too large to\nﬁt into any memory, and its size could be unknown at the\nstart of the processing, rendering algorithms that require\nmultiple passes impractical. They suﬀer from other issues\nas well. Quickselect requires partitioning on a storage of\nsizeO(ℓ), a data-dependent memory movement. This can\nresult in excessive memory transactions, or requiring parallel\npreﬁx sums to determine write oﬀsets, with synchronization\noverhead.",
"Quickselect requires partitioning on a storage of\nsizeO(ℓ), a data-dependent memory movement. This can\nresult in excessive memory transactions, or requiring parallel\npreﬁx sums to determine write oﬀsets, with synchronization\noverhead. Radix selection has no partitioning but multiple\npasses are still required. Heap parallelism. In similarity search applications, one\nis usually interested only in a small number of results, k <\n3",
"1000 or so. In this regime, selection via max-heap is a typi-\ncal choice on the CPU, but heaps do not expose much data\nparallelism (due to serial tree update) and cannot saturate\nSIMD execution units. The ad-heap [31] takes better advan-\ntage of parallelism available in heterogeneous systems, but\nstill attempts to partition serial and parallel work between\nappropriate execution units. Despite the serial nature of\nheap update, for small kthe CPU can maintain all of its\nstate in the L1 cache with little eﬀort, and L1 cache latency\nand bandwidth remains a limiting factor. Other similarity\nsearch components, like PQ code manipulation, tend to have\ngreater impact on CPU performance [2]. GPU heaps. Heaps can be similarly implemented on a\nGPU [7]. However, a straightforward GPU heap implemen-\ntation suﬀers from high warp divergence and irregular, data-\ndependent memory movement, since the path taken for each\ninserted element depends upon other values in the heap. GPU parallel priority queues [24] improve over the serial\nheap update by allowing multiple concurrent updates, but\nthey require a potential number of small sorts for each insert\nand data-dependent memory movement. Moreover, it uses\nmultiple synchronization barriers through kernel launches in\ndiﬀerent streams, plus the additional latency of successive\nkernel launches and coordination with the CPU host. Other more novel GPU algorithms are available for small\nk, namely the selection algorithm in the fgknn library [41]. This is a complex algorithm that may suﬀer from too many\nsynchronization points, greater kernel launch overhead, us-\nage of slower memories, excessive use of hierarchy, partition-\ning and buﬀering. However, we take inspiration from this\nparticular algorithm through the use of parallel merges as\nseen in their merge queue structure.",
"For any CPU or GPU algorithm, either memory or arith-\nmetic throughput should be the limiting factor as per the\nrooﬂine performance model [48]. For input from global mem-\nory,k-selection cannot run faster than the time required to\nscan the input once at peak memory bandwidth. We aim to\nget as close to this limit as possible. Thus, we wish to per-\nform a single pass over the input data (from global memory\nor produced on-the-ﬂy, perhaps fused with a kernel that is\ngenerating the data). We want to keep intermediate state in the fastest memory:\nthe register ﬁle. The major disadvantage of register memory\nis that the indexing into the register ﬁle must be known at\nassembly time, which is a strong constraint on the algorithm. 4.1 In-register sorting\nWe use an in-register sorting primitive as a building block. Sorting networks are commonly used on SIMD architec-\ntures [13], as they exploit vector parallelism. They are eas-\nily implemented on the GPU, and we build sorting networks\nwith lane-stride register arrays. We use a variant of Batcher’s bitonic sorting network [8],\nwhich is a set of parallel merges on an array of size 2k. Each\nmerge takes sarrays of length t(sandta power of 2) to s/2\narrays of length 2 t, using log2(t) parallel steps.",
"We use a variant of Batcher’s bitonic sorting network [8],\nwhich is a set of parallel merges on an array of size 2k. Each\nmerge takes sarrays of length t(sandta power of 2) to s/2\narrays of length 2 t, using log2(t) parallel steps. A bitonic\nsort applies this merge recursively: to sort an array of length\nℓ, mergeℓarrays of length 1 to ℓ/2 arrays of length 2, to ℓ/4\narrays of length 4, successively to 1 sorted array of length ℓ,\nleading to1\n2(log2(ℓ)2+ log2(ℓ)) parallel merge steps.Algorithm 1 Odd-size merging network\nfunction merge-odd ([Li]i=0:ℓL,[Ri]i=0:ℓR)\nparallel for i←0 : min(ℓL,ℓR)do\n⊿inverted 1st stage; inputs are already sorted\ncompare-swap (LℓL−i−1,Ri)\nend for\nparallel do\n⊿IfℓL=ℓRand a power-of-2, these are equivalent\nmerge-odd-continue ([Li]i=0:ℓL,left)\nmerge-odd-continue ([Ri]i=0:ℓR,right )\nend do\nend function\nfunction merge-odd-continue ([xi]i=0:ℓ,p)\nifℓ>1then\nh←2⌈log2ℓ⌉−1⊿largest power-of-2 <ℓ\nparallel for i←0 :ℓ−hdo\n⊿Implemented with warp shuﬄe butterﬂy\ncompare-swap (xi,xi+h)\nend for\nparallel do\nifp=leftthen ⊿left side recursion\nmerge-odd-continue ([xi]i=0:ℓ−h,left)\nmerge-odd-continue ([xi]i=ℓ−h:ℓ,right )\nelse ⊿right side recursion\nmerge-odd-continue ([xi]i=0:h,left)\nmerge-odd-continue ([xi]i=h:ℓ,right )\nend if\nend do\nend if\nend function\nOdd-size merging and sorting networks. If some input\ndata is already sorted, we can modify the network to avoid\nmerging steps. We may also not have a full power-of-2 set of\ndata, in which case we can eﬃciently shortcut to deal with\nthe smaller size. Algorithm 1 is an odd-sized merging network that merges\nalready sorted leftandright arrays, each of arbitrary length. While the bitonic network merges bitonic sequences, we start\nwith monotonic sequences: sequences sorted monotonically. A bitonic merge is made monotonic by reversing the ﬁrst\ncomparator stage.",
"While the bitonic network merges bitonic sequences, we start\nwith monotonic sequences: sequences sorted monotonically. A bitonic merge is made monotonic by reversing the ﬁrst\ncomparator stage. The odd size algorithm is derived by considering arrays to\nbe padded to the next highest power-of-2 size with dummy\n1 3 4 8 9 0 3 7\n1 3 4 3 0 9 8 7\n0 3 4 3 1 7 8 9\n0 3 1 3 4 7 8 9\n0 1 3 3 4 7 8 9step 1\nstep 2\nstep 3\nstep 4\nFigure 1: Odd-size network merging arrays of sizes\n5 and 3. Bullets indicate parallel compare/swap. Dashed lines are elided elements or comparisons. 4",
"input \ninsertionthread queuemerging  networkwarp queue\nlane 0\nlane 1\nlane 31coalesced \nread\n. . . . .. . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Figure 2: Overview of WarpSelect. The input val-\nues stream in on the left, and the warp queue on the\nright holds the output result. elements that are never swapped (the merge is monotonic)\nand are already properly positioned; any comparisons with\ndummy elements are elided. A left array is considered to\nbe padded with dummy elements at the start; a right ar-\nray has them at the end. A merge of two sorted arrays\nof lengthℓLandℓRto a sorted array of ℓL+ℓRrequires\n⌈log2(max(ℓL,ℓR))⌉+ 1 parallel steps. Figure 1 shows Algo-\nrithm 1’s merging network for arrays of size 5 and 3, with 4\nparallel steps. Thecompare-swap is implemented using warp shuﬄes on\na lane-stride register array. Swaps with a stride a multiple\nof 32 occur directly within a lane as the lane holds both\nelements locally. Swaps of stride ≤16 or a non-multiple of\n32 occur with warp shuﬄes. In practice, used array lengths\nare multiples of 32 as they are held in lane-stride arrays. Algorithm 2 Odd-size sorting network\nfunction sort-odd ([xi]i=0:ℓ)\nifℓ>1then\nparallel do\nsort-odd ([xi]i=0:⌊ℓ/2⌋)\nsort-odd ([xi]i=⌊ℓ/2⌋:ℓ)\nend do\nmerge-odd ([xi]i=0:⌊ℓ/2⌋,[xi]i=⌊ℓ/2⌋:ℓ)\nend if\nend function\nAlgorithm 2 extends the merge to a full sort. Assuming no\nstructure present in the input data,1\n2(⌈log2(ℓ)⌉2+⌈log2(ℓ)⌉)\nparallel steps are required for sorting data of length ℓ. 4.2 WarpSelect\nOurk-selection implementation, WarpSelect , maintains\nstate entirely in registers, requires only a single pass over\ndata and avoids cross-warp synchronization. It uses merge-\nodd andsort-odd as primitives. Since the register ﬁle pro-\nvides much more storage than shared memory, it supports\nk≤1024.",
"It uses merge-\nodd andsort-odd as primitives. Since the register ﬁle pro-\nvides much more storage than shared memory, it supports\nk≤1024. Each warp is dedicated to k-selection to a single\none of thenarrays [ai]. Ifnis large enough, a single warp\nper each [ai] will result in full GPU occupancy. Large ℓper\nwarp is handled by recursive decomposition, if ℓis known in\nadvance. Overview. Our approach (Algorithm 3 and Figure 2) oper-\nates on values, with associated indices carried along (omit-\nted from the description for simplicity). It selects the kleast\nvalues that come from global memory, or from intermediate\nvalue registers if fused into another kernel providing the val-\nues. Let [ai]i=0:ℓbe the sequence provided for selection.The elements (on the left of Figure 2) are processed in\ngroups of 32, the warp size. Lane jis responsible for pro-\ncessing{aj,a32+j,...}; thus, if the elements come from global\nmemory, the reads are contiguous and coalesced into a min-\nimal number of memory transactions. Data structures. Each lanejmaintains a small queue\noftelements in registers, called the thread queues [Tj\ni]i=0:t,\nordered from largest to smallest ( Tj\ni≥Tj\ni+1). The choice of\ntis made relative to k, see Section 4.3. The thread queue is\na ﬁrst-level ﬁlter for new values coming in. If a new a32i+j\nis greater than the largest key currently in the queue, Tj\n0, it\nis guaranteed that it won’t be in the ksmallest ﬁnal results. The warp shares a lane-stride register array of ksmallest\nseen elements, [ Wi]i=0:k, called the warp queue . It is ordered\nfrom smallest to largest ( Wi≤Wi+1); if the requested kis\nnot a multiple of 32, we round it up. This is a second level\ndata structure that will be used to maintain all of the k\nsmallest warp-wide seen values. The thread and warp queues\nare initialized to maximum sentinel values, e.g., +∞. Update.",
"This is a second level\ndata structure that will be used to maintain all of the k\nsmallest warp-wide seen values. The thread and warp queues\nare initialized to maximum sentinel values, e.g., +∞. Update. The three invariants maintained are:\n•all per-lane Tj\n0are not in the min- k\n•all per-lane Tj\n0are greater than all warp queue keys\nWi\n•allaiseen so far in the min- kare contained in either\nsome lane’s thread queue ([ Tj\ni]i=0:t,j=0:32), or in the\nwarp queue. Lanejreceives a new a32i+jand attempts to insert it into\nits thread queue. If a32i+j> Tj\n0, then the new pair is by\ndeﬁnition not in the kminimum, and can be rejected. Otherwise, it is inserted into its proper sorted position\nin the thread queue, thus ejecting the old Tj",
"complete doing this with their new received pair and their\nthread queue, but it is now possible that the second invariant\nhave been violated. Using the warp ballot instruction, we\ndetermine if any lane has violated the second invariant. If\nnot, we are free to continue processing new elements. Restoring the invariants. If any lane has its invariant\nviolated, then the warp uses odd-merge to merge and sort\nthe thread and warp queues together. The new warp queue\nAlgorithm 3 WarpSelect pseudocode for lane j\nfunction WarpSelect (a)\nifa<Tj\n0then\ninsertainto our [Tj\ni]i=0:t\nend if\nifwarp-ballot (Tj\n0<W k−1)then\n⊿Reinterpret thread queues as lane-stride array\n[αi]i=0:32 t←cast ([Tj\ni]i=0:t,j=0:32)\n⊿concatenate and sort thread queues\nsort-odd ([αi]i=0:32 t)\nmerge-odd ([Wi]i=0:k,[αi]i=0:32 t)\n⊿Reinterpret lane-stride array as thread queues\n[Tj\ni]i=0:t,j=0:32←cast ([αi]i=0:32 t)\nreverse-array ([Ti]i=0:t)\n⊿Back in thread queue order, invariant restored\nend if\nend function\n5",
"will be the min- kelements across the merged, sorted queues,\nand the new thread queues will be the remainder, from min-\n(k+1) to min-( k+32t+1). This restores the invariants and\nwe are free to continue processing subsequent elements. Since the thread and warp queues are already sorted, we\nmerge the sorted warp queue of length kwith 32 sorted\narrays of length t. Supporting odd-sized merges is important\nbecause Batcher’s formulation would require that 32 t=k\nand is a power-of-2; thus if k= 1024,tmust be 32. We\nfound that the optimal tis way smaller (see below). Using odd-merge to merge the 32 already sorted thread\nqueues would require a struct-of-arrays toarray-of-structs\ntransposition in registers across the warp, since the tsucces-\nsive sorted values are held in diﬀerent registers in the same\nlane rather than a lane-stride array. This is possible [12],\nbut would use a comparable number of warp shuﬄes, so we\njust reinterpret the thread queue registers as an (unsorted)\nlane-stride array and sort from scratch. Signiﬁcant speedup\nis realizable by using odd-merge for the merge of the ag-\ngregate sorted thread queues with the warp queue. Handling the remainder. If there are remainder elements\nbecauseℓis not a multiple of 32, those are inserted into the\nthread queues for the lanes that have them, after which we\nproceed to the output stage. Output. A ﬁnal sort and merge is made of the thread and\nwarp queues, after which the warp queue holds all min- k\nvalues. 4.3 Complexity and parameter selection\nFor each incoming group of 32 elements, WarpSelect\ncan perform 1, 2 or 3 constant-time operations, all happen-\ning in warp-wide parallel time:\n1. read 32 elements, compare to all thread queue heads\nTj\n0, costC1, happensN1times;\n2. if∃j∈{0,...,31},a32n+j<Tj\n0, perform insertion sort\non those speciﬁc thread queues, cost C2=O(t), hap-\npensN2times;\n3. if∃j,Tj\n0< W k−1, sort and merge queues, cost C3=\nO(tlog(32t)2+klog(max(k,32t))), happens N3times.",
"if∃j∈{0,...,31},a32n+j<Tj\n0, perform insertion sort\non those speciﬁc thread queues, cost C2=O(t), hap-\npensN2times;\n3. if∃j,Tj\n0< W k−1, sort and merge queues, cost C3=\nO(tlog(32t)2+klog(max(k,32t))), happens N3times. Thus, the total cost is N1C1+N2C2+N3C3.N1=ℓ/32,\nand on random data drawn independently, N2=O(klog(ℓ))\nandN3=O(klog(ℓ)/t), see the Appendix for a full deriva-\ntion. Hence, the trade-oﬀ is to balance a cost in N2C2and\none inN3C3. The practical choice for tgivenkandℓwas\nmade by experiment on a variety of k-NN data. For k≤32,\nwe uset= 2,k≤128 usest= 3,k≤256 usest= 4, and\nk≤1024 usest= 8, all irrespective of ℓ.",
"This section explains how IVFADC, one of the indexing\nmethods originally built upon product quantization [25], is\nimplemented eﬃciently. Details on distance computations\nand articulation with k-selection are the key to understand-\ning why this method can outperform more recent GPU-\ncompliant approximate nearest neighbor strategies [47]. 5.1 Exact search\nWe brieﬂy come back to the exhaustive search method,\noften referred to as exact brute-force. It is interesting on itsown for exact nearest neighbor search in small datasets. It\nis also a component of many indexes in the literature. In\nour case, we use it for the IVFADC coarse quantizer q1. As stated in Section 2, the distance computation boils\ndown to a matrix multiplication. We use optimized GEMM\nroutines in the cuBLAS library to calculate the −2⟨xj,yi⟩\nterm for L2 distance, resulting in a partial distance matrix\nD′. To complete the distance calculation, we use a fused\nk-selection kernel that adds the ∥yi∥2term to each entry of\nthe distance matrix and immediately submits the value to\nk-selection in registers. The ∥xj∥2term need not be taken\ninto account before k-selection. Kernel fusion thus allows\nfor only 2 passes (GEMM write, k-select read) over D′, com-\npared to other implementations that may require 3 or more. Row-wisek-selection is likely not fusable with a well-tuned\nGEMM kernel, or would result in lower overall eﬃciency. AsD′does not ﬁt in GPU memory for realistic problem\nsizes, the problem is tiled over the batch of queries, with\ntq≤nqqueries being run in a single tile. Each of the ⌈nq/tq⌉\ntiles are independent problems, but we run two in parallel\non diﬀerent streams to better occupy the GPU, so the eﬀec-\ntive memory requirement of DisO(2ℓtq). The computation\ncan similarly be tiled over ℓ. For very large input coming\nfrom the CPU, we support buﬀering with pinned memory\nto overlap CPU to GPU copy with GPU compute. 5.2 IVFADC indexing\nPQ lookup tables. At its core, the IVFADC requires com-\nputing the distance from a vector to a set of product quanti-\nzation reproduction values.",
"5.2 IVFADC indexing\nPQ lookup tables. At its core, the IVFADC requires com-\nputing the distance from a vector to a set of product quanti-\nzation reproduction values. By developing Equation (6) for\na database vector y, we obtain:\n∥x−q(y)∥2\n2=∥x−q1(y)−q2(y−q1(y))∥2\n2. (7)\nIf we decompose the residual vectors left after q1as:\ny−q1(y) = [˜y1···˜yb] and (8)\nx−q1(y) = [˜x1···˜xb] (9)\nthen the distance is rewritten as:\n∥x−q(y)∥2\n2=∥˜x1−q1(˜y1)∥2\n2+...+∥˜xb−qb(˜yb)∥2\n2.(10)\nEach quantizer q1,...,qbhas 256 reproduction values, so\nwhenxandq1(y) are known all distances can be precom-\nputed and stored in tables T1,...,T beach of size 256 [25]. Computing the sum (10) consists of blook-ups and addi-\ntions. Comparing the cost to compute ndistances:\n•Explicit computation: n×dmutiply-adds;\n•With lookup tables: 256 ×dmultiply-adds and n×b\nlookup-adds. This is the key to the eﬃciency of the product quantizer. In our GPU implementation, bis any multiple of 4 up to",
"vector within lists. IVFADC lookup tables. When scanning over the ele-\nments of the inverted list IL(where by deﬁnition q1(y) is\nconstant), the look-up table method can be applied, as the\nqueryxandq1(y) are known. 6",
"Moreover, the computation of the tables T1...T bis fur-\nther optimized [5]. The expression of ∥x−q(y)∥2\n2in Equation\n(7) can be decomposed as:\n∥q2(...)∥2\n2+ 2⟨q1(y),q2(...)⟩\nterm 1+∥x−q1(y)∥2\n2\nterm 2−2⟨x,q2(...)⟩\nterm 3. (11)\nThe objective is to minimize inner loop computations. The computations we can do in advance and store in lookup\ntables are as follows:\n•Term 1 is independent of the query. It can be precom-\nputed from the quantizers, and stored in a table Tof\nsize|C1|×256×b;\n•Term 2 is the distance to q1’s reproduction value. It is\nthus a by-product of the ﬁrst-level quantizer q1;\n•Term 3 can be computed independently of the inverted\nlist. Its computation costs d×256 multiply-adds. This decomposition is used to produce the lookup tables\nT1...T bused during the scan of the inverted list. For a\nsingle query, computing the τ×btables from scratch costs\nτ×d×256 multiply-adds, while this decomposition costs\n256×dmultiply-adds and τ×b×256 additions. On the GPU,\nthe memory usage of Tcan be prohibitive, so we enable the\ndecomposition only when memory is a not a concern. 5.3 GPU implementation\nAlgorithm 4 summarizes the process as one would im-\nplement it on a CPU. The inverted lists are stored as two\nseparate arrays, for PQ codes and associated IDs. IDs are\nresolved only if k-selection determines k-nearest member-\nship. This lookup yields a few sparse memory reads in a\nlarge array, thus the IDs can optionally be stored on CPU\nfor tiny performance cost. List scanning. A kernel is responsible for scanning the τ\nclosest inverted lists for each query, and calculating the per-\nvector pair distances using the lookup tables Ti. TheTiare\nstored in shared memory: up to nq×τ×max i|Ii|×blookups\nare required for a query set (trillions of accesses in practice),\nand are random access. This limits bto at most 48 (32-\nbit ﬂoating point) or 96 (16-bit ﬂoating point) with current\narchitectures. In case we do not use the decomposition of\nEquation (11), the Tiare calculated by a separate kernel\nbefore scanning. Multi-pass kernels.",
"This limits bto at most 48 (32-\nbit ﬂoating point) or 96 (16-bit ﬂoating point) with current\narchitectures. In case we do not use the decomposition of\nEquation (11), the Tiare calculated by a separate kernel\nbefore scanning. Multi-pass kernels. Eachnq×τpairs of query against\ninverted list can be processed independently. At one ex-\ntreme, a block is dedicated to each of these, resulting in up\ntonq×τ×max i|Ii|partial results being written back to\nglobal memory, which is then k-selected to nq×kﬁnal re-\nsults. This yields high parallelism but can exceed available\nGPU global memory; as with exact search, we choose a tile\nsizetq≤nqto reduce memory consumption, bounding its\ncomplexity byO(2tqτmax i|Ii|) with multi-streaming. A single warp could be dedicated to k-selection of each\ntqset of lists, which could result in low parallelism. We\nintroduce a two-pass k-selection, reducing tq×τ×max i|Ii|\ntotq×f×kpartial results for some subdivision factor f. This is reduced again via k-selection to the ﬁnal tq×kresults. Fused kernel. As with exact search, we experimented with\na kernel that dedicates a single block to scanning all τlistsfor a single query, with k-selection fused with distance com-\nputation. This is possible as WarpSelect does not ﬁght for\nthe shared memory resource which is severely limited. This\nreduces global memory write-back, since almost all interme-\ndiate results can be eliminated. However, unlike k-selection\noverhead for exact computation, a signiﬁcant portion of the\nruntime is the gather from the Tiin shared memory and lin-\near scanning of the Iifrom global memory; the write-back is\nnot a dominant contributor. Timing for the fused kernel is\nimproved by at most 15%, and for some problem sizes would\nbe subject to lower parallelism and worse performance with-\nout subsequent decomposition. Therefore, and for reasons\nof implementation simplicity, we do not use this layout.",
"Timing for the fused kernel is\nimproved by at most 15%, and for some problem sizes would\nbe subject to lower parallelism and worse performance with-\nout subsequent decomposition. Therefore, and for reasons\nof implementation simplicity, we do not use this layout. Algorithm 4 IVFPQ batch search routine\nfunction ivfpq-search ([x1,...,x nq],I1,...,I|C1|)\nfori←0 :nqdo⊿batch quantization of Section 5.1\nLi\nIVF←τ-argminc∈C1∥x−c∥2\nend for\nfori←0 :nqdo\nL←[] ⊿distance table\nCompute term 3 (see Section 5.2)\nforLinLi\nIVFdo ⊿ τloops\nCompute distance tables T1,...,T b\nforjinILdo\n⊿distance estimation, Equation (10)\nd←∥xi−q(yj)∥2\n2\nAppend (d,L,j ) toL\nend for\nend for\nRi←k-select smallest distances dfromL\nend for\nreturn R\nend function\n5.4 Multi-GPU parallelism\nModern servers can support several GPUs. We employ\nthis capability for both compute power and memory. Replication. If an index instance ﬁts in the memory of a\nsingle GPU, it can be replicated across Rdiﬀerent GPUs. To\nquerynqvectors, each replica handles a fraction nq/Rof the\nqueries, joining the results back together on a single GPU\nor in CPU memory. Replication has near linear speedup,\nexcept for a potential loss in eﬃciency for small nq. Sharding. If an index instance does not ﬁt in the memory\nof a single GPU, an index can be sharded across Sdiﬀer-\nent GPUs. For adding ℓvectors, each shard receives ℓ/Sof\nthe vectors, and for query, each shard handles the full query\nsetnq, joining the partial results (an additional round of k-\nselection is still required) on a single GPU or in CPU mem-\nory. For a given index size ℓ, sharding will yield a speedup\n(sharding has a query of nqagainstℓ/Sversus replication\nwith a query of nq/Ragainstℓ), but is usually less than\npure replication due to ﬁxed overhead and cost of subse-\nquentk-selection. Replication and sharding can be used together ( Sshards,\neach withRreplicas forS×R GPUs in total). Sharding or\nreplication are both fairly trivial, and the same principle can\nbe used to distribute an index across multiple machines. 7",
"�������������\n����� ����� ������ ������������������\n����������������������������������\n������������\n����������\n����������������������Figure 3: Runtimes for diﬀerent k-selection meth-\nods, as a function of array length ℓ. Simultaneous\narrays processed are nq= 10000 .k= 100 for full lines,\nk= 1000 for dashed lines.",
"This section compares our GPU k-selection and nearest-\nneighbor approach to existing libraries. Unless stated other-\nwise, experiments are carried out on a 2 ×2.8GHz Intel Xeon\nE5-2680v2 with 4 Maxwell Titan X GPUs on CUDA 8.0. 6.1 k-selection performance\nWe compare against two other GPU small k-selection im-\nplementations: the row-based Merge Queue with Buﬀered\nSearch and Hierarchical Partition extracted from the fgknn\nlibrary of Tang et al. [41] and Truncated Bitonic Sort ( TBiS )\nfrom Sismanis et al. [40]. Both were extracted from their re-\nspective exact search libraries. We evaluate k-selection for k= 100 and 1000 of each row\nfrom a row-major matrix nq×ℓof random 32-bit ﬂoating\npoint values on a single Titan X. The batch size nqis ﬁxed\nat 10000, and the array lengths ℓvary from 1000 to 128000. Inputs and outputs to the problem remain resident in GPU\nmemory, with the output being of size nq×k, with corre-\nsponding indices. Thus, the input problem sizes range from\n40 MB (ℓ= 1000) to 5.12 GB ( ℓ= 128k). TBiS requires large\nauxiliary storage, and is limited to ℓ≤48000 in our tests. Figure 3 shows our relative performance against TBiS and\nfgknn. It also includes the peak possible performance given\nby the memory bandwidth limit of the Titan X. The rela-\ntive performance of WarpSelect over fgknn increases for\nlargerk; even TBiS starts to outperform fgknn for larger ℓ\natk= 1000. We look especially at the largest ℓ= 128000. WarpSelect is 1.62×faster atk= 100, 2.01×atk= 1000. Performance against peak possible drops oﬀ for all imple-\nmentations at larger k.WarpSelect operates at 55% of\npeak atk= 100 but only 16% of peak at k= 1000. This\nis due to additional overhead assocated with bigger thread\nqueues and merge/sort networks for large k. Diﬀerences from fgknn.",
"This\nis due to additional overhead assocated with bigger thread\nqueues and merge/sort networks for large k. Diﬀerences from fgknn. WarpSelect is inﬂuenced by\nfgknn, but has several improvements: all state is maintained\nin registers (no shared memory), no inter-warp synchroniza-\ntion or buﬀering is used, no “hierarchical partition”, the k-\nselection can be fused into other kernels, and it uses odd-size\nnetworks for eﬃcient merging and sorting.# centroids\nmethod # GPUs 256 4096\nBIDMach [11] 1 320 s 735 s\nOurs 1 140 s 316 s\nOurs 4 84 s 100 s\nTable 1: MNIST8m k-means performance\n6.2 k-means clustering\nThe exact search method with k= 1 can be used by a k-\nmeans clustering method in the assignment stage, to assign\nnqtraining vectors to |C1|centroids. Despite the fact that\nit does not use the IVFADC and k= 1 selection is trivial (a\nparallel reduction is used for the k= 1 case, not WarpSe-\nlect ),k-means is a good benchmark for the clustering used\nto train the quantizer q1. We apply the algorithm on MNIST8m images. The 8.1M\nimages are graylevel digits in 28x28 pixels, linearized to vec-\ntors of 784-d. We compare this k-means implementation to\nthe GPUk-means of BIDMach [11], which was shown to be\nmore eﬃcient than several distributed k-means implemen-\ntations that require dozens of machines3. Both algorithms\nwere run for 20 iterations. Table 1 shows that our imple-\nmentation is more than 2×faster , although both are built\nupon cuBLAS. Our implementation receives some beneﬁt\nfrom thek-selection fusion into L2 distance computation. For multi-GPU execution via replicas, the speedup is close\nto linear for large enough problems (3.16 ×for 4 GPUs with\n4096 centroids). Note that this benchmark is somewhat un-\nrealistic, as one would typically sub-sample the dataset ran-\ndomly when so few centroids are requested. Large scale. We can also compare to [3], an approximate\nCPU method that clusters 108128-d vectors to 85k cen-\ntroids. Their clustering method runs in 46 minutes, but re-\nquires 56 minutes (at least) of pre-processing to encode the\nvectors.",
"Large scale. We can also compare to [3], an approximate\nCPU method that clusters 108128-d vectors to 85k cen-\ntroids. Their clustering method runs in 46 minutes, but re-\nquires 56 minutes (at least) of pre-processing to encode the\nvectors. Our method performs exact k-means on 4 GPUs in\n52 minutes without any pre-processing. 6.3 Exact nearest neighbor search\nWe consider a classical dataset used to evaluate nearest\nneighbor search: Sift1M [25]. Its characteristic sizes are\nℓ= 106,d= 128,nq= 104. Computing the partial distance\nmatrixD′costsnq×ℓ×d= 1.28 Tﬂop, which runs in less\nthan one second on current GPUs. Figure 4 shows the cost\nof the distance computations against the cost of our tiling\nof the GEMM for the −2⟨xj,yi⟩term of Equation 2 and\nthe peak possible k-selection performance on the distance\nmatrix of size nq×ℓ, which additionally accounts for reading\nthe tiled result matrix D′at peak memory bandwidth. In addition to our method from Section 5, we include\ntimes from the two GPU libraries evaluated for k-selection\nperformance in Section 6.1. We make several observations:\n•fork-selection, the naive algorithm that sorts the full\nresult array for each query using thrust::sort_by_key\nis more than 10×slower than the comparison methods;\n•L2 distance and k-selection cost is dominant for all but\nour method, which has 85 % of the peak possible\nperformance, assuming GEMM usage and our tiling\n3BIDMach numbers from https://github.com/BIDData/\nBIDMach/wiki/Benchmarks#KMeans\n8",
"��������������������������\n������������������������������\n�����������������������\n����������������������\n����������\n����������������������\n�����Figure 4: Exact search k-NN time for the SIFT1M\ndataset with varying kon 1 Titan X GPU. of the partial distance matrix D′on top of GEMM is\nclose to optimal. The cuBLAS GEMM itself has low\neﬃciency for small reduction sizes ( d= 128);\n•Our fused L2/ k-selection kernel is important. Our\nsame exact algorithm without fusion (requiring an ad-\nditional pass through D′) is at least 25% slower. Eﬃcientk-selection is even more important in situations\nwhere approximate methods are used to compute distances,\nbecause the relative cost of k-selection with respect to dis-\ntance computation increases. 6.4 Billion-scale approximate search\nThere are few studies on GPU-based approximate nearest-\nneighbor search on large datasets ( ℓ≫106). We report a\nfew comparison points here on index search, using standard\ndatasets and evaluation protocol in this ﬁeld. SIFT1M. For the sake of completeness, we ﬁrst compare\nour GPU search speed on Sift1M with the implementation\nof Wieschollek et al. [47]. They obtain a nearest neighbor re-\ncall at 1 (fraction of queries where the true nearest neighbor\nis in the top 1 result) of R@1 = 0.51, and R@100 = 0.86 in\n0.02 ms per query on a Titan X. For the same time budget,\nour implementation obtains R@1 = 0.80 and R@100 = 0.95. SIFT1B. We compare again with Wieschollek et al., on the\nSift1B dataset [26] of 1 billion SIFT image features at nq=",
"memory usage for similar accuracy (more accurate methods\nmay involve greater search time or memory usage). On a\nsingle GPU, with m= 8 bytes per vector, R@10 = 0.376 in\n17.7µs per query vector, versus their reported R@10 = 0.35\nin 150µs per query vector. Thus, our implementation is\nmore accurate at a speed 8.5×faster . DEEP1B. We also experimented on the Deep1B dataset [6]\nofℓ=1 billion CNN representations for images at nq= 104. The paper that introduces the dataset reports CPU results\n(1 thread): R@1 = 0.45 in 20 ms search time per vector. We\nuse a PQ encoding of m= 20, with d= 80 via OPQ [17],\nand|C1|= 218, which uses a comparable dataset storage as\nthe original paper (20 GB). This requires multiple GPUs as\nit is too large for a single GPU’s global memory, so we con-\nsider 4 GPUs with S= 2,R= 2. We obtain a R@1 = 0.4517\nin 0.0133 ms per vector. While the hardware platforms are\n����������������������\n���������������������������������������������������������������\n������������������������������������������������������\n�������������������������\n�������������������������\n������������������\n�����������������������������������������������������������������\n����������������������������������������������������\n�������������������������\n���������������������\n���������������������Figure 5: Speed/accuracy trade-oﬀ of brute-force\n10-NN graph construction for the YFCC100M and\nDEEP1B datasets. diﬀerent, it shows that making searches on GPUs is a game-\nchanger in terms of speed achievable on a single machine. 6.5 The k-NN graph\nAn example usage of our similarity search method is to\nconstruct a k-nearest neighbor graph of a dataset via brute\nforce (all vectors queried against the entire index). Experimental setup. We evaluate the trade-oﬀ between\nspeed, precision and memory on two datasets: 95 million\nimages from the Yfcc100M dataset [42] and Deep1B . For\nYfcc100M , we compute CNN descriptors as the one-before-\nlast layer of a ResNet [23], reduced to d= 128 with PCA.",
"We evaluate the trade-oﬀ between\nspeed, precision and memory on two datasets: 95 million\nimages from the Yfcc100M dataset [42] and Deep1B . For\nYfcc100M , we compute CNN descriptors as the one-before-\nlast layer of a ResNet [23], reduced to d= 128 with PCA. The evaluation measures the trade-oﬀ between:\n•Speed: How much time it takes to build the IVFADC\nindex from scratch and construct the whole k-NN graph\n(k= 10) by searching nearest neighbors for all vectors\nin the dataset. Thus, this is an end-to-end test that\nincludes indexing as well as search time;\n•Quality: We sample 10,000 images for which we com-\npute the exact nearest neighbors. Our accuracy mea-\nsure is the fraction of 10 found nearest neighbors that\nare within the ground-truth 10 nearest neighbors. ForYfcc100M , we use a coarse quantizer (216centroids),\nand consider m= 16, 32 and 64 byte PQ encodings for each\nvector. For Deep1B , we pre-process the vectors to d= 120\nvia OPQ, use|C1|= 218and consider m= 20, 40. For a\ngiven encoding, we vary τfrom 1 to 256, to obtain trade-\noﬀs between eﬃciency and quality, as seen in Figure 5. 9",
"Figure 6: Path in the k-NN graph of 95 million images from YFCC100M. The ﬁrst and the last image are\ngiven; the algorithm computes the smoothest path between them. Discussion. ForYfcc100M we usedS= 1,R= 4. An\naccuracy of more than 0.8 is obtained in 35 minutes. For\nDeep1B , a lower-quality graph can be built in 6 hours,\nwith higher quality in about half a day. We also experi-\nmented with more GPUs by doubling the replica set, us-\ning 8 Maxwell M40s (the M40 is roughly equivalent in per-\nformance to the Titan X). Performance is improved sub-\nlinearly (∼1.6×form= 20,∼1.7×form= 40). For comparison, the largest k-NN graph construction we\nare aware of used a dataset comprising 36.5 million 384-\nd vectors, which took a cluster of 128 CPU servers 108.7\nhours of compute [45], using NN-Descent [15]. Note that\nNN-Descent could also build or reﬁne the k-NN graph for\nthe datasets we consider, but it has a large memory over-\nhead over the graph storage, which is already 80 GB for\nDeep1B . Moreover it requires random access across all vec-\ntors (384 GB for Deep1B ). The largest GPU k-NN graph construction we found is a\nbrute-force construction using exact search with GEMM, of\na dataset of 20 million 15,000-d vectors, which took a cluster\nof 32 Tesla C2050 GPUs 10 days [14]. Assuming computa-\ntion scales with GEMM cost for the distance matrix, this\napproach for Deep1B would take an impractical 200 days\nof computation time on their cluster. 6.6 Using the k-NN graph\nWhen ak-NN graph has been constructed for an image\ndataset, we can ﬁnd paths in the graph between any two\nimages, provided there is a single connected component (this\nis the case). For example, we can search the shortest path\nbetween two images of ﬂowers, by propagating neighbors\nfrom a starting image to a destination image. Denoting by\nSandDthe source and destination images, and dijthe\ndistance between nodes, we search the path P={p1,...,p n}\nwithp1=Sandpn=Dsuch that\nmin\nPmax\ni=1..ndpipi+1, (12)\ni.e., we want to favor smooth transitions. An example re-\nsult is shown in Figure 6 from Yfcc100M4.",
"An example re-\nsult is shown in Figure 6 from Yfcc100M4. It was ob-\ntained after 20 seconds of propagation in a k-NN graph with\nk= 15 neighbors. Since there are many ﬂower images in the\ndataset, the transitions are smooth. 4The mapping from vectors to images is not available for\nDeep1B7. CONCLUSION\nThe arithmetic throughput and memory bandwidth of\nGPUs are well into the teraﬂops and hundreds of gigabytes\nper second. However, implementing algorithms that ap-\nproach these performance levels is complex and counter-\nintuitive. In this paper, we presented the algorithmic struc-\nture of similarity search methods that achieves near-optimal\nperformance on GPUs. This work enables applications that needed complex ap-\nproximate algorithms before. For example, the approaches\npresented here make it possible to do exact k-means cluster-\ning or to compute the k-NN graph with simple brute-force\napproaches in less time than a CPU (or a cluster of them)\nwould take to do this approximately. GPU hardware is now very common on scientiﬁc work-\nstations, due to their popularity for machine learning algo-\nrithms. We believe that our work further demonstrates their\ninterest for database applications. Along with this work, we\nare publishing a carefully engineered implementation of this\npaper’s algorithms, so that these GPUs can now also be used\nfor eﬃcient similarity search.",
"[1] T. Alabi, J. D. Blanchard, B. Gordon, and R. Steinbach. Fast k-selection algorithms for graphics processing units. ACM Journal of Experimental Algorithmics ,\n17:4.2:4.1–4.2:4.29, October 2012. [2] F. Andr´ e, A.-M. Kermarrec, and N. L. Scouarnec. Cache\nlocality is not enough: High-performance nearest neighbor\nsearch with product quantization fast scan. In Proc. International Conference on Very Large DataBases , pages\n288–299, 2015. [3] Y. Avrithis, Y. Kalantidis, E. Anagnostopoulos, and I. Z. Emiris. Web-scale image clustering revisited. In Proc. International Conference on Computer Vision , pages\n1502–1510, 2015. [4] A. Babenko and V. Lempitsky. The inverted multi-index. InProc. IEEE Conference on Computer Vision and\nPattern Recognition , pages 3069–3076, June 2012. [5] A. Babenko and V. Lempitsky. Improving bilayer product\nquantization for billion-scale approximate nearest neighbors\nin high dimensions. arXiv preprint arXiv:1404.1831 , 2014. [6] A. Babenko and V. Lempitsky. Eﬃcient indexing of\nbillion-scale datasets of deep descriptors. In Proc. IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 2055–2063, June 2016. [7] R. Barrientos, J. G´ omez, C. Tenllado, M. Prieto, and\nM. Marin. knn query processing in metric spaces using\nGPUs. In International European Conference on Parallel\nand Distributed Computing , volume 6852 of Lecture Notes\n10",
"in Computer Science , pages 380–392, Bordeaux, France,\nSeptember 2011. Springer. [8] K. E. Batcher. Sorting networks and their applications. In\nProc. Spring Joint Computer Conference , AFIPS ’68\n(Spring), pages 307–314, New York, NY, USA, 1968. ACM. [9] P. Boncz, W. Lehner, and T. Neumann. Special issue:\nModern hardware. The VLDB Journal , 25(5):623–624,\n2016. [10] J. Canny, D. L. W. Hall, and D. Klein. A multi-teraﬂop\nconstituency parser using GPUs. In Proc. Empirical\nMethods on Natural Language Processing , pages 1898–1907. ACL, 2013. [11] J. Canny and H. Zhao. Bidmach: Large-scale learning with\nzero memory allocation. In BigLearn workshop, NIPS ,\n2013. [12] B. Catanzaro, A. Keller, and M. Garland. A decomposition\nfor in-place matrix transposition. In Proc. ACM\nSymposium on Principles and Practice of Parallel\nProgramming , PPoPP ’14, pages 193–206, 2014. [13] J. Chhugani, A. D. Nguyen, V. W. Lee, W. Macy,\nM. Hagog, Y.-K. Chen, A. Baransi, S. Kumar, and\nP. Dubey. Eﬃcient implementation of sorting on multi-core\nsimd cpu architecture. Proc. VLDB Endow. ,\n1(2):1313–1324, August 2008. [14] A. Dashti. Eﬃcient computation of k-nearest neighbor\ngraphs for large high-dimensional data sets on gpu clusters. Master’s thesis, University of Wisconsin Milwaukee, August\n2013. [15] W. Dong, M. Charikar, and K. Li. Eﬃcient k-nearest\nneighbor graph construction for generic similarity measures. InWWW: Proceeding of the International Conference on\nWorld Wide Web , pages 577–586, March 2011. [16] M. Douze, H. J´ egou, and F. Perronnin. Polysemous codes. InProc. European Conference on Computer Vision , pages\n785–801. Springer, October 2016. [17] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product\nquantization. IEEE Trans. PAMI , 36(4):744–755, 2014. [18] Y. Gong and S. Lazebnik. Iterative quantization: A\nprocrustean approach to learning binary codes. In Proc. IEEE Conference on Computer Vision and Pattern\nRecognition , pages 817–824, June 2011. [19] Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale\norderless pooling of deep convolutional activation features. InProc. European Conference on Computer Vision , pages\n392–407, 2014. [20] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep\nimage retrieval: Learning global representations for image\nsearch. In Proc.",
"Lazebnik. Multi-scale\norderless pooling of deep convolutional activation features. InProc. European Conference on Computer Vision , pages\n392–407, 2014. [20] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep\nimage retrieval: Learning global representations for image\nsearch. In Proc. European Conference on Computer Vision ,\npages 241–257, 2016. [21] S. Han, H. Mao, and W. J. Dally. Deep compression:\nCompressing deep neural networks with pruning, trained\nquantization and huﬀman coding. arXiv preprint\narXiv:1510.00149 , 2015. [22] K. He, F. Wen, and J. Sun. K-means hashing: An\naﬃnity-preserving quantization method for learning binary\ncompact codes. In Proc. IEEE Conference on Computer\nVision and Pattern Recognition , pages 2938–2945, June\n2013. [23] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\nlearning for image recognition. In Proc. IEEE Conference\non Computer Vision and Pattern Recognition , pages\n770–778, June 2016. [24] X. He, D. Agarwal, and S. K. Prasad. Design and\nimplementation of a parallel priority queue on many-core\narchitectures. IEEE International Conference on High\nPerformance Computing , pages 1–10, 2012. [25] H. J´ egou, M. Douze, and C. Schmid. Product quantization\nfor nearest neighbor search. IEEE Trans. PAMI ,\n33(1):117–128, January 2011. [26] H. J´ egou, R. Tavenard, M. Douze, and L. Amsaleg. Searching in one billion vectors: re-rank with source\ncoding. In International Conference on Acoustics, Speech,and Signal Processing , pages 861–864, May 2011. [27] Y. Kalantidis and Y. Avrithis. Locally optimized product\nquantization for approximate nearest neighbor search. In\nProc. IEEE Conference on Computer Vision and Pattern\nRecognition , pages 2329–2336, June 2014. [28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in Neural Information Processing Systems , pages\n1097–1105, 2012. [29] F. T. Leighton. Introduction to Parallel Algorithms and\nArchitectures: Array, Trees, Hypercubes . Morgan\nKaufmann Publishers Inc., San Francisco, CA, USA, 1992. [30] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym. NVIDIA Tesla: a uniﬁed graphics and computing\narchitecture. IEEE Micro , 28(2):39–55, March 2008. [31] W. Liu and B. Vinter.",
"Morgan\nKaufmann Publishers Inc., San Francisco, CA, USA, 1992. [30] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym. NVIDIA Tesla: a uniﬁed graphics and computing\narchitecture. IEEE Micro , 28(2):39–55, March 2008. [31] W. Liu and B. Vinter. Ad-heap: An eﬃcient heap data\nstructure for asymmetric multicore processors. In Proc. of\nWorkshop on General Purpose Processing Using GPUs ,\npages 54:54–54:63. ACM, 2014. [32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean. Distributed representations of words and phrases\nand their compositionality. In Advances in Neural\nInformation Processing Systems , pages 3111–3119, 2013. [33] L. Monroe, J. Wendelberger, and S. Michalak. Randomized\nselection on the GPU. In Proc. ACM Symposium on High\nPerformance Graphics , pages 89–98, 2011. [34] M. Norouzi and D. Fleet. Cartesian k-means. In Proc. IEEE Conference on Computer Vision and Pattern\nRecognition , pages 3017–3024, June 2013. [35] M. Norouzi, A. Punjani, and D. J. Fleet. Fast search in\nHamming space with multi-index hashing. In Proc. IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 3108–3115, 2012. [36] J. Pan and D. Manocha. Fast GPU-based locality sensitive\nhashing for k-nearest neighbor computation. In Proc. ACM\nInternational Conference on Advances in Geographic\nInformation Systems , pages 211–220, 2011. [37] L. Paulev´ e, H. J´ egou, and L. Amsaleg. Locality sensitive\nhashing: a comparison of hash function types and querying\nmechanisms. Pattern recognition letters , 31(11):1348–1358,\nAugust 2010. [38] O. Shamir. Fundamental limits of online and distributed\nalgorithms for statistical learning and estimation. In\nAdvances in Neural Information Processing Systems , pages\n163–171, 2014. [39] A. Sharif Razavian, H. Azizpour, J. Sullivan, and\nS. Carlsson. CNN features oﬀ-the-shelf: an astounding\nbaseline for recognition. In CVPR workshops , pages\n512–519, 2014. [40] N. Sismanis, N. Pitsianis, and X. Sun. Parallel search of\nk-nearest neighbors with synchronous operations. In IEEE\nHigh Performance Extreme Computing Conference , pages\n1–6, 2012. [41] X. Tang, Z. Huang, D. M. Eyers, S. Mills, and M. Guo. Eﬃcient selection algorithm for fast k-nn search on GPUs.",
"Sun. Parallel search of\nk-nearest neighbors with synchronous operations. In IEEE\nHigh Performance Extreme Computing Conference , pages\n1–6, 2012. [41] X. Tang, Z. Huang, D. M. Eyers, S. Mills, and M. Guo. Eﬃcient selection algorithm for fast k-nn search on GPUs. InIEEE International Parallel & Distributed Processing\nSymposium , pages 397–406, 2015. [42] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde,\nK. Ni, D. Poland, D. Borth, and L.-J. Li. YFCC100M: The\nnew data in multimedia research. Communications of the\nACM , 59(2):64–73, January 2016. [43] V. Volkov and J. W. Demmel. Benchmarking GPUs to tune\ndense linear algebra. In Proc. ACM/IEEE Conference on\nSupercomputing , pages 31:1–31:11, 2008. [44] A. Wakatani and A. Murakami. GPGPU implementation of\nnearest neighbor search with product quantization. In\nIEEE International Symposium on Parallel and Distributed\nProcessing with Applications , pages 248–253, 2014. [45] T. Warashina, K. Aoyama, H. Sawada, and T. Hattori. Eﬃcient k-nearest neighbor graph construction using\nmapreduce for large-scale data sets. IEICE Transactions ,\n11",
"97-D(12):3142–3154, 2014. [46] R. Weber, H.-J. Schek, and S. Blott. A quantitative\nanalysis and performance study for similarity-search\nmethods in high-dimensional spaces. In Proc. International\nConference on Very Large DataBases , pages 194–205, 1998. [47] P. Wieschollek, O. Wang, A. Sorkine-Hornung, and\nH. P. A. Lensch. Eﬃcient large-scale approximate nearest\nneighbor search on the GPU. In Proc. IEEE Conference on\nComputer Vision and Pattern Recognition , pages\n2027–2035, June 2016. [48] S. Williams, A. Waterman, and D. Patterson. Rooﬂine: An\ninsightful visual performance model for multicore\narchitectures. Communications of the ACM , 52(4):65–76,\nApril 2009. Appendix: Complexity analysis of WarpSelect\nWe derive the average number of times updates are triggered\ninWarpSelect , for use in Section 4.3. Let the input to k-selection be a sequence {a1,a2,...,a ℓ}\n(1-based indexing), a randomly chosen permutation of a set\nof distinct elements. Elements are read sequentially in c\ngroups of size w(the warp; in our case, w= 32); assume ℓ\nis a multiple of w, soc=ℓ/w. Recall that tis the thread\nqueue length. We call elements prior to or at position n\nin the min- kseen so far the successive min- k(atn). The\nlikelihood that anis in the successive min- katnis:\nα(n,k) :={\n1 ifn≤k\nk/n ifn>k(13)\nas eachan,n>k has ak/nchance as all permutations are\nequally likely, and all elements in the ﬁrst kqualify. Counting the insertion sorts. In a given lane, an inser-\ntion sort is triggered if the incoming value is in the successive\nmin-k+tvalues, but the lane has “seen” only wc0+ (c−c0)\nvalues, where c0is the previous won warp ballot. The prob-\nability of this happening is:\nα(wc0+ (c−c0),k+t)≈k+t\nwcforc>k. (14)\nThe approximation considers that the thread queue has seen\nallthewcvalues, not just those assigned to its lane. The\nprobability of anylane triggering an insertion sort is then:\n1−(\n1−k+t\nwc)w\n≈k+t\nc. (15)\nHere the approximation is a ﬁrst-order Taylor expansion. Summing up the probabilities over cgives an expected num-\nber of insertions of N2≈(k+t) log(c) =O(klog(ℓ/w)). Counting full sorts.",
"(15)\nHere the approximation is a ﬁrst-order Taylor expansion. Summing up the probabilities over cgives an expected num-\nber of insertions of N2≈(k+t) log(c) =O(klog(ℓ/w)). Counting full sorts. We seekN3=π(ℓ,k,t,w ), the ex-\npected number of full sorts required for WarpSelect . Single lane. For now, we assume w= 1, soc=ℓ. Let\nγ(ℓ,m,k ) be the probability that in an sequence {a1,...,a ℓ},\nexactlymof the elements as encountered by a sequential\nscanner (w= 1) are in the successive min- k. Givenm, there\nare(ℓ\nm)\nplaces where these successive min- kelements can\noccur. It is given by a recurrence relation:\nγ(ℓ,m,k ) :=\n\n1 ℓ= 0 andm= 0\n0 ℓ= 0 andm> 0\n0 ℓ>0 andm= 0\n(γ(ℓ−1,m−1,k)·α(ℓ,k)+\nγ(ℓ−1,m,k )·(1−α(ℓ,k))) otherwise . (16)The last case is the probability of: there is a ℓ−1 se-\nquence with m−1 successive min- kelements preceding us,\nand the current element is in the successive min- k, or the\ncurrent element is not in the successive min- k,mones are\nbefore us. We can then develop a recurrence relationship for\nπ(ℓ,k,t, 1). Note that\nδ(ℓ,b,k,t ) :=min(( bt+max(0 ,t−1)),ℓ)∑\nm=btγ(ℓ,m,k ) (17)\nforbwhere 0≤bt≤ℓis the fraction of all sequences of\nlengthℓthat will force bsorts of data by winning the thread\nqueue ballot, as there have to be btto (bt+ max(0,t−1))\nelements in the successive min- kfor these sorts to happen (as\nthe min-kelements will overﬂow the thread queues). There\nare at most⌊ℓ/t⌋won ballots that can occur, as it takes t\nseparate sequential current min- kseen elements to win the\nballot.π(ℓ,k,t, 1) is thus the expectation of this over all\npossibleb:\nπ(ℓ,k,t, 1) =⌊ℓ/t⌋∑\nb=1b·δ(ℓ,b,k,t ). (18)\nThis can be computed by dynamic programming. Analyti-\ncally, note that for t= 1,k= 1,π(ℓ,1,1,1) is the harmonic\nnumberHℓ= 1+1\n2+1\n3+...+1\nℓ, which converges to ln( ℓ)+γ\n(the Euler-Mascheroni constant γ) asℓ→∞ . Fort= 1,k > 1,ℓ > k ,π(ℓ,k,1,1) =k+k(Hℓ−Hk)\norO(klog(ℓ)), as the ﬁrst kelements are in the successive\nmin-k, and the expectation for the rest isk\nk+1+k\nk+2+...+k\nℓ.",
"Fort= 1,k > 1,ℓ > k ,π(ℓ,k,1,1) =k+k(Hℓ−Hk)\norO(klog(ℓ)), as the ﬁrst kelements are in the successive\nmin-k, and the expectation for the rest isk\nk+1+k\nk+2+...+k\nℓ. Fort>1,k > 1,ℓ>k , note that there are some number\nD,k≤D≤ℓof successive min- kdeterminations Dmade\nfor each possible{a1,...,a ℓ}. The number of won ballots for\neach case is by deﬁnition ⌊D/t⌋, as the thread queue must\nﬁll upttimes. Thus, π(ℓ,k,t, 1) =O(klog(ℓ)/t). Multiple lanes. Thew > 1 case is complicated by the\nfact that there are joint probabilities to consider (if more\nthan one of the wworkers triggers a sort for a given group,\nonly one sort takes place). However, the likelihood can be\nbounded. Let π′(ℓ,k,t,w ) be the expected won ballots as-\nsuming no mutual interference between the wworkers for\nwinning ballots (i.e., we win bballots if there are b≤w\nworkers that independently win a ballot at a single step),\nbut with the shared min- kset after each sort from the joint\nsequence. Assume that k≥w. Then:\nπ′(ℓ,k,1,w)≤w(⌈k\nw⌉\n+⌈ℓ/w⌉−⌈k/w⌉∑\ni=1k\nw(⌈k/w⌉+i))\n≤wπ(⌈ℓ/w⌉,k,1,1) =O(wklog(ℓ/w))\n(19)\nwhere the likelihood of the wworkers seeing a successive\nmin-kelement has an upper bound of that of the ﬁrst worker\nat each step. As before, the number of won ballots is scaled\nbyt, soπ′(ℓ,k,t,w ) =O(wklog(ℓ/w)/t). Mutual interfer-\nence can only reduce the number of ballots, so we obtain the\nsame upper bound for π(ℓ,k,t,w ). 12",
"Dense Passage Retrieval for Open-Domain Question Answering\nVladimir Karpukhin∗, Barlas O ˘guz∗, Sewon Min†, Patrick Lewis,\nLedell Wu, Sergey Edunov, Danqi Chen‡, Wen-tau Yih\nFacebook AI†University of Washington‡Princeton University\n{vladk, barlaso, plewis, ledell, edunov, scottyih }@fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\nAbstract\nOpen-domain question answering relies on ef-\nﬁcient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented us-\ningdense representations alone, where em-\nbeddings are learned from a small number\nof questions and passages by a simple dual-\nencoder framework. When evaluated on a\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong Lucene-\nBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks.1\n1 Introduction\nOpen-domain question answering (QA) (V oorhees,\n1999) is a task that answers factoid questions us-\ning a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), inter alia ), the advances of reading\ncomprehension models suggest a much simpliﬁed\ntwo-stage framework: (1) a context retriever ﬁrst\nselects a small subset of passages where some\nof them contain the answer to the question, and\nthen (2) a machine reader can thoroughly exam-\nine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\nopen-domain QA to machine reading is a very rea-\nsonable strategy, a huge performance degradation\nis often observed in practice2, indicating the needs\nof improving retrieval. ∗Equal contribution\n1The code and trained models have been released at\nhttps://github.com/facebookresearch/DPR.",
"∗Equal contribution\n1The code and trained models have been released at\nhttps://github.com/facebookresearch/DPR. 2For instance, the exact match score on SQuAD v1.1 drops\nfrom above 80% to less than 40% (Yang et al., 2019a).Retrieval in open-domain QA is usually imple-\nmented using TF-IDF or BM25 (Robertson and\nZaragoza, 2009), which matches keywords efﬁ-\nciently with an inverted index and can be seen\nas representing the question and context in high-\ndimensional, sparse vectors (with weighting). Con-\nversely, the dense , latent semantic encoding is com-\nplementary to sparse representations by design. For\nexample, synonyms or paraphrases that consist of\ncompletely different tokens may still be mapped to\nvectors close to each other. Consider the question\n“Who is the bad guy in lord of the rings?” , which can\nbe answered from the context “Sala Baker is best\nknown for portraying the villain Sauron in the Lord\nof the Rings trilogy. ” A term-based system would\nhave difﬁculty retrieving such a context, while\na dense retrieval system would be able to better\nmatch “bad guy” with “villain” and fetch the cor-\nrect context. Dense encodings are also learnable\nby adjusting the embedding functions, which pro-\nvides additional ﬂexibility to have a task-speciﬁc\nrepresentation. With special in-memory data struc-\ntures and indexing schemes, retrieval can be done\nefﬁciently using maximum inner product search\n(MIPS) algorithms (e.g., Shrivastava and Li (2014);\nGuo et al. (2016)). However, it is generally believed that learn-\ning a good dense vector representation needs a\nlarge number of labeled pairs of question and con-\ntexts. Dense retrieval methods have thus never\nbe shown to outperform TF-IDF/BM25 for open-\ndomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\nobjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then ﬁne-\ntuned using pairs of questions and answers jointly.",
"The\nquestion encoder and the reader model are then ﬁne-\ntuned using pairs of questions and answers jointly. Although ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domainarXiv:2004.04906v3  [cs.CL]  30 Sep 2020",
"QA datasets, it also suffers from two weaknesses. First, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot ﬁne-tuned using pairs of questions and answers,\nthe corresponding representations could be subop-\ntimal. In this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), with-\noutadditional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\nof question and passage pairs. Through a series\nof careful ablation studies, our ﬁnal solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective compar-\ning all pairs of questions and passages in a batch. OurDense Passage Retriever (DPR) is exception-\nally strong. It not only outperforms BM25 by a\nlarge margin (65.2% vs. 42.9% in Top-5 accuracy),\nbut also results in a substantial improvement on\nthe end-to-end QA accuracy compared to ORQA\n(41.5% vs. 33.3%) in the open Natural Questions\nsetting (Lee et al., 2019; Kwiatkowski et al., 2019). Our contributions are twofold. First, we demon-\nstrate that with the proper training setup, sim-\nply ﬁne-tuning the question and passage encoders\non existing question-passage pairs is sufﬁcient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\nopen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve compara-\nble or better results on multiple QA datasets in the\nopen-retrieval setting, compared to several, much\ncomplicated systems.",
"By applying a modern reader model\nto the top retrieved passages, we achieve compara-\nble or better results on multiple QA datasets in the\nopen-retrieval setting, compared to several, much\ncomplicated systems. 2 Background\nThe problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as “ Who ﬁrst voiced Meg on Family\nGuy? ” or “ Where was the 8th Dalai Lama born? ”, a\nsystem is required to answer it using a large corpus\nof diversiﬁed topics. More speciﬁcally, we assumethe extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more pas-\nsages in the corpus. Assume that our collection\ncontainsDdocuments, d1,d2,···,dD. We ﬁrst\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units3and getM\ntotal passages in our corpus C={p1,p2,...,p M},\nwhere each passage pican be viewed as a sequence\nof tokensw(i)\n1,w(i)\n2,···,w(i)\n|pi|. Given a question q,\nthe task is to ﬁnd a span w(i)\ns,w(i)\ns+1,···,w(i)\nefrom\none of the passages pithat can answer the question. Notice that to cover a wide variety of domains, the\ncorpus size can easily range from millions of docu-\nments (e.g., Wikipedia) to billions (e.g., the Web). As a result, any open-domain QA system needs to\ninclude an efﬁcient retriever component that can se-\nlect a small set of relevant texts, before applying the\nreader to extract the answer (Chen et al., 2017).4\nFormally speaking, a retriever R: (q,C)→CF\nis a function that takes as input a question qand a\ncorpusCand returns a much smaller ﬁlter set of\ntextsCF⊂C, where|CF|=k≪|C| . For a ﬁxed\nk, aretriever can be evaluated in isolation on top-k\nretrieval accuracy , which is the fraction of ques-\ntions for whichCFcontains a span that answers the\nquestion. 3 Dense Passage Retriever (DPR)\nWe focus our research in this work on improv-\ning the retrieval component in open-domain QA.",
"3 Dense Passage Retriever (DPR)\nWe focus our research in this work on improv-\ning the retrieval component in open-domain QA. Given a collection of Mtext passages, the goal of\nour dense passage retriever (DPR) is to index all\nthe passages in a low-dimensional and continuous\nspace, such that it can retrieve efﬁciently the top\nkpassages relevant to the input question for the\nreader at run-time. Note that Mcan be very large\n(e.g., 21 million passages in our experiments, de-\nscribed in Section 4.1) and kis usually small, such\nas20–100. 3.1 Overview\nOur dense passage retriever (DPR) uses a dense\nencoderEP(·)which maps any text passage to a d-\ndimensional real-valued vectors and builds an index\nfor all theMpassages that we will use for retrieval. 3The ideal size and boundary of a text passage are func-\ntions of both the retriever and reader. We also experimented\nwith natural paragraphs in our preliminary trials and found that\nusing ﬁxed-length passages performs better in both retrieval\nand ﬁnal QA accuracy, as observed by Wang et al. (2019). 4Exceptions include (Seo et al., 2019) and (Roberts et al.,\n2020), which retrieves andgenerates the answers, respectively.",
"At run-time, DPR applies a different encoder EQ(·)\nthat maps the input question to a d-dimensional\nvector, and retrieves kpassages of which vectors\nare the closest to the question vector. We deﬁne\nthe similarity between the question and the passage\nusing the dot product of their vectors:\nsim(q,p) =EQ(q)⊺EP(p). (1)\nAlthough more expressive model forms for measur-\ning the similarity between a question and a passage\ndo exist, such as networks consisting of multiple\nlayers of cross attentions, the similarity function\nneeds to be decomposable so that the represen-\ntations of the collection of passages can be pre-\ncomputed. Most decomposable similarity functions\nare some transformations of Euclidean distance\n(L2). For instance, cosine is equivalent to inner\nproduct for unit vectors and the Mahalanobis dis-\ntance is equivalent to L2 distance in a transformed\nspace. Inner product search has been widely used\nand studied, as well as its connection to cosine\nsimilarity and L2 distance (Mussmann and Ermon,\n2016; Ram and Gray, 2012). As our ablation study\nﬁnds other similarity functions perform compara-\nbly (Section 5.2; Appendix B), we thus choose\nthe simpler inner product function and improve the\ndense passage retriever by learning better encoders. Encoders Although in principle the question and\npassage encoders can be implemented by any neu-\nral networks, in this work we use two independent\nBERT (Devlin et al., 2019) networks (base, un-\ncased) and take the representation at the [CLS]\ntoken as the output, so d= 768 . Inference During inference time, we apply the\npassage encoder EPto all the passages and index\nthem using FAISS (Johnson et al., 2017) ofﬂine. FAISS is an extremely efﬁcient, open-source li-\nbrary for similarity search and clustering of dense\nvectors, which can easily be applied to billions of\nvectors. Given a question qat run-time, we derive\nits embedding vq=EQ(q)and retrieve the top k\npassages with embeddings closest to vq. 3.2 Training\nTraining the encoders so that the dot-product sim-\nilarity (Eq.",
"Given a question qat run-time, we derive\nits embedding vq=EQ(q)and retrieve the top k\npassages with embeddings closest to vq. 3.2 Training\nTraining the encoders so that the dot-product sim-\nilarity (Eq. (1)) becomes a good ranking function\nfor retrieval is essentially a metric learning prob-\nlem (Kulis, 2013). The goal is to create a vector\nspace such that relevant pairs of questions and pas-\nsages will have smaller distance (i.e., higher simi-larity) than the irrelevant ones, by learning a better\nembedding function. LetD={⟨qi,p+\ni,p−\ni,1,···,p−\ni,n⟩}m\ni=1be the\ntraining data that consists of minstances. Each\ninstance contains one question qiand one relevant\n(positive) passage p+\ni, along with nirrelevant (neg-\native) passages p−\ni,j. We optimize the loss function\nas the negative log likelihood of the positive pas-\nsage:\nL(qi,p+\ni,p−\ni,1,···,p−\ni,n) (2)\n=−logesim(qi,p+\ni)\nesim(qi,p+\ni)+∑n\nj=1esim(qi,p−\ni,j). Positive and negative passages For retrieval\nproblems, it is often the case that positive examples\nare available explicitly, while negative examples\nneed to be selected from an extremely large pool. For instance, passages relevant to a question may\nbe given in a QA dataset, or can be found using the\nanswer. All other passages in the collection, while\nnot speciﬁed explicitly, can be viewed as irrelevant\nby default. In practice, how to select negative ex-\namples is often overlooked but could be decisive\nfor learning a high-quality encoder. We consider\nthree different types of negatives: (1) Random: any\nrandom passage from the corpus; (2) BM25: top\npassages returned by BM25 which don’t contain\nthe answer but match most question tokens; (3)\nGold: positive passages paired with other questions\nwhich appear in the training set. We will discuss the\nimpact of different types of negative passages and\ntraining schemes in Section 5.2. Our best model\nuses gold passages from the same mini-batch and\none BM25 negative passage.",
"We will discuss the\nimpact of different types of negative passages and\ntraining schemes in Section 5.2. Our best model\nuses gold passages from the same mini-batch and\none BM25 negative passage. In particular, re-using\ngold passages from the same batch as negatives\ncan make the computation efﬁcient while achiev-\ning great performance. We discuss this approach\nbelow. In-batch negatives Assume that we have B\nquestions in a mini-batch and each one is asso-\nciated with a relevant passage. Let QandPbe the\n(B×d)matrix of question and passage embeddings\nin a batch of size B.S=QPTis a(B×B)ma-\ntrix of similarity scores, where each row of which\ncorresponds to a question, paired with Bpassages. In this way, we reuse computation and effectively\ntrain onB2(qi,pj) question/passage pairs in each\nbatch. Any ( qi,pj) pair is a positive example when\ni=j, and negative otherwise. This creates Btrain-\ning instances in each batch, where there are B−1",
"negative passages for each question. The trick of in-batch negatives has been used in\nthe full batch setting (Yih et al., 2011) and more\nrecently for mini-batch (Henderson et al., 2017;\nGillick et al., 2019). It has been shown to be an\neffective strategy for learning a dual-encoder model\nthat boosts the number of training examples. 4 Experimental Setup\nIn this section, we describe the data we used for\nexperiments and the basic setup. 4.1 Wikipedia Data Pre-processing\nFollowing (Lee et al., 2019), we use the English\nWikipedia dump from Dec. 20, 2018 as the source\ndocuments for answering questions. We ﬁrst apply\nthe pre-processing code released in DrQA (Chen\net al., 2017) to extract the clean, text-portion of\narticles from the Wikipedia dump. This step re-\nmoves semi-structured data, such as tables, info-\nboxes, lists, as well as the disambiguation pages. We then split each article into multiple, disjoint text\nblocks of 100 words as passages , serving as our\nbasic retrieval units, following (Wang et al., 2019),\nwhich results in 21,015,324 passages in the end.5\nEach passage is also prepended with the title of the\nWikipedia article where the passage is from, along\nwith an [SEP] token. 4.2 Question Answering Datasets\nWe use the same ﬁve QA datasets and train-\ning/dev/testing splitting method as in previous\nwork (Lee et al., 2019). Below we brieﬂy describe\neach dataset and refer readers to their paper for the\ndetails of data preparation. Natural Questions (NQ) (Kwiatkowski et al.,\n2019) was designed for end-to-end question an-\nswering. The questions were mined from real\nGoogle search queries and the answers were spans\nin Wikipedia articles identiﬁed by annotators. TriviaQA (Joshi et al., 2017) contains a set of trivia\nquestions with answers that were originally scraped\nfrom the Web. WebQuestions (WQ) (Berant et al., 2013) consists\nof questions selected using Google Suggest API,\nwhere the answers are entities in Freebase. CuratedTREC (TREC) (Baudi ˇs and ˇSediv `y,\n2015) sources questions from TREC QA tracks\n5However, Wang et al.",
"WebQuestions (WQ) (Berant et al., 2013) consists\nof questions selected using Google Suggest API,\nwhere the answers are entities in Freebase. CuratedTREC (TREC) (Baudi ˇs and ˇSediv `y,\n2015) sources questions from TREC QA tracks\n5However, Wang et al. (2019) also propose splitting docu-\nments into overlapping passages, which we do not ﬁnd advan-\ntageous compared to the non-overlapping version.Dataset Train Dev Test\nNatural Questions 79,168 58,880 8,757 3,610\nTriviaQA 78,785 60,413 8,837 11,313\nWebQuestions 3,417 2,474 361 2,032\nCuratedTREC 1,353 1,125 133 694\nSQuAD 78,713 70,096 8,886 10,570\nTable 1: Number of questions in each QA dataset. The\ntwo columns of Train denote the original training ex-\namples in the dataset and the actual questions used for\ntraining DPR after ﬁltering. See text for more details. as well as various Web sources and is intended for\nopen-domain QA from unstructured corpora. SQuAD v1.1 (Rajpurkar et al., 2016) is a popu-\nlar benchmark dataset for reading comprehension. Annotators were presented with a Wikipedia para-\ngraph, and asked to write questions that could be\nanswered from the given text. Although SQuAD\nhas been used previously for open-domain QA re-\nsearch, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1. Selection of positive passages Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA6, we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the ques-\ntion will be discarded. For SQuAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool.7We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing.",
"Table 1 shows the num-\nber of questions in training/dev/test sets for all the\ndatasets and the actual questions used for training\nthe retriever. 5 Experiments: Passage Retrieval\nIn this section, we evaluate the retrieval perfor-\nmance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n6We use the unﬁltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing. 7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Ap-\npendix A.",
"Training Retriever Top-20 Top-100\nNQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD\nNone BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0\nSingleDPR 78.4 79.4 73.2 79.8 63.2 85.4 85.0 81.4 89.1 77.2\nBM25 + DPR 76.6 79.8 71.0 85.2 71.5 83.8 84.5 80.5 92.7 81.3\nMultiDPR 79.4 78.8 75.0 89.1 51.6 86.0 84.7 82.9 93.9 67.6\nBM25 + DPR 78.0 79.9 74.7 88.5 66.2 83.9 84.4 82.3 94.1 78.6\nTable 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved\npassages that contain the answer. Single andMulti denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQuAD). See text for more details. traditional retrieval methods, the effects of different\ntraining schemes and the run-time efﬁciency. The DPR model used in our main experiments\nis trained using the in-batch negative setting (Sec-\ntion 3.2) with a batch size of 128and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQuAD)\nand100epochs for small datasets (TREC, WQ),\nwith a learning rate of 10−5using Adam, linear\nscheduling with warm-up and dropout rate 0.1. While it is good to have the ﬂexibility to adapt\nthe retriever to each dataset, it would also be de-\nsirable to obtain a single retriever that works well\nacross the board. To this end, we train a multi -\ndataset encoder by combining training data from\nall datasets excluding SQuAD.8In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method9and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Speciﬁcally, we obtain two initial sets\nof top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25(q,p) +λ·sim(q,p)as the ranking function. We usedλ= 1.1based on the retrieval accuracy in\nthe development set.",
"Speciﬁcally, we obtain two initial sets\nof top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25(q,p) +λ·sim(q,p)as the ranking function. We usedλ= 1.1based on the retrieval accuracy in\nthe development set. 5.1 Main Results\nTable 2 compares different passage retrieval sys-\ntems on ﬁve QA datasets, using the top- kaccuracy\n(k∈{20,100}). With the exception of SQuAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when kis\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\non Natural Questions). When training with mul-\n8SQuAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\nmore in Section 5.1. 9Lucene implementation. BM25 parameters b= 0.4(doc-\nument length normalization) and k1= 0.9(term frequency\nscaling) are tuned using development sets. 20 40 60 80 100\nk: # of retrieved passages405060708090Top-k accuracy (%)\nBM25",
"bers of training examples used in our dense passage re-\ntriever vs BM25. The results are measured on the de-\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25. tiple datasets, TREC, the smallest dataset of the\nﬁve, beneﬁts greatly from more training examples. In contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly. Results can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings. We conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annota-\ntors wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and questions, which gives BM25 a clear\nadvantage. Second, the data was collected from\nonly 500+ Wikipedia articles and thus the distribu-\ntion of training examples is extremely biased, as\nargued previously by Lee et al. (2019). 5.2 Ablation Study on Model Training\nTo understand further how different model training\noptions affect the results, we conduct several addi-\ntional experiments and discuss our ﬁndings below.",
"Sample efﬁciency We explore how many train-\ning examples are needed to achieve good passage\nretrieval performance. Figure 1 illustrates the top- k\nretrieval accuracy with respect to different num-\nbers of training examples, measured on the devel-\nopment set of Natural Questions. As is shown, a\ndense passage retriever trained using only 1,000 ex-\namples already outperforms BM25. This suggests\nthat with a general pretrained language model, it is\npossible to train a high-quality dense retriever with\na small number of question–passage pairs. Adding\nmore training examples (from 1k to 59k) further\nimproves the retrieval accuracy consistently. In-batch negative training We test different\ntraining schemes on the development set of Natural\nQuestions and summarize the results in Table 3. The top block is the standard 1-of- Ntraining set-\nting, where each question in the batch is paired\nwith a positive passage and its own set of nneg-\native passages (Eq. (2)). We ﬁnd that the choice\nof negatives — random, BM25 or gold passages\n(positive passages from other questions) — does\nnot impact the top- kaccuracy much in this setting\nwhenk≥20. The middle bock is the in-batch negative training\n(Section 3.2) setting. We ﬁnd that using a similar\nconﬁguration (7 gold negative passages), in-batch\nnegative training improves the results substantially. The key difference between the two is whether the\ngold negative passages come from the same batch\nor from the whole training set. Effectively, in-batch\nnegative training is an easy and memory-efﬁcient\nway to reuse the negative examples already in the\nbatch rather than creating new ones. It produces\nmore pairs and thus increases the number of train-\ning examples, which might contribute to the good\nmodel performance. As a result, accuracy consis-\ntently improves as the batch size grows. Finally, we explore in-batch negative training\nwith additional “hard” negative passages that have\nhigh BM25 scores given the question, but do not\ncontain the answer string (the bottom block).",
"As a result, accuracy consis-\ntently improves as the batch size grows. Finally, we explore in-batch negative training\nwith additional “hard” negative passages that have\nhigh BM25 scores given the question, but do not\ncontain the answer string (the bottom block). These\nadditional passages are used as negative passages\nfor all questions in the same batch. We ﬁnd that\nadding a single BM25 negative passage improves\nthe result substantially while adding two does not\nhelp further. Impact of gold passages We use passages that\nmatch the gold contexts in the original datasets\n(when available) as positive examples (Section 4.2).Type #N IB Top-5 Top-20 Top-100\nRandom 7 \u0017 47.0 64.3 77.8\nBM25 7 \u0017 50.0 63.3 74.8\nGold 7 \u0017 42.6 63.1 78.3\nGold 7 \u0013 51.1 69.1 80.8\nGold 31 \u0013 52.1 70.8 82.1\nGold 127 \u0013 55.8 73.0 83.1\nG.+BM25(1)31+32 \u0013 65.0 77.3 84.4\nG.+BM25(2)31+64 \u0013 64.5 76.4 84.0\nG.+BM25(1)127+128 \u0013 65.8 78.0 84.9\nTable 3: Comparison of different training schemes,\nmeasured as top- kretrieval accuracy on Natural Ques-\ntions (development set). #N: number of negative\nexamples, IB: in-batch training. G.+BM25(1)and\nG.+BM25(2)denote in-batch training with 1 or 2 ad-\nditional BM25 negatives, which serve as negative pas-\nsages for all questions in the batch. Our experiments on Natural Questions show that\nswitching to distantly-supervised passages (using\nthe highest-ranked BM25 passage that contains the\nanswer), has only a small impact: 1 point lower\ntop-kaccuracy for retrieval. Appendix A contains\nmore details. Similarity and loss Besides dot product, cosine\nand Euclidean L2 distance are also commonly used\nas decomposable similarity functions. We test these\nalternatives and ﬁnd that L2 performs compara-\nble to dot product, and both of them are superior\nto cosine. Similarly, in addition to negative log-\nlikelihood, a popular option for ranking is triplet\nloss, which compares a positive passage and a nega-\ntive one directly with respect to a question (Burges\net al., 2005). Our experiments show that using\ntriplet loss does not affect the results much.",
"Our experiments show that using\ntriplet loss does not affect the results much. More\ndetails can be found in Appendix B. Cross-dataset generalization One interesting\nquestion regarding DPR’s discriminative training\nis how much performance degradation it may suf-\nfer from a non-iid setting. In other words, can\nit still generalize well when directly applied to\na different dataset without additional ﬁne-tuning? To test the cross-dataset generalization, we train\nDPR on Natural Questions only and test it directly\non the smaller WebQuestions and CuratedTREC\ndatasets. We ﬁnd that DPR generalizes well, with\n3-5 points loss from the best performing ﬁne-tuned\nmodel in top-20 retrieval accuracy (69.9/86.3 vs. 75.0/89.1 for WebQuestions and TREC, respec-\ntively), while still greatly outperforming the BM25\nbaseline (55.0/70.9).",
"5.3 Qualitative Analysis\nAlthough DPR performs better than BM25 in gen-\neral, passages retrieved by these two methods dif-\nfer qualitatively. Term-matching methods like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\nor semantic relationships better. See Appendix C\nfor examples and more discussion. 5.4 Run-time Efﬁciency\nThe main reason that we require a retrieval compo-\nnent for open-domain QA is to reduce the number\nof candidate passages that the reader needs to con-\nsider, which is crucial for answering user’s ques-\ntions in real-time. We proﬁled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors10,\nDPR can be made incredibly efﬁcient, processing\n995.0 questions per second, returning top 100 pas-\nsages per question. In contrast, BM25/Lucene (im-\nplemented in Java, using ﬁle index) processes 23.7\nquestions per second per CPU thread. On the other hand, the time required for building\nan index for dense vectors is much longer. Com-\nputing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\non a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total. 6 Experiments: Question Answering\nIn this section, we experiment with how different\npassage retrievers affect the ﬁnal QA accuracy. 6.1 End-to-end QA System\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA sys-\ntem consists of a neural reader that outputs the\nanswer to the question. Given the top kretrieved\npassages (up to 100in our experiments), the reader\nassigns a passage selection score to each passage. In addition, it extracts an answer span from each\npassage and assigns a span score.",
"Given the top kretrieved\npassages (up to 100in our experiments), the reader\nassigns a passage selection score to each passage. In addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n10FAISS conﬁguration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.score is chosen as the ﬁnal answer. The passage\nselection model serves as a reranker through cross-\nattention between the question and the passage. Al-\nthough cross-attention is not feasible for retrieving\nrelevant passages in a large corpus due to its non-\ndecomposable nature, it has more capacity than the\ndual-encoder model sim(q,p)as in Eq. (1). Apply-\ning it to selecting the passage from a small number\nof retrieved candidates has been shown to work\nwell (Wang et al., 2019, 2018; Lin et al., 2018). Speciﬁcally, let Pi∈RL×h(1≤i≤k) be\na BERT (base, uncased in our experiments) rep-\nresentation for the i-th passage, where Lis the\nmaximum length of the passage and hthe hidden\ndimension. The probabilities of a token being the\nstarting/ending positions of an answer span and a\npassage being selected are deﬁned as:\nPstart,i(s) = softmax(\nPiwstart)\ns, (3)\nPend,i(t) = softmax(\nPiwend)\nt, (4)\nPselected (i) = softmax(ˆP⊺wselected)\ni,(5)\nwhere ˆP= [P[CLS]\n1,...,P[CLS]\nk]∈Rh×kand\nwstart,wend,wselected∈Rhare learnable vectors. We compute a span score of the s-th tot-th words\nfrom thei-th passage as Pstart,i(s)×Pend,i(t), and\na passage selection score of the i-th passage as\nPselected (i). During training, we sample one positive and\n˜m−1negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. ˜mis a hyper-parameter and we\nuse˜m= 24 in all the experiments.",
"During training, we sample one positive and\n˜m−1negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. ˜mis a hyper-parameter and we\nuse˜m= 24 in all the experiments. The training ob-\njective is to maximize the marginal log-likelihood\nof all the correct answer spans in the positive pas-\nsage (the answer string may appear multiple times\nin one passage), combined with the log-likelihood\nof the positive passage being selected. We use the\nbatch size of 16 for large (NQ, TriviaQA, SQuAD)\nand 4 for small (TREC, WQ) datasets, and tune k\non the development set. For experiments on small\ndatasets under the Multi setting, in which using\nother datasets is allowed, we ﬁne-tune the reader\ntrained on Natural Questions to the target dataset. All experiments were done on eight 32GB GPUs. 6.2 Results\nTable 4 summarizes our ﬁnal end-to-end QA re-\nsults, measured by exact match with the reference\nanswer after minor normalization as in (Chen et al.,\n2017; Lee et al., 2019). From the table, we can",
"Training Model NQ TriviaQA WQ TREC SQuAD\nSingle BM25+BERT (Lee et al., 2019) 26.5 47.1 17.7 21.3 33.2\nSingle ORQA (Lee et al., 2019) 33.3 45.0 36.4 30.1 20.2\nSingle HardEM (Min et al., 2019a) 28.1 50.9 - - -\nSingle GraphRetriever (Min et al., 2019b) 34.5 56.0 36.4 - -\nSingle PathRetriever (Asai et al., 2020) 32.6 - - - 56.5\nSingle REALM Wiki(Guu et al., 2020) 39.2 - 40.2 46.8 -\nSingle REALM News (Guu et al., 2020) 40.4 - 40.7 42.9 -\nSingleBM25 32.6 52.4 29.9 24.9 38.1\nDPR 41.5 56.8 34.6 25.9 29.8\nBM25+DPR 39.0 57.0 35.2 28.0 36.7\nMultiDPR 41.5 56.8 42.4 49.4 24.1\nBM25+DPR 38.8 57.9 41.1 50.6 35.8\nTable 4: End-to-end QA (Exact Match) Accuracy. The ﬁrst block of results are copied from their cited papers. REALM Wikiand REALM Newsare the same model but pretrained on Wikipedia and CC-News, respectively. Single\nandMulti denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets\n(all except SQuAD). For WQ and TREC in the Multi setting, we ﬁne-tune the reader trained on NQ. see that higher retriever accuracy typically leads to\nbetter ﬁnal QA results: in all cases except SQuAD,\nanswers extracted from the passages retrieved by\nDPR are more likely to be correct, compared to\nthose from BM25. For large datasets like NQ and\nTriviaQA, models trained using multiple datasets\n(Multi) perform comparably to those trained using\nthe individual training set (Single). Conversely,\non smaller datasets like WQ and TREC, the multi-\ndataset setting has a clear advantage. Overall, our\nDPR-based models outperform the previous state-\nof-the-art results on four out of the ﬁve datasets,\nwith 1% to 12% absolute differences in exact match\naccuracy. It is interesting to contrast our results to\nthose of ORQA (Lee et al., 2019) and also the\nconcurrently developed approach, REALM (Guu\net al., 2020).",
"It is interesting to contrast our results to\nthose of ORQA (Lee et al., 2019) and also the\nconcurrently developed approach, REALM (Guu\net al., 2020). While both methods include addi-\ntional pretraining tasks and employ an expensive\nend-to-end training regime, DPR manages to out-\nperform them on both NQ and TriviaQA, simply\nby focusing on learning a strong passage retrieval\nmodel using pairs of questions and answers. The\nadditional pretraining tasks are likely more useful\nonly when the target training sets are small. Al-\nthough the results of DPR on WQ and TREC in the\nsingle-dataset setting are less competitive, adding\nmore question–answer pairs helps boost the perfor-\nmance, achieving the new state of the art. To compare our pipeline training approach with\njoint learning, we run an ablation on Natural Ques-\ntions where the retriever and reader are jointlytrained, following Lee et al. (2019). This approach\nobtains a score of 39.8 EM, which suggests that our\nstrategy of training a strong retriever and reader in\nisolation can leverage effectively available supervi-\nsion, while outperforming a comparable joint train-\ning approach with a simpler design (Appendix D). One thing worth noticing is that our reader does\nconsider more passages compared to ORQA, al-\nthough it is not completely clear how much more\ntime it takes for inference. While DPR processes\nup to 100 passages for each question, the reader\nis able to ﬁt all of them into one batch on a sin-\ngle 32GB GPU, thus the latency remains almost\nidentical to the single passage case (around 20ms). The exact impact on throughput is harder to mea-\nsure: ORQA uses 2-3x longer passages compared\nto DPR (288 word pieces compared to our 100\ntokens) and the computational complexity is super-\nlinear in passage length. We also note that we\nfoundk= 50 to be optimal for NQ, and k= 10\nleads to only marginal loss in exact match accu-\nracy (40.8 vs. 41.5 EM on NQ), which should be\nroughly comparable to ORQA’s 5-passage setup.",
"We also note that we\nfoundk= 50 to be optimal for NQ, and k= 10\nleads to only marginal loss in exact match accu-\nracy (40.8 vs. 41.5 EM on NQ), which should be\nroughly comparable to ORQA’s 5-passage setup. 7 Related Work\nPassage retrieval has been an important compo-\nnent for open-domain QA (V oorhees, 1999). It\nnot only effectively reduces the search space for\nanswer extraction, but also identiﬁes the support\ncontext for users to verify the answer. Strong sparse\nvector space models like TF-IDF or BM25 have",
"been used as the standard method applied broadly\nto various QA tasks (e.g., Chen et al., 2017; Yang\net al., 2019a,b; Nie et al., 2019; Min et al., 2019a;\nWolfson et al., 2020). Augmenting text-based re-\ntrieval with external structured information, such\nas knowledge graph and Wikipedia hyperlinks, has\nalso been explored recently (Min et al., 2019b; Asai\net al., 2020). The use of dense vector representations for re-\ntrieval has a long history since Latent Semantic\nAnalysis (Deerwester et al., 1990). Using labeled\npairs of queries and documents, discriminatively\ntrained dense encoders have become popular re-\ncently (Yih et al., 2011; Huang et al., 2013; Gillick\net al., 2019), with applications to cross-lingual\ndocument retrieval, ad relevance prediction, Web\nsearch and entity retrieval. Such approaches com-\nplement the sparse vector methods as they can po-\ntentially give high similarity scores to semantically\nrelevant text pairs, even without exact token match-\ning. The dense representation alone, however, is\ntypically inferior to the sparse one. While not the\nfocus of this work, dense representations from pre-\ntrained models, along with cross-attention mecha-\nnisms, have also been shown effective in passage\nor dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates\nthe feasibility of full dense retrieval in IR tasks. Instead of employing the dual-encoder framework,\nthey introduced a late-interaction operator on top\nof the BERT encoders. Dense retrieval for open-domain QA has been\nexplored by Das et al. (2019), who propose to re-\ntrieve relevant passages iteratively using reformu-\nlated question vectors. As an alternative approach\nthat skips passage retrieval, Seo et al. (2019) pro-\npose to encode candidate answer phrases as vectors\nand directly retrieve the answers to the input ques-\ntions efﬁciently. Using additional pretraining with\nthe objective that matches surrogates of questions\nand relevant passages, Lee et al.",
"(2019) pro-\npose to encode candidate answer phrases as vectors\nand directly retrieve the answers to the input ques-\ntions efﬁciently. Using additional pretraining with\nthe objective that matches surrogates of questions\nand relevant passages, Lee et al. (2019) jointly train\nthe question encoder and reader. Their approach\noutperforms the BM25 plus reader paradigm on\nmultiple open-domain QA datasets in QA accuracy,\nand is further extended by REALM (Guu et al.,\n2020), which includes tuning the passage encoder\nasynchronously by re-indexing the passages dur-\ning training. The pretraining objective has also\nrecently been improved by Xiong et al. (2020b). In contrast, our model provides a simple and yeteffective solution that shows stronger empirical per-\nformance, without relying on additional pretraining\nor complex joint training schemes. DPR has also been used as an important mod-\nule in very recent work. For instance, extending\nthe idea of leveraging hard negatives, Xiong et al. (2020a) use the retrieval model trained in the pre-\nvious iteration to discover new negatives and con-\nstruct a different set of examples in each training\niteration. Starting from our trained DPR model,\nthey show that the retrieval performance can be\nfurther improved. Recent work (Izacard and Grave,\n2020; Lewis et al., 2020b) have also shown that\nDPR can be combined with generation models\nsuch as BART (Lewis et al., 2020a) and T5 (Raf-\nfel et al., 2019), achieving good performance on\nopen-domain QA and other knowledge-intensive\ntasks. 8 Conclusion\nIn this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the tradi-\ntional sparse retrieval component in open-domain\nquestion answering. While a simple dual-encoder\napproach can be made to work surprisingly well,\nwe showed that there are some critical ingredients\nto training a dense retriever successfully. Moreover,\nour empirical analysis and ablation studies indicate\nthat more complex model frameworks or similarity\nfunctions do not necessarily provide additional val-\nues.",
"Moreover,\nour empirical analysis and ablation studies indicate\nthat more complex model frameworks or similarity\nfunctions do not necessarily provide additional val-\nues. As a result of improved retrieval performance,\nwe obtained new state-of-the-art results on multiple\nopen-domain question answering benchmarks. Acknowledgments\nWe thank the anonymous reviewers for their helpful\ncomments and suggestions. References\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learn-\ning to retrieve reasoning paths over Wikipedia graph\nfor question answering. In International Conference\non Learning Representations (ICLR) . Petr Baudi ˇs and Jan ˇSediv `y. 2015. Modeling of the\nquestion answering task in the yodaqa system. In In-\nternational Conference of the Cross-Language Eval-\nuation Forum for European Languages , pages 222–\n228. Springer. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from",
"question-answer pairs. In Empirical Methods in Nat-\nural Language Processing (EMNLP) . Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard\nS¨ackinger, and Roopak Shah. 1994. Signature veriﬁ-\ncation using a “Siamese” time delay neural network. InNIPS , pages 737–744. Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,\nMatt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In\nProceedings of the 22nd international conference on\nMachine learning , pages 89–96. Danqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Association for Computa-\ntional Linguistics (ACL) , pages 1870–1879. Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nand Andrew McCallum. 2019. Multi-step retriever-\nreader interaction for scalable open-domain question\nanswering. In International Conference on Learn-\ning Representations (ICLR) . Scott Deerwester, Susan T Dumais, George W Fur-\nnas, Thomas K Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Jour-\nnal of the American society for information science ,\n41(6):391–407. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL) . David A Ferrucci. 2012. Introduction to “This is Wat-\nson”. IBM Journal of Research and Development ,\n56(3.4):1–1. Daniel Gillick, Sayali Kulkarni, Larry Lansing,\nAlessandro Presta, Jason Baldridge, Eugene Ie, and\nDiego Garcia-Olano. 2019. Learning dense repre-\nsentations for entity retrieval. In Computational Nat-\nural Language Learning (CoNLL) . Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and\nDavid Simcha. 2016. Quantization based fast inner\nproduct search. In Artiﬁcial Intelligence and Statis-\ntics, pages 482–490. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020. REALM:\nRetrieval-augmented language model pre-training. ArXiv , abs/2002.08909.",
"2016. Quantization based fast inner\nproduct search. In Artiﬁcial Intelligence and Statis-\ntics, pages 482–490. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020. REALM:\nRetrieval-augmented language model pre-training. ArXiv , abs/2002.08909. Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-\nhsuan Sung, L ´aszl´o Luk ´acs, Ruiqi Guo, Sanjiv Ku-\nmar, Balint Miklos, and Ray Kurzweil. 2017. Efﬁ-\ncient natural language response suggestion for smart\nreply. ArXiv , abs/1705.00652. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for Web search usingclickthrough data. In ACM International Confer-\nence on Information and Knowledge Management\n(CIKM) , pages 2333–2338. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In International Conference\non Learning Representations (ICLR) . Gautier Izacard and Edouard Grave. 2020. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. ArXiv , abs/2007.01282. Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017. Billion-scale similarity search with GPUs. ArXiv ,\nabs/1702.08734. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Association for Computational Lin-\nguistics (ACL) , pages 1601–1611. Omar Khattab and Matei Zaharia. 2020. ColBERT:\nEfﬁcient and effective passage search via contextu-\nalized late interaction over BERT. In ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval (SIGIR) , pages 39–48. Brian Kulis. 2013. Metric learning: A survey. Foun-\ndations and Trends in Machine Learning , 5(4):287–\n364. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N.",
"Foun-\ndations and Trends in Machine Learning , 5(4):287–\n364. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics (TACL) . Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.",
"domain question answering. In Association for Com-\nputational Linguistics (ACL) , pages 6086–6096. Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Association for Computa-\ntional Linguistics (ACL) , pages 7871–7880. Patrick Lewis, Ethan Perez, Aleksandara Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K ¨uttler, Mike Lewis, Wen-tau Yih,\nTim Rockt ¨aschel, Sebastian Riedel, and Douwe\nKiela. 2020b. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in\nNeural Information Processing Systems (NeurIPS) .",
"question answering. In Association for Computa-\ntional Linguistics (ACL) , pages 1736–1745. Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\nproach for weakly supervised question answering. InEmpirical Methods in Natural Language Process-\ning (EMNLP) . Sewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\ntrieval and reading for open domain question answer-\ning. ArXiv , abs/1911.03868. Dan Moldovan, Marius Pas ¸ca, Sanda Harabagiu, and\nMihai Surdeanu. 2003. Performance issues and er-\nror analysis in an open-domain question answering\nsystem. ACM Transactions on Information Systems\n(TOIS) , 21(2):133–154. Stephen Mussmann and Stefano Ermon. 2016. Learn-\ning and inference via maximum inner product search. InInternational Conference on Machine Learning\n(ICML) , pages 2587–2596. Yixin Nie, Songhe Wang, and Mohit Bansal. 2019. Re-\nvealing the importance of semantic retrieval for ma-\nchine reading at scale. In Empirical Methods in Nat-\nural Language Processing (EMNLP) . Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage\nre-ranking with BERT. ArXiv , abs/1901.04085. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. ArXiv , abs/1910.10683. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 2383–2392. Parikshit Ram and Alexander G Gray. 2012. Maximum\ninner-product search using cone trees. In Proceed-\nings of the 18th ACM SIGKDD international con-\nference on Knowledge discovery and data mining ,\npages 931–939. Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param-\neters of a language model? ArXiv , abs/2002.08910. Stephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval , 3(4):333–389.",
"2020. How much knowledge can you pack into the param-\neters of a language model? ArXiv , abs/2002.08910. Stephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval , 3(4):333–389. Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur\nParikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-time open-domain question answering with\ndense-sparse phrase index. In Association for Com-\nputational Linguistics (ACL) .Anshumali Shrivastava and Ping Li. 2014. Asymmet-\nric LSH (ALSH) for sublinear time maximum inner\nproduct search (MIPS). In Advances in Neural In-\nformation Processing Systems (NIPS) , pages 2321–\n2329. Ellen M V oorhees. 1999. The TREC-8 question an-\nswering track report. In TREC , volume 99, pages\n77–82. Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,\nTim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018. Rˆ3:\nReinforced ranker-reader for open-domain question\nanswering. In Conference on Artiﬁcial Intelligence\n(AAAI) . Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nalla-\npati, and Bing Xiang. 2019. Multi-passage BERT:\nA globally normalized bert model for open-domain\nquestion answering. In Empirical Methods in Natu-\nral Language Processing (EMNLP) . Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-\nner, Yoav Goldberg, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question under-\nstanding benchmark. Transactions of the Associa-\ntion of Computational Linguistics (TACL) . Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020a. Approximate nearest neighbor\nnegative contrastive learning for dense text retrieval. ArXiv , abs/2007.00808. Wenhan Xiong, Hankang Wang, and William Yang\nWang. 2020b. Progressively pretrained dense corpus\nindex for open-domain question answering. ArXiv ,\nabs/2005.00038. Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a. End-to-end open-domain question answering with\nbertserini.",
"2020b. Progressively pretrained dense corpus\nindex for open-domain question answering. ArXiv ,\nabs/2005.00038. Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a. End-to-end open-domain question answering with\nbertserini. In North American Association for Com-\nputational Linguistics (NAACL) , pages 72–77. Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming\nLi, and Jimmy Lin. 2019b. Data augmentation for\nbert ﬁne-tuning in open-domain question answering. ArXiv , abs/1904.06652. Wen-tau Yih, Kristina Toutanova, John C Platt, and\nChristopher Meek. 2011. Learning discriminative\nprojections for text similarity measures. In Com-\nputational Natural Language Learning (CoNLL) ,\npages 247–256.",
"A Distant Supervision\nWhen training our ﬁnal DPR model using Natural\nQuestions, we use the passages in our collection\nthat best match the gold context as the positive\npassages. As some QA datasets contain only the\nquestion and answer pairs, it is thus interesting\nto see when using the passages that contain the\nanswers as positives (i.e., the distant supervision\nsetting), whether there is a signiﬁcant performance\ndegradation. Using the question and answer to-\ngether as the query, we run Lucene-BM25 and pick\nthe top passage that contains the answer as the pos-\nitive passage. Table 5 shows the performance of\nDPR when trained using the original setting and\nthe distant supervision setting. B Alternative Similarity Functions &\nTriplet Loss\nIn addition to dot product (DP) and negative log-\nlikelihood based on softmax (NLL), we also exper-\niment with Euclidean distance (L2) and the triplet\nloss. We negate L2 similarity scores before ap-\nplying softmax and change signs of question-to-\npositive and question-to-negative similarities when\napplying the triplet loss on dot product scores. The\nmargin value of the triplet loss is set to 1. Ta-\nble 6 summarizes the results. All these additional\nexperiments are conducted using the same hyper-\nparameters tuned for the baseline (DP, NLL). Note that the retrieval accuracy for our “baseline”\nsettings reported in Table 5 (Gold) and Table 6\n(DP, NLL) is slightly better than those reported in\nTable 3. This is due to a better hyper-parameter\nsetting used in these analysis experiments, which\nis documented in our code release. C Qualitative Analysis\nAlthough DPR performs better than BM25 in gen-\neral, the retrieved passages of these two retrievers\nactually differ qualitatively. Methods like BM25\nare sensitive to highly selective keywords and\nphrases, but cannot capture lexical variations or se-\nmantic relationships well. In contrast, DPR excels\nat semantic representation, but might lack sufﬁcient\ncapacity to represent salient phrases which appear\nrarely. Table 7 illustrates this phenomenon with\ntwo examples.",
"In contrast, DPR excels\nat semantic representation, but might lack sufﬁcient\ncapacity to represent salient phrases which appear\nrarely. Table 7 illustrates this phenomenon with\ntwo examples. In the ﬁrst example, the top scor-\ning passage from BM25 is irrelevant, even though\nkeywords such as England andIreland appear mul-\ntiple times. In comparison, DPR is able to returnTop-1 Top-5 Top-20 Top-100\nGold 44.9 66.8 78.1 85.0\nDist. Sup. 43.9 65.3 77.1 84.4\nTable 5: Retrieval accuracy on the development set of\nNatural Questions, trained on passages that match the\ngold context (Gold) or the top BM25 passage that con-\ntains the answer (Dist. Sup.). Sim Loss Retrieval Accuracy\nTop-1 Top-5 Top-20 Top-100\nDPNLL 44.9 66.8 78.1 85.0\nTriplet 41.6 65.0 77.2 84.5\nL2NLL 43.5 64.7 76.1 83.1\nTriplet 42.2 66.0 78.1 84.9\nTable 6: Retrieval Top- kaccuracy on the development\nset of Natural Questions using different similarity and\nloss functions. the correct answer, presumably by matching “body\nof water” with semantic neighbors such as seaand\nchannel , even though no lexical overlap exists. The\nsecond example is one where BM25 does better. The salient phrase “Thoros of Myr” is critical, and\nDPR is unable to capture it. D Joint Training of Retriever and\nReader\nWe ﬁx the passage encoder in our joint-training\nscheme while allowing only the question encoder\nto receive backpropagation signal from the com-\nbined (retriever + reader) loss function. This allows\nus to leverage the HNSW-based FAISS index for\nefﬁcient low-latency retrieving, without reindexing\nthe passages during model updates. Our loss func-\ntion largely follows ORQA’s approach, which uses\nlog probabilities of positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is ﬁxed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss.",
"Since the passage\nencoder is ﬁxed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Speciﬁcally, we get top 100 passages for each\nquestion in a mini-batch and use the method similar\nto in-batch negative training: all retrieved passages’\nvectors participate in the loss calculation for all\nquestions in a batch. Our training batch size is set\nto 16, which effectively gives 1,600 passages per\nquestion to calculate retriever loss. The reader still\nuses 24 passages per question, which are selected",
"Question Passage received by BM25 Passage retrieved by DPR\nWhat is the body of water\nbetween England and Ireland?Title:British Cycling Title: Irish Sea\n. . .England is not recognised as a region by the UCI, and\nthere is no English cycling team outside the Commonwealth\nGames. For those occasions, British Cycling selects and sup-\nports the England team. Cycling is represented on the Isle\nof Man by the Isle of Man Cycling Association. Cycling in\nNorthern Ireland is organised under Cycling Ulster, part of\nthe all-Ireland governing body Cycling Ireland . Until 2006,\na rival governing body existed, . . .. . . Annual trafﬁc between Great Britain and Ireland amounts\nto over 12 million passengers and of traded goods. The Irish\nSea is connected to the North Atlantic at both its northern\nand southern ends. To the north, the connection is through\nthe North Channel between Scotland and Northern Ireland\nand the Malin Sea. The southern end is linked to the Atlantic\nthrough the St George’s Channel between Ireland and Pem-\nbrokeshire, and the Celtic Sea. . . . Who plays Thoros of Myr in\nGame of Thrones?Title: No One (Game of Thrones) Title: P ˚al Sverre Hagen\n. . . He may be ”no one,” but there’s still enough of a person\nleft in him to respect, and admire who this girl is and what\nshe’s become. Arya ﬁnally tells us something that we’ve kind\nof known all along, that she’s not no one, she’s Arya Stark\nof Winterfell.” ”No One” saw the reintroduction of Richard\nDormer and Paul Kaye , who portrayed Beric Dondarrion and\nThoros ofMyr , respectively, in the third season, . . .P˚al Sverre Valheim Hagen (born 6 November 1980) is a Nor-\nwegian stage and screen actor. He appeared in the Norwe-\ngian ﬁlm ”Max Manus” and played Thor Heyerdahl in the\nOscar-nominated 2012 ﬁlm ”Kon-Tiki”. Pl Hagen was born\nin Stavanger, Norway, the son of Roar Hagen, a Norwegian\ncartoonist who has long been associated with Norway ´s largest\ndaily, ”VG”. He lived in Jtten, a neighborhood in the city of\nStavanger in south-western Norway. . . .",
"Pl Hagen was born\nin Stavanger, Norway, the son of Roar Hagen, a Norwegian\ncartoonist who has long been associated with Norway ´s largest\ndaily, ”VG”. He lived in Jtten, a neighborhood in the city of\nStavanger in south-western Norway. . . . Table 7: Examples of passages returned from BM25 and DPR. Correct answers are written in blue and the content\nwords in the question are written in bold. from the top 5 positive and top 30 negative passages\n(from the set of top 100 passages retrieved from\nthe same question). The question encoder’s initial\nstate is taken from a DPR model previously trained\non the NQ dataset. The reader’s initial state is a\nBERT-base model. In terms of the end-to-end QA\nresults, our joint-training scheme does not provide\nbetter results compared to the usual retriever/reader\ntraining pipeline, resulting in the same 39.8 exact\nmatch score on NQ dev as in our regular reader\nmodel training.",
"PASSAGE RE-RANKING WITH BERT\nRodrigo Nogueira\nNew York University\nrodrigonogueira@nyu.eduKyunghyun Cho\nNew York University\nFacebook AI Research\nCIFAR Azrieli Global Scholar\nkyunghyun.cho@nyu.edu",
"Recently, neural models pretrained on a language modeling task, such as\nELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (De-\nvlin et al., 2018), have achieved impressive results on various natural language\nprocessing tasks such as question-answering and natural language inference. In\nthis paper, we describe a simple re-implementation of BERT for query-based pas-\nsage re-ranking. Our system is the state of the art on the TREC-CAR dataset and\nthe top entry in the leaderboard of the MS MARCO passage retrieval task, outper-\nforming the previous state of the art by 27% (relative) in MRR@10. The code\nto reproduce our results is available at https://github.com/nyu-dl/\ndl4marco-bert\n1 I NTRODUCTION\nWe have seen rapid progress in machine reading compression in recent years with the introduction\nof large-scale datasets, such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016),\nSearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and QUASAR-T (Dhingra et al., 2017),\nand the broad adoption of neural models, such as BiDAF (Seo et al., 2016), DrQA (Chen et al.,\n2017), DocumentQA (Clark & Gardner, 2017), and QAnet (Yu et al., 2018). The information retrieval (IR) community has also experienced a ﬂourishing development of neural\nranking models, such as DRMM (Guo et al., 2016), KNRM (Xiong et al., 2017), Co-PACRR (Hui\net al., 2018), and DUET (Mitra et al., 2017). However, until recently, there were only a few large\ndatasets for passage ranking, with the notable exception of the TREC-CAR (Dietz et al., 2017). This, at least in part, prevented the neural ranking models from being successful when compared to\nmore classical IR techniques (Lin, 2019). We argue that the same two ingredients that made possible much progress on the reading compre-\nhension task are now available for passage ranking task.",
"We argue that the same two ingredients that made possible much progress on the reading compre-\nhension task are now available for passage ranking task. Namely, the MS MARCO passage ranking\ndataset, which contains one million queries from real users and their respective relevant passages\nannotated by humans, and BERT, a powerful general purpose natural language processing model. In this paper, we describe in detail how we have re-purposed BERT as a passage re-ranker and\nachieved state-of-the-art results on the MS MARCO passage re-ranking task. 2 P ASSAGE RE-RANKING WITH BERT\nTask A simple question-answering pipeline consists of three main stages. First, a large number\n(for example, a thousand) of possibly relevant documents to a given question are retrieved from a\ncorpus by a standard mechanism, such as BM25. In the second stage, passage re-ranking , each\nof these documents is scored and re-ranked by a more computationally-intensive method. Finally,\nthe top ten or ﬁfty of these documents will be the source for the candidate answers by an answer\ngeneration module. In this paper, we describe how we implemented the second stage of this pipeline,\npassage re-ranking. Method The job of the re-ranker is to estimate a score siof how relevant a candidate passage\ndiis to a query q. We use BERT as our re-ranker. Using the same notation used by Devlin et al. 1arXiv:1901.04085v5  [cs.IR]  14 Apr 2020",
"(2018), we feed the query as sentence A and the passage text as sentence B. We truncate the query\nto have at most 64 tokens. We also truncate the passage text such that the concatenation of query,\npassage, and separator tokens have the maximum length of 512 tokens. We use a BERT LARGE model\nas a binary classiﬁcation model, that is, we use the [CLS]vector as input to a single layer neural\nnetwork to obtain the probability of the passage being relevant. We compute this probability for\neach passage independently and obtain the ﬁnal list of passages by ranking them with respect to\nthese probabilities. We start training from a pre-trained BERT model and ﬁne-tune it to our re-ranking task using the\ncross-entropy loss:\nL=−∑\nj∈Jposlog(sj)−∑\nj∈Jneglog(1−sj), (1)\nwhereJposis the set of indexes of the relevant passages and Jnegis the set of indexes of non-relevant\npassages in top-1,000 documents retrieved with BM25. 3 E XPERIMENTS\nWe train and evaluate our models on two passage-ranking datasets, MS MARCO and TREC-CAR. 3.1 MS MARCO\nThe training set contains approximately 400M tuples of a query, relevant and non-relevant passages. The development set contains approximately 6,900 queries, each paired with the top 1,000 pas-\nsages retrieved with BM25 from the MS MARCO corpus. On average, each query has one relevant\npassage. However, some have no relevant passage because the corpus was initially constructed by\nretrieving the top-10 passages from the Bing search engine and then annotated. Hence, some of the\nrelevant passages might not be retrieved by BM25. An evaluation set with approximately 6,800 queries and their top 1,000 retrieved passages without\nrelevance annotations is also provided. Training We ﬁne-tune the model using a TPU v3-81with a batch size of 128 (128 sequences *\n512 tokens = 65,536 tokens/batch) for 100k iterations, which takes approximately 30 hours. This\ncorresponds to training on 12.8M (100k * 128) query-passage pairs or less than 2% of the full\ntraining set .",
"This\ncorresponds to training on 12.8M (100k * 128) query-passage pairs or less than 2% of the full\ntraining set . We could not see any improvement in the dev set when training for another 3 days,\nwhich equivalent to seeing 50M pairs in total. We use ADAM (Kingma & Ba, 2014) with the initial learning rate set to 3×10−6,β1= 0.9,\nβ2= 0.999, L2 weight decay of 0.01, learning rate warmup over the ﬁrst 10,000 steps, and linear\ndecay of the learning rate. We use a dropout probability of 0.1on all layers. 3.2 TREC-CAR\nIntroduced by Dietz et al. (2017), in this dataset, the input query is the concatenation of a Wikipedia\narticle title with the title of one of its section. The relevant passages are the paragraphs within that\nsection. The corpus consists of all of the English Wikipedia paragraphs, except the abstracts. The\nreleased dataset has ﬁve predeﬁned folds, and we use the ﬁrst four as a training set (approximately\n2.3M queries), and the remaining as a validation set (approximately 580k queries). The test set is\nthe same one used to evaluate the submissions to TREC-CAR 2017 (approx. 2,254 queries). Although TREC-CAR 2017 organizers provide manual annotations for the test set, only the top\nﬁve passages retrieved by the systems submitted to the competition have manual annotations. This\nmeans that true relevant passages are not annotated if they rank low. Hence, we evaluate using the\nautomatic annotations, which provide relevance scores for all possible query-passage pairs. Training We follow the same procedure described for the MS MARCO dataset to ﬁne-tune our\nmodels on TREC-CAR. However, there is an important difference. The ofﬁcial pre-trained BERT\n1https://cloud.google.com/tpu/\n2",
"MS MARCO TREC-CAR\nMRR@10 MAP\nMethod Dev Eval Test\nBM25 (Lucene, no tuning) 16.7 16.5 12.3\nBM25 (Anserini, tuned) - - 15.3\nCo-PACRR⋆(MacAvaney et al., 2017) - - 14.8\nKNRM (Xiong et al., 2017) 21.8 19.8 -\nConv-KNRM (Dai et al., 2018) 29.0 27.1 -\nIRNet†27.8 28.1 -\nBERT Base 34.7 - 31.0\nBERT Large 36.5 35.8 33.5\nTable 1: Main Result on the passage re-ranking datasets. ⋆Best Entry in the TREC-CAR 2017. †Previous SOTA in the MS MARCO leaderboard as of 01/04/2019; unpublished work. Number of training question-passage pairsMRR@10\n00.10.20.30.4\n1k 10k 100k 1M 10M 100MBERT Large\nIR-NET (previous \nSOTA)\nFigure 1: Number of MS MARCO examples seen during training vs. MRR@10 performance. models2were pre-trained on the full Wikipedia, and therefore they have seen, although in an unsu-\npervised way, Wikipedia documents that are used in the test set of TREC-CAR. Thus, to avoid this\nleak of test data into training, we pre-trained the BERT re-ranker only on the half of Wikipedia used\nby TREC-CAR’s training set. For the ﬁne-tuning data, we generate our query-passage pairs by retrieving the top ten passages\nfrom the entire TREC-CAR corpus using BM25.3This means that we end up with 30M example\npairs (3M queries * 10 passages/query) to train our model. We train it for 400k iterations, or 12.8M\nexamples (400k iterations * 32 pairs/batch), which corresponds to only 40% of the training set. Similarly to MS MARCO experiments, we did not see any gain on the dev set by training the\nmodels longer. 3.3 R ESULTS\nWe show the main result in Table 1. Despite training on a fraction of the data available, the proposed\nBERT-based models surpass the previous state-of-the-art models by a large margin on both of the\ntasks. Training size vs performance: We found that the pretrained models used in this work require few\ntraining examples from the end task to achieve a good performance 1. For example, a BERT LARGE\ntrained on 100k question-passage pairs (less than 0.3% of the MS MARCO training data) is already\n1.4 MRR@10 points better than the previous state-of-the-art, IR-NET.",
"For example, a BERT LARGE\ntrained on 100k question-passage pairs (less than 0.3% of the MS MARCO training data) is already\n1.4 MRR@10 points better than the previous state-of-the-art, IR-NET. 2https://github.com/google-research/bert\n3We use the Anserini toolkit (Yang et al., 2018) to index and retrieve the passages. 3",
"4 C ONCLUSION\nWe have described a simple adaptation of BERT as a passage re-ranker that has become the state of\nthe art on two different tasks, which are TREC-CAR and MS MARCO. We have made the code to\nreproduce our experiments publicly available.",
"Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-\ndomain questions. arXiv preprint arXiv:1704.00051 , 2017. Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. arXiv preprint arXiv:1710.10723 , 2017. Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks for\nsoft-matching n-grams in ad-hoc search. In Proceedings of the Eleventh ACM International Con-\nference on Web Search and Data Mining , pp. 126–134. ACM, 2018. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. Bhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. Quasar: Datasets for question answer-\ning by search and reading. arXiv preprint arXiv:1707.03904 , 2017. Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. Trec complex answer retrieval\noverview. TREC, 2017. Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, V olkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint\narXiv:1704.05179 , 2017. Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for\nad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information\nand Knowledge Management , pp. 55–64. ACM, 2016. Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. Co-pacrr: A context-aware neural\nir model for ad-hoc retrieval. In Proceedings of the Eleventh ACM International Conference on\nWeb Search and Data Mining , pp. 279–287. ACM, 2018. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014. Jimmy Lin. The neural hype and comparisons against weak baseline. 2019. Sean MacAvaney, Andrew Yates, and Kai Hui. Contextualized pacrr for complex answer retrieval. 2017.",
"Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014. Jimmy Lin. The neural hype and comparisons against weak baseline. 2019. Sean MacAvaney, Andrew Yates, and Kai Hui. Contextualized pacrr for complex answer retrieval. 2017. Bhaskar Mitra, Fernando Diaz, and Nick Craswell. Learning to match using local and distributed\nrepresentations of text for web search. In Proceedings of the 26th International Conference on\nWorld Wide Web , pp. 1291–1299. International World Wide Web Conferences Steering Commit-\ntee, 2017. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and\nLi Deng. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint\narXiv:1611.09268 , 2016. Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. Semi-supervised\nsequence tagging with bidirectional language models. arXiv preprint arXiv:1705.00108 , 2017. 4",
"Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/research-covers/language-unsupervised/language understanding paper. pdf , 2018. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250 , 2016. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention\nﬂow for machine comprehension. arXiv preprint arXiv:1611.01603 , 2016. Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural ad-\nhoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR Conference\non Research and Development in Information Retrieval , pp. 55–64. ACM, 2017. Peilin Yang, Hui Fang, and Jimmy Lin. Anserini: Reproducible ranking baselines using lucene. Journal of Data and Information Quality (JDIQ) , 10(4):16, 2018. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and\nQuoc V Le. Qanet: Combining local convolution with global self-attention for reading compre-\nhension. arXiv preprint arXiv:1804.09541 , 2018. 5",
"Precise Zero-Shot Dense Retrieval without Relevance Labels\nLuyu Gao∗†Xueguang Ma∗‡Jimmy Lin‡Jamie Callan†\n†Language Technologies Institute, Carnegie Mellon University\n‡David R. Cheriton School of Computer Science, University of Waterloo\n{luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca\nAbstract\nWhile dense retrieval has been shown effec-\ntive and efﬁcient across tasks and languages,\nit remains difﬁcult to create effective fully\nzero-shot dense retrieval systems when no rel-\nevance label is available. In this paper, we\nrecognize the difﬁculty of zero-shot learning\nand encoding relevance. Instead, we pro-\npose to pivot through Hy pothetical D ocument\nEmbeddings ( HyDE ). Given a query, HyDE ﬁrst\nzero-shot instructs an instruction-following\nlanguage model (e.g. InstructGPT ) to gen-\nerate a hypothetical document. The docu-\nment captures relevance patterns but is unreal\nand may contain false details. Then, an un-\nsupervised contrastively learned encoder (e.g. Contriever ) encodes the document into an\nembedding vector. This vector identiﬁes a\nneighborhood in the corpus embedding space,\nwhere similar real documents are retrieved\nbased on vector similarity. This second step\nground the generated document to the actual\ncorpus, with the encoder’s dense bottleneck\nﬁltering out the incorrect details. Our exper-\niments show that HyDE signiﬁcantly outper-\nforms the state-of-the-art unsupervised dense\nretriever Contriever and shows strong per-\nformance comparable to ﬁne-tuned retrievers,\nacross various tasks (e.g. web search, QA, fact\nveriﬁcation) and languages (e.g. sw, ko, ja).1\n1 Introduction\nDense retrieval (Lee et al., 2019; Karpukhin et al.,\n2020), the method of retrieving documents using\nsemantic embedding similarities, has been shown\nsuccessful across tasks like web search, question\nanswering, and fact veriﬁcation. A variety of meth-\nods such as negative mining (Xiong et al., 2021; Qu\net al., 2021), distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofstätter et al., 2021) and task-speciﬁc\n∗Equal contribution.",
"A variety of meth-\nods such as negative mining (Xiong et al., 2021; Qu\net al., 2021), distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofstätter et al., 2021) and task-speciﬁc\n∗Equal contribution. 1No models were trained or ﬁne-tuned in making this pre-\nprint. Our open source code is available at https://github. com/texttron/hyde .pre-training (Izacard et al., 2021; Gao and Callan,\n2021; Lu et al., 2021; Gao and Callan, 2022; Liu\nand Shao, 2022) have been proposed to improve the\neffectiveness of supervised dense retrieval models. On the other hand, zero-shot dense retrieval still\nremains difﬁcult. Many recent works consider the\nalternative transfer learning setup, where the dense\nretrievers are trained on a high-resource dataset and\nthen evaluated on queries from new tasks. The MS-\nMARCO collection (Bajaj et al., 2016), a massive\njudged dataset with a large number of judged query-\ndocument pairs, is arguably the most commonly\nused. As argued by Izacard et al. (2021), in prac-\ntice, however, the existence of such a large dataset\ncannot always be assumed. Even MS-MARCO re-\nstricts commercial use and cannot be adopted in a\nvariety of real-world search scenarios. In this paper, we aim to build effective fully\nzero-shot dense retrieval systems that require no\nrelevance supervision, work out-of-box and gener-\nalize across tasks. As supervision is not available,\nwe start by examining self-supervised representa-\ntion learning methods. Modern deep learning en-\nables two distinct learning algorithms. At the token\nlevel, generative large language models (LLM) pre-\ntrained on large corpus have demonstrated strong\nnatural language understanding (NLU) and gen-\neration (NLG) capabilities (Brown et al., 2020;\nChen et al., 2021; Rae et al., 2021; Hoffmann\net al., 2022; Thoppilan et al., 2022; Chowdhery\net al., 2022). At the document level, text (chunk)\nencoders pre-trained with contrastive objectives\nlearn to encode document-document similarity into\ninner-product (Izacard et al., 2021; Gao and Callan,\n2022).",
"At the document level, text (chunk)\nencoders pre-trained with contrastive objectives\nlearn to encode document-document similarity into\ninner-product (Izacard et al., 2021; Gao and Callan,\n2022). On top of these, one extra insight into LLM\nis borrowed: the LLMs further trained to follow\ninstructions can zero-shot generalize to diverse un-\nseen instructions (Ouyang et al., 2022; Sanh et al.,\n2022; Min et al., 2022; Wei et al., 2022). Ouyang\net al. (2022) show that with a small amount of data,\nGPT-3 (Brown et al., 2020) models can be alignedarXiv:2212.10496v1  [cs.IR]  20 Dec 2022",
"HyDE\nGPTContrieverhow long does it take to remove\nwisdom tooth It usually takes between 30\nminutes and two hours to\nremove a wisdom tooth...How wisdom teeth are removed... Some ... a few minutes, whereas\nothers can take 20 minutes or\nlonger .... How has the COVID-19 pandemic impacted\nmental health?...depression and anxiety had\nincreased by 20% since the\nstart of the pandemic...... two studies investigating\nCOVID-19 patients ... significantly\nhigher level of depressive ...write a passage to answer the question\nwrite a scientific paper passage to answer\nthe question\n인간은  언제  불을  사용했는가 ?write a passage in Korean to answer the\nquestion in detail인간이  불을  사용한  기록은  약\n800 만년  전부터  나타난다 ...... 불을  처음  사용한  시기는  호모\n에렉투스가  살았던  142 만  년  전으\n로 거슬러간다 ... instruction query generated document real documentFigure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries\nwithout changing the underlying GPT-3 andContriever /mContriever models. to human intent to follow instructions. With these ingredients, we propose to\npivot through Hypothetical Document\nEmbeddings ( HyDE ), and decompose dense\nretrieval into two tasks, a generative task per-\nformed by an instruction-following language\nmodel and a document-document similarity task\nperformed by a contrastive encoder (Figure 1). First, we feed the query to the generative model\nand instruct it to \"write a document that answers\nthe question\", i.e. a hypothetical document. We expect the generative process to capture\n\"relevance\" by giving an example; the generated\ndocument is not real, can contain factual errors but\nis like a relevant document. In the second step,\nwe use an unsupervised contrastive encoder to\nencode this document into an embedding vector. Here, we expect the encoder’s dense bottleneck\nto serve a lossy compressor, where the extra\n(hallucinated) details are ﬁltered out from the\nembedding. We use this vector to search against\nthe corpus embeddings. The most similar real\ndocuments are retrieved and returned.",
"We use this vector to search against\nthe corpus embeddings. The most similar real\ndocuments are retrieved and returned. The retrieval\nleverages document-document similarity encoded\nin the inner-product during contrastive training. Note that, interestingly, with HyDE factorization,\nthe query-document similarity score is no longer\nexplicitly modeled nor computed. Instead, the\nretrieval task is cast into two NLU and NLG tasks. HyDE appears unsupervised. Nomodel is trained\ninHyDE : both the generative model and the con-\ntrastive encoder remain intact. Supervision signals\nwere only involved in instruction learning of our\nbackbone LLM. In our experiments, we show HyDE using Instruct-\nGPT (Ouyang et al., 2022) and Contriever (Izacard\net al., 2021) as backbone models signiﬁcantly out-\nperforms the previous state-of-the-art Contriever-\nonly zero-shot no-relevance system on 11 queriessets, covering tasks like Web Search, Question\nAnswering, Fact Veriﬁcation and languages like\nSwahili, Korean, Japanese. 2 Related Works\nDense Retrieval (Lee et al., 2019; Karpukhin\net al., 2020) has been extensively studied after the\nemergence of pre-trained Transformer language\nmodels (Devlin et al., 2019). Researchers stud-\nied the metric learning problems, such as training\nloss (Karpukhin et al., 2020) and negative sam-\npling (Xiong et al., 2021; Qu et al., 2021), and also\nintroduced distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofstätter et al., 2021). Later works studied\nthe second stage pre-training of language model\nspeciﬁcally for retrieval (Izacard et al., 2021; Gao\nand Callan, 2021; Lu et al., 2021; Gao and Callan,\n2022; Liu and Shao, 2022). The popularity of dense retrieval can be partially\nattributed to the rich and successful research in very\nefﬁcient minimum inner product search (MIPS) at\nvery large (billion) scales (Johnson et al., 2017).",
"The popularity of dense retrieval can be partially\nattributed to the rich and successful research in very\nefﬁcient minimum inner product search (MIPS) at\nvery large (billion) scales (Johnson et al., 2017). Instructions-Following Language Models\nSoon after the emergence of LLMs, several groups\nof researchers discover that LLMs trained on data\nconsisting of instructions and their execution can\nzero-shot generalize to perform new tasks with new\ninstructions (Ouyang et al., 2022; Sanh et al., 2022;\nMin et al., 2022; Wei et al., 2022). This can be\ndone by standard supervised sequence-to-sequence\nlearning or more effectively with reinforcement\nlearning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied\n“Task-aware Retrieval with Instructions”. They\nﬁne-tuned dense encoders that can also encode\ntask-speciﬁc instruction prepended to query. In\ncomparison, we use an unsupervised encoder and\nhandle different tasks and their instruction with an",
"instruction following generative LLM, as described\nabove. Zero-Shot Dense Retrieval The tasks of zero-\nshot (dense) retrieval are arguably empirically de-\nﬁned by Thakur et al. (2021) for the neural re-\ntrieval community. Their BEIR benchmark con-\nsists of diverse retrieval tasks. The paper and\nmany follow-up research generally consider the\nTransfer Learning setup where the dense re-\ntriever is ﬁrst learned using a diverse and richly\nsupervised corpus and query collection, namely\nMS-MARCO (Thakur et al., 2021; Wang et al.,\n2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such\na large collection can rarely be assumed. In this\npaper, therefore, we study the problem of building\neffective dense retrieval systems without relevance\nlabels. Similar to Izacard et al. (2021), we also\ndo not assume access to the test time corpora for\ntraining. This is a more realistic setup and prevents\nover-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our\nsetup can be roughly considered as “unsuper-\nvised” . Strictly, as with Sachan et al. (2022), the\nonly supervision resides in the LLM, in the pro-\ncessing of learning to follow instructions. Generative Retrieval Generative search is a new\nclass of retrieval methods that use neural generative\nmodels as search indices (Metzler et al., 2021; Tay\net al., 2022; Bevilacqua et al., 2022; Lee et al.,\n2022). These models use (constrained) decoding\nto generate document identiﬁers, such as id and\nsub-string, which map directly to realdocuments. They have to go through special training procedures\nover relevance data; effective search may also need\nto use novel forms of search indices (Bevilacqua\net al., 2022; Lee et al., 2022). In comparison, our\nmethod uses the standard MIPS index and requires\nno training or training data. Our generative model\nproduces an intermediate hypothetical document\nto be fed into a dense encoder, instead of a real\ndocument. 3 Methodology\nIn this section, we ﬁrst formally deﬁne the prob-\nlem of (zero-shot) dense retrieval.",
"Our generative model\nproduces an intermediate hypothetical document\nto be fed into a dense encoder, instead of a real\ndocument. 3 Methodology\nIn this section, we ﬁrst formally deﬁne the prob-\nlem of (zero-shot) dense retrieval. Then we will\nintroduce how HyDE is designed to solve it.3.1 Preliminaries\nDense retrieval models similarity between query\nand document with inner product similarity. Given\na query qand document d, it uses two encoder\nfunction encqandencdto map them into ddimen-\nsion vectors vq,vd, whose inner product is used\nas similarity measurement. sim(q,d) =⟨encq(q),encd(d)⟩=⟨vq,vd⟩(1)\nFor zero-shot retrieval, we consider Lquery sets\nQ1, Q2, ..., Q Land their corresponding search cor-\npus, document sets D1, D2, ..., D L. Denote the\nj-th query from i-th set query set Qiasqij. We\nneed to fully deﬁne mapping functions encqand\nencdwithout access to any query set Qi, document\nsetDi, or any relevance judgment rij. The difﬁculty of zero-shot dense retrieval lies\nprecisely in Equation 1: it requires learning of two\nembedding functions (for query and document re-\nspectively) into the same embedding space where\ninner product captures relevance . Without rele-\nvance judgments/scores to ﬁt, learning becomes\nintractable. 3.2 HyDE\nHyDE circumvents the aforementioned learning\nproblem by performing search in document-\nonly embedding space that captures document-\ndocument similarity. This can be easily learned\nusing unsupervised contrastive learning (Izacard\net al., 2021; Gao et al., 2021; Gao and Callan,\n2022). We set document encoder encddirectly as a\ncontrastive encoder enc con. f=encd=enc con (2)\nThis function is also denoted as ffor simplic-\nity. This unsupervised contrastive encoder will\nbe shared by all incoming document corpus. vd=f(d)∀d∈D1∪D2∪...∪DL (3)\nTo build the query vector, we consider in addition\nan instruction following LM, InstructLM. It takes a\nquery qand a textual instruction INST and follows\nthem to perform the task speciﬁed by INST .",
"vd=f(d)∀d∈D1∪D2∪...∪DL (3)\nTo build the query vector, we consider in addition\nan instruction following LM, InstructLM. It takes a\nquery qand a textual instruction INST and follows\nthem to perform the task speciﬁed by INST . For\nsimplicity, denote,\ng(q,INST) =InstructLM (q,INST) (4)\nNow we can use gto map queries to \"hypotheti-\ncal\" documents by sampling from g, setting INST",
"to be“write a paragraph that answers the\nquestion” . The generated document is not real,\ncan and is likely to be ungrounded factually (Brown\net al., 2020; Thoppilan et al., 2022). We only re-\nquire it to capture relevance pattern. This is done\nby generating documents, i.e. providing exam-\nples. Critically, here we ofﬂoad relevance mod-\neling from representation learning model to an\nNLG model that generalizes signiﬁcantly more eas-\nily, naturally, and effectively (Brown et al., 2020;\nOuyang et al., 2022). Generating examples also\nreplaces explicit modeling of relevance scores. We can now encode the generated document using\nthe document encoder f. Write,\nE[vqij] =E[f(g(qij,INST i))] (5)\nFormally, gdeﬁnes a probability distribution based\non the chain rule. In this paper, we simply consider\nthe expectation value, assuming the distribution of\nvqijis uni-modal, i.e. the query is not ambiguous. The study of ambiguous queries and diversity is\nleft to future work. We estimate Equation 5 by\nsampling Ndocuments from g,[ˆd1,ˆd2, ...,ˆdN]. ˆvqij=1\nN∑\nˆdk∼g(qij,INST i)f(dk) (6)\n=1\nNN∑\nk=1f(ˆdk) (7)\nWe also consider the query as a possible hypothesis,\nˆvqij=1\nN+ 1[N∑\nk=1f(ˆdk) +f(qij)] (8)\nInner product is computed between ˆvqijand the\nset of all document vectors {f(d)|d∈Di}. The\nmost similar documents are retrieved. Here the\nencoder function fserves as a lossy compressor\nthat outputs dense vectors, where the extra details\nare ﬁltered and left out from the vector. It further\ngrounds the hypothetical vector to the actual corpus\nand the real documents. The full HyDE system is\nillustrated in Figure 1. 4 Experiments\n4.1 Setup\nImplementation We implement HyDE using\nInstructGPT , a GPT-3 model from the instruct\nseries (text-davinci-003 ; Ouyang et al. (2022))\nandContriever models (Izacard et al., 2021). Wesample from InstructGPT using the OpenAI play-\nground default temperature of 0.7 for open-ended\ngenerations. We use the English-only Contriever\nmodel for English retrieval tasks and multilingual\nmContriever for non-English tasks.",
"Wesample from InstructGPT using the OpenAI play-\nground default temperature of 0.7 for open-ended\ngenerations. We use the English-only Contriever\nmodel for English retrieval tasks and multilingual\nmContriever for non-English tasks. We conducted\nretrieval experiments with the Pyserini toolkit (Lin\net al., 2021a). Datasets We consider web search query sets\nTREC DL19 (Craswell et al., 2020a) and\nDL20 (Craswell et al., 2020b); they are based on\nthe MS-MARCO dataset (Bajaj et al., 2016). We\nalso use a diverse collection of 6 low-resource\ndatasets from the BEIR dataset (Thakur et al.,\n2021). For non-English retrieval, we consider\nSwahili, Korean, Japanese, and Bengali from the\nMr.Tydi dataset (Zhang et al., 2021). We use different instructions for each dataset. They share a similar structure but have different\nquantiﬁers to control the exact form of the gener-\nated hypothetical documents. These instructions\ncan be found in subsection A.1. Compared Systems Contriever models,\nContriever andmContriever , serve as our major\nbaseline. They are trained using unsupervised\ncontrastive learning. HyDE retrievers share the\nexact same embedding spaces with them. The\nonly difference is how the query vector is built. These comparisons allow us to easily examine\nthe effect of HyDE . The classical heuristic-based\nlexical retriever BM25 is also included. Several systems that involve ﬁne-tuning on mas-\nsive relevance data are also included as refer-\nences. We consider models ﬁne-tuned on MS-\nMARCO and transferred, DPR and ANCE, from\nthe BEIR paper. For multilingual, we include\nthe mDPR model from Mr.Tydi paper and MS-\nMARCO ﬁne-tuned mBERT and XLM-R from\nthe Contriever paper. We also include the state-of-\nthe-art transfer learning models: Contriever and\nmContriever ﬁne-tuned on MS-MARCO, denoted\nContrieverFTandmContrieverFT.",
"We also include the state-of-\nthe-art transfer learning models: Contriever and\nmContriever ﬁne-tuned on MS-MARCO, denoted\nContrieverFTandmContrieverFT. These mod-\nels have run through the state-of-the-art retrieval\nmodel training pipeline that involves second-stage\nretrieval-speciﬁc pre-training (Lee et al., 2019) and\na few rounds of ﬁne-tuning (Qu et al., 2021); they\nshould be considered empirical upper bounds. 4.2 Web Search\nIn Table 1, we show retrieval results on TREC\nDL19 and TREC DL20. We see HyDE bring sizable\nimprovements to Contriever across the board for",
"DL19 DL20\nmap ndcg@10 recall@1k map ndcg@10 recall@1k\nw/o relevance judgement\nBM25 30.1 50.6 75.0 28.6 48.0 78.6\nContriever 24.0 44.5 74.6 24.0 42.1 75.4\nHyDE 41.8 61.3 88.0 38.2 57.9 84.4\nw/ relevance judgement\nDPR 36.5 62.2 76.9 41.8 65.3 81.4\nANCE 37.1 64.5 75.5 40.8 64.6 77.6\nContrieverFT41.7 62.1 83.6 43.6 63.2 85.8\nTable 1: Results for web search on DL19/20. Best performing w/o relevance and overall system(s) are marked\nbold . DPR, ANCE and ContrieverFTare in-domain supervised models that are ﬁnetuned on MS MARCO training\ndata. Scifact Arguana Trec-Covid FiQA DBPedia TREC-NEWS\nnDCG@10\nw/o relevance judgement\nBM25 67.9 39.7 59.5 23.6 31.8 39.5\nContriever 64.9 37.9 27.3 24.5 29.2 34.8\nHyDE 69.1 46.6 59.3 27.3 36.8 44.0\nw/ relevance judgement\nDPR 31.8 17.5 33.2 29.5 26.3 16.1\nANCE 50.7 41.5 65.4 30.0 28.1 38.2\nContrieverFT67.7 44.6 59.6 32.9 41.3 42.8\nRecall@100\nw/o relevance judgement\nBM25 92.5 93.2 49.8 54.0 46.8 44.7\nContriever 92.6 90.1 17.2 56.2 45.3 42.3\nHyDE 96.4 97.9 41.4 62.1 47.2 50.9\nw/ relevance judgement\nDPR 72.7 75.1 21.2 34.2 34.9 21.5\nANCE 81.6 93.7 45.7 58.1 31.9 39.8\nContrieverFT94.7 97.7 40.7 65.6 54.1 49.2\nTable 2: Low resource tasks from BEIR. Best performing w/o relevance and overall system(s) are marked bold . both precision-oriented and recall metrics. While\nunsupervised Contriever can underperform the\nclassical BM25 approach, HyDE outperforms BM25\nby large margins. HyDE remains competitive even when compared\nto ﬁne-tuned models. Note that TREC DL19/20\nare search tasks deﬁned on MS-MARCO and\nthere, all the ﬁne-tuned models are richly super-\nvised . On TREC DL19, HyDE shows comparable\nmap and ndcg@10 to ContrieverFTand best re-\ncall@1k. On DL20, HyDE gets around 10% lower\nmap and ndcg@10 than ContrieverFTand sim-\nilar recall@1k. The ANCE model shows better\nndcg@10 numbers than HyDE but lower recall, sug-\ngesting it may be biased to a subset of queries\nand/or relevant documents.4.3 Low Resource Retrieval\nIn Table 2, we show retrieval results on low-\nresource tasks from BEIR.",
"The ANCE model shows better\nndcg@10 numbers than HyDE but lower recall, sug-\ngesting it may be biased to a subset of queries\nand/or relevant documents.4.3 Low Resource Retrieval\nIn Table 2, we show retrieval results on low-\nresource tasks from BEIR. Similar to web\nsearch,HyDE again brings sizable improvements to\nContriever across the board in terms of both ndcg\nand recall. HyDE is only outperformed by BM25 on\none dataset, TREC-Covid but with a tiny 0.2 mar-\ngin; in comparison, the underlying Contriever\nunderperforms by more than 50%. We also observe HyDE demonstrates strong\nperformance compared to ﬁne-tuned models. HyDE generally shows better performance than\nANCE and DPR, even though the two are\nﬁne-tuned on MS-MARCO and ANCE also in-\nvolves some sophisticated hard negative techniques. ContrieverFTshows performance advantages on\nFiQA and DBPedia. These involve retrieval of ﬁ-\nnancial posts or entities respectively. We believe\nthe performance difference can be attributed to the",
"Swahili Korean Japanese Bengali\nw/o relevance judgement\nBM25 38.9 28.5 21.2 41.8\nmContriever 38.3 22.3 19.5 35.3\nHyDE 41.7 30.6 30.7 41.3\nw/ relevance judgement\nmDPR 7.3 21.9 18.1 25.8\nmBERT 37.4 28.1 27.1 35.1\nXLM-R 35.1 32.2 24.8 41.7\nmContrieverFT51.2 34.2 32.4 42.3\nTable 3: MRR@100 on Mr.Tydi. Best performing w/o\nrelevance and overall system(s) are marked bold . under-speciﬁcation of the instruction; more elabo-\nrative instructions may help. 4.4 Multilingual Retrieval\nMultilingual setup poses several additional chal-\nlenges to HyDE . The small-sized contrastive en-\ncoder gets saturated as the number of languages\nscales (Conneau et al., 2020; Izacard et al., 2021). Meanwhile, our generative LLM faces an opposite\nissue: with languages of not as high resource as\nEnglish or French, the high capacity LLM can get\nunder-trained (Hoffmann et al., 2022). Nevertheless, in Table 3, we still ﬁnd HyDE\nable to improve the mContriever model. It can\noutperform non-Contriever models ﬁne-tuned on\nand transferred from MS-MARCO. On the other\nhand, we do observe some margins between HyDE\nand ﬁne-tuned mContrieverFT. SinceHyDE and\nmContrieverFTuse similar contrastive encoders,\nwe hypothesize this is because the non-English lan-\nguages we considered are under-trained in both\npre-training and instruction learning stages. 5 Analysis\nThe generative LLM and contrastive encoder make\nup the backbone of HyDE . In this section, we study\nthe effect of changing their realizations. In partic-\nular, we consider smaller language models (LM)\nand ﬁne-tuned encoders. We conduct our studies\non TREC DL19/20. 5.1 Effect of Different Generative Models\nIn Table 4, we show HyDE using other\ninstruction-following language models. In\nparticular, we consider a 52-billion Cohere\nmodel ( command-xlarge-20221108 ) and a\n11-billion FLAN model ( FLAN-T5-xxl ; Wei\net al.",
"5.1 Effect of Different Generative Models\nIn Table 4, we show HyDE using other\ninstruction-following language models. In\nparticular, we consider a 52-billion Cohere\nmodel ( command-xlarge-20221108 ) and a\n11-billion FLAN model ( FLAN-T5-xxl ; Wei\net al. (2022)).2Generally, we observe that all\n2Model sizes are from https://crfm.stanford.edu/\nhelm/v1.0/?models .Model DL19 DL20\nContriever 44.5 42.1\nContrieverFT62.1 63.2\nHyDE\nw/ Contriever\nw/ Flan-T5 (11b) 48.9 52.9\nw/ Cohere (52b) 53.8 53.8\nw/ GPT (175b) 61.3 57.9\nw/ ContrieverFT\nw/ Flan-T5 (11b) 60.2 62.1\nw/ Cohere (52b) 61.4 63.1\nw/ GPT (175b) 67.4 63.5\nTable 4: NDCG@10 on TREC DL19/20. Effect\nof changing different instruction LMs and using ﬁne-\ntuned encoder. Best w/o relevance and overall models\nare marked bold . models bring improvement to the unsupervised\nContriever , with larger models bringing larger\nimprovements. At the time when this paper is\nwritten, the Cohere model is still experimental\nwithout much detail disclosed. We can only\ntentatively hypothesize that training techniques\nmay have also played some role in the performance\ndifference. 5.2 HyDE with Fine-tuned Encoder\nTo begin with, HyDE with ﬁne-tuned encoder is\nnotthe intended usage: HyDE is more powerful\nand irreplaceable when few relevance labels are\npresent. Here we are interested to ﬁnd out if\nand howHyDE embedding can affect ﬁne-tuned en-\ncoders. In Table 4, we see that less powerful instruc-\ntion LMs can negatively impact the overall perfor-\nmance of the ﬁne-tuned retriever. (To remind our\nreaders,ContrieverFTis in-domain supervisedly\nﬁne-tuned for TREC DL19/20). The performance\ndegradations remain small. On the other hand, we\nalso observe the InstructGPT model able to fur-\nther bring up the performance, especially on DL19. This suggests that there may still exist certain fac-\ntors not captured by the ﬁne-tuned encoder but only\nby the generative model. 6 Conclusion\nAt the end of the paper, we encourage the readers\nto take a moment and reﬂect on the HyDE model. Compare it to some of the other recently seen re-\ntrievers or re-ranker.",
"6 Conclusion\nAt the end of the paper, we encourage the readers\nto take a moment and reﬂect on the HyDE model. Compare it to some of the other recently seen re-\ntrievers or re-ranker. These other models probably\ndiffer in their architecture, training method, and/or\ntask, but probably all of them involve modeling\nrelevance scores between a pair of query and docu-",
"ment. Dense retrievers consider vector similarities\nwhile self-attentive re-rankers regression scores. In\ncomparison, the concept of relevance in HyDE is\ncaptured by an NLG model and the language gener-\nation process. We demonstrate in many cases, HyDE\ncan be as effective as dense retrievers that learn to\nmodel numerical relevance scores. So, is numeri-\ncal relevance just a statistical artifact of language\nunderstanding? Will a weak retriever theoretically\nsufﬁce as the NLU & NLG models rapidly become\nstronger? Rushing to conclusions is not smart;\nmore works need to be done to get answers. With\nthis paper, we just want to raise these questions. Concretely in this paper, we introduce a new\nparadigm of interactions between LLM and dense\nencoder/retriever. We demonstrate (part of) rel-\nevance modeling and instruction understanding\ncan be delegated to the more powerful and ﬂex-\nible LLM. As a consequence, the need for rele-\nvance labels is removed. We are excited to see\nhow this can be generalized further to more so-\nphisticated tasks like multi-hop retrieval/QA and\nconversational search. We argue HyDE is also of practical use though not\nnecessarily over the entire lifespan of a search sys-\ntem. At the very beginning of the life of the search\nsystem, serving queries using HyDE offers perfor-\nmance comparable to a ﬁne-tuned model, which\nno other relevance-free model can offer. As the\nsearch log grows, a supervised dense retriever can\nbe gradually rolled out. As the dense retriever\ngrows stronger, more queries will be routed to it,\nwith only less common and emerging ones going\ntoHyDE backend. References\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware re-\ntrieval with instructions. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-\nwary, and Tong Wang. 2016. Ms marco: A human\ngenerated machine reading comprehension dataset.",
"2016. Ms marco: A human\ngenerated machine reading comprehension dataset. Michele Bevilacqua, Giuseppe Ottaviano, Patrick\nLewis, Wen-tau Yih, Sebastian Riedel, and Fabio\nPetroni. 2022. Autoregressive search engines: Gen-\nerating substrings as document identiﬁers. CoRR ,\nabs/2204.10628.Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual . Mark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Eval-\nuating large language models trained on code.",
"2021. Eval-\nuating large language models trained on code. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-",
"moyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\nCampos, and Ellen M. V oorhees. 2020a. Overview\nof the trec 2019 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz,\nDaniel Fernando Campos, and Ellen M. V oorhees. 2020b. Overview of the trec 2020 deep learning\ntrack. ArXiv , abs/2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics. Luyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing , pages 981–993,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics. Luyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense\npassage retrieval. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 2843–2853,\nDublin, Ireland. Association for Computational Lin-\nguistics. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence\nembeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.",
"2021. SimCSE: Simple contrastive learning of sentence\nembeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models. Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong\nYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-\nﬁciently teaching an effective dense retriever with\nbalanced topic aware sampling. In Proceedings of\nthe 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval ,SIGIR ’21, page 113–122, New York, NY , USA. As-\nsociation for Computing Machinery. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Towards unsupervised\ndense information retrieval with contrastive learning. CoRR , abs/2112.09118. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus. CoRR ,\nabs/1702.08734. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769–\n6781, Online. Association for Computational Lin-\nguistics. Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon\nSeo. 2022. Generative multi-hop retrieval. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.",
"domain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics , pages 6086–6096, Florence,\nItaly. Association for Computational Linguistics. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021a. Pyserini: A Python toolkit for reproducible\ninformation retrieval research with sparse and dense\nrepresentations. In Proceedings of the 44th Annual\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR\n2021) , pages 2356–2362. Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021b. In-batch negatives for knowledge distillation\nwith tightly-coupled teachers for dense retrieval. In\nProceedings of the 6th Workshop on Representation\nLearning for NLP (RepL4NLP-2021) , pages 163–\n173, Online. Association for Computational Linguis-\ntics. Zheng Liu and Yingxia Shao. 2022. Retromae: Pre-\ntraining retrieval-oriented transformers via masked\nauto-encoder. ArXiv , abs/2205.12035. Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed\nMalik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu,\nand Arnold Overwijk. 2021. Less is more: Pre-\ntrain a strong Siamese encoder for dense text re-\ntrieval using a weak decoder. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing , pages 2780–2791, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics. Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork.",
"Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies , pages 2791–2809, Seattle, United States. Association for Computational Linguistics. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan\nLowe. 2022. Training language models to follow in-\nstructions with human feedback. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2021. RocketQA: An opti-\nmized training approach to dense passage retrieval\nfor open-domain question answering. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n5835–5847, Online. Association for Computational\nLinguistics. Jack W.",
"In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n5835–5847, Online. Association for Computational\nLinguistics. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, An-\ntonia Creswell, Nat McAleese, Amy Wu, Erich\nElsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan,\nMichela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\nmatzadeh, Elena Gribovskaya, Domenic Donato,\nAngeliki Lazaridou, Arthur Mensch, Jean-Baptiste\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d’Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones,\nJames Bradbury, Matthew Johnson, Blake Hecht-\nman, Laura Weidinger, Iason Gabriel, William Isaac,\nEd Lockhart, Simon Osindero, Laura Rimell, Chris\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-\nway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. 2021. Scal-\ning language models: Methods, analysis & insights\nfrom training gopher. Devendra Singh Sachan, Mike Lewis, Mandar Joshi,\nArmen Aghajanyan, Wen-tau Yih, Joelle Pineau, and\nLuke Zettlemoyer. 2022. Improving passage re-\ntrieval with zero-shot question generation.Victor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V .",
"Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\nheesht Sharma, Andrea Santilli, Thibault Févry, Ja-\nson Alan Fries, Ryan Teehan, Teven Le Scao, Stella\nBiderman, Leo Gao, Thomas Wolf, and Alexan-\nder M. Rush. 2022. Multitask prompted training\nenables zero-shot task generalization. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni,\nDara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\nZhao, Jai Prakash Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022. Transformer\nmemory as a differentiable search index. CoRR ,\nabs/2202.06991. Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogenous benchmark for zero-shot evalu-\nation of information retrieval models. CoRR ,\nabs/2104.08663. Romal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kath-\nleen S. Meier-Hellstern, Meredith Ringel Morris,\nTulsee Doshi, Renelito Delos Santos, Toju Duke,\nJohnny Soraker, Ben Zevenbergen, Vinodkumar\nPrabhakaran, Mark Diaz, Ben Hutchinson, Kristen\nOlson, Alejandra Molina, Erin Hoffman-John, Josh\nLee, Lora Aroyo, Ravi Rajakumar, Alena Butryna,\nMatthew Lamm, Viktoriya Kuzmina, Joe Fenton,\nAaron Cohen, Rachel Bernstein, Ray Kurzweil,\nBlaise Aguera-Arcas, Claire Cui, Marian Croak,\nEd H. Chi, and Quoc Le. 2022. Lamda: Lan-\nguage models for dialog applications. CoRR ,\nabs/2201.08239. Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna\nGurevych. 2022.",
"Chi, and Quoc Le. 2022. Lamda: Lan-\nguage models for dialog applications. CoRR ,\nabs/2201.08239. Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna\nGurevych. 2022. GPL: Generative pseudo label-\ning for unsupervised domain adaptation of dense re-\ntrieval. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies , pages 2345–2360, Seattle, United States. Association for Computational Linguistics. Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-",
"drew M. Dai, and Quoc V . Le. 2022. Finetuned lan-\nguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021 . OpenReview.net. Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and\nArnold Overwijk. 2022. Coco-dr: Combating dis-\ntribution shifts in zero-shot dense retrieval with con-\ntrastive and distributionally robust learning. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing . Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021. Mr. TyDi: A multi-lingual benchmark for\ndense retrieval. arXiv:2108.08787 .",
"A Appendix\nA.1 Instructions\nA.1.1 Web Search\nPlease write a passage to answer the question\nQuestion: [QUESTION]\nPassage:\nA.1.2 SciFact\nPlease write a scientiﬁc paper passage to support/refute the claim\nClaim: [Claim]\nPassage:\nA.1.3 Arguana\nPlease write a counter argument for the passage\nPassage: [PASSAGE]\nCounter Argument:\nA.1.4 TREC-COVID\nPlease write a scientiﬁc paper passage to answer the question\nQuestion: [QUESTION]\nPassage:\nA.1.5 FiQA\nPlease write a ﬁnancial article passage to answer the question\nQuestion: [QUESTION]\nPassage:\nA.1.6 DBPedia-Entity\nPlease write a passage to answer the question. Question: [QUESTION]\nPassage:\nA.1.7 TREC-NEWS\nPlease write a news passage about the topic. Topic: [TOPIC]\nPassage:\nA.1.8 Mr.TyDi\nPlease write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail. Question: [QUESTION]\nPassage:",
"Retrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks\nPatrick Lewis†‡, Ethan Perez⋆,\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n†Facebook AI Research;‡University College London;⋆New York University;\nplewis@fb.com\nAbstract\nLarge pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\nedge is still limited, and hence on knowledge-intensive tasks, their performance\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\ndecisions and updating their world knowledge remain open research problems. Pre-\ntrained models with a differentiable access mechanism to explicit non-parametric\nmemory have so far been only investigated for extractive downstream tasks. We\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\nory for language generation. We introduce RAG models where the parametric\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\npare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, and another which can use different passages\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\narchitectures. For language generation tasks, we ﬁnd that RAG models generate\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\nseq2seq baseline.",
"For language generation tasks, we ﬁnd that RAG models generate\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\nseq2seq baseline. 1 Introduction\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\nedge from data [ 47]. They can do so without any access to an external memory, as a parameterized\nimplicit knowledge base [ 51,52]. While this development is exciting, such models do have down-\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\ntheir predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric\nmemory with non-parametric (i.e., retrieval-based) memories [ 20,26,48] can address some of these\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\ncombine masked language models [ 8] with a differentiable retriever, have shown promising results,arXiv:2005.11401v4  [cs.CL]  12 Apr 2021",
"The\tDivine\nComedy\t(x) qQuery\nEncoder\nq(x)\nMIPS p θGenerator  pθ\n(Parametric)\nMargin-\nalize\nThis\t14th\tcentury\twork\nis\tdivided\tinto\t3\nsections:\t\"Inferno\",\n\"Purgatorio\"\t&\n\"Paradiso\"\t\t\t\t\t\t\t\t\t (y)End-to-End Backprop through q and  p θ\nBarack\tObama\twas\nborn\tin\tHawaii. (x)\nFact V eriﬁcation: Fact Querysupports \t(y)\nQuestion GenerationFact V eriﬁcation:\nLabel GenerationDocument\nIndexDefine\t\"middle\tear\" (x)\nQuestion Answering:\nQuestion QueryThe\tmiddle\tear\tincludes\nthe\ttympanic\tcavity\tand\nthe\tthree\tossicles. (y)\nQuestion Answering:\nAnswer GenerationRetriever pη\n(Non-Parametric)\nz 4\nz3\nz2\nz 1d(z)\nJeopardy Question\nGeneration:\nAnswer QueryFigure 1: Overview of our approach. We combine a pre-trained retriever ( Query Encoder +Document\nIndex ) with a pre-trained seq2seq model ( Generator ) and ﬁne-tune end-to-end. For query x, we use\nMaximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we\ntreatzas a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric\nand non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through\na general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\nretriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on\nthe input, and the seq2seq model (BART [ 32]) then conditions on these latent documents together with\nthe input to generate the output.",
"1). The\nretriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on\nthe input, and the seq2seq model (BART [ 32]) then conditions on these latent documents together with\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\nbasis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG\ncan be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric\nmemory which are trained from scratch for speciﬁc tasks, e.g. memory networks [ 64,55], stack-\naugmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\npresent without additional training. Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera-\ntion for knowledge-intensive tasks —tasks that humans could not reasonably be expected to perform\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\non open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform\nrecent approaches that use specialised pre-training objectives on TriviaQA [ 24]. Despite these being\nextractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question\ngeneration, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and\ndiverse than a BART baseline. For FEVER [ 56] fact veriﬁcation, we achieve results within 4.3% of\nstate-of-the-art pipeline models which use strong retrieval supervision.",
"For FEVER [ 56] fact veriﬁcation, we achieve results within 4.3% of\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\nthe non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\n2 Methods\nWe explore RAG models, which use the input sequence xto retrieve text documents zand use them\nas additional context when generating the target sequence y. As shown in Figure 1, our models\nleverage two components: (i) a retriever pη(z|x)with parameters ηthat returns (top-K truncated)\ndistributions over text passages given a query xand (ii) a generator pθ(yi|x,z,y 1:i−1)parametrized\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\ners Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/\nexamples/rag/ . An interactive demo of RAG models can be found at https://huggingface.co/rag/\n2",
"byθthat generates a current token based on a context of the previous i−1tokensy1:i−1, the original\ninputxand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a\ndistribution over generated text. In one approach, RAG-Sequence , the model uses the same document\nto predict each target token. The second approach, RAG-Token , can predict each target token based\non a different document. In the following, we formally introduce both models and then describe the\npηandpθcomponents, as well as the training and decoding procedure. 2.1 Models\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\nthe complete sequence . Technically, it treats the retrieved document as a single latent variable that\nis marginalized to get the seq2seq probability p(y|x)via a top-K approximation. Concretely, the\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\nprobability for each document, which are then marginalized,\npRAG-Sequence (y|x)≈∑\nz∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑\nz∈top-k(p(·|x))pη(z|x)N∏\nipθ(yi|x,z,y 1:i−1)\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\ntarget token and marginalize accordingly. This allows the generator to choose content from several\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\nretriever, and then the generator produces a distribution for the next output token for each document,\nbefore marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\npRAG-Token (y|x)≈N∏\ni∑\nz∈top-k(p(·|x))pη(z|x)pθ(yi|x,z,y 1:i−1)\nFinally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR\nThe retrieval component pη(z|x)is based on DPR [26].",
"2.2 Retriever: DPR\nThe retrieval component pη(z|x)is based on DPR [26]. DPR follows a bi-encoder architecture:\npη(z|x)∝exp(\nd(z)⊤q(x))\nd(z) =BERTd(z),q(x) =BERTq(x)\nwhere d(z)is a dense representation of a document produced by a BERT BASE document encoder [8],\nandq(x)a query representation produced by a query encoder , also based on BERT BASE. Calculating\ntop-k (pη(·|x)), the list ofkdocumentszwith highest prior probability pη(z|x), is a Maximum Inner\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\nretriever was trained to retrieve documents which contain answers to TriviaQA [ 24] questions and\nNatural Questions [29]. We refer to the document index as the non-parametric memory . 2.3 Generator: BART\nThe generator component pθ(yi|x,z,y 1:i−1)could be modelled using any encoder-decoder. We use\nBART-large [ 32], a pre-trained seq2seq transformer [ 58] with 400M parameters. To combine the input\nxwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\nmodels [32]. We refer to the BART generator parameters θas the parametric memory henceforth. 2.4 Training\nWe jointly train the retriever and generator components without any direct supervision on what\ndocument should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj,yj), we\n3",
"minimize the negative marginal log-likelihood of each target,∑\nj−logp(yj|xj)using stochastic\ngradient descent with Adam [ 28]. Updating the document encoder BERTdduring training is costly as\nit requires the document index to be periodically updated as REALM does during pre-training [ 20]. We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and\nindex) ﬁxed, only ﬁne-tuning the query encoder BERT qand the BART generator. 2.5 Decoding\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxyp(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\ntor with transition probability: p′\nθ(yi|x,y 1:i−1) =∑\nz∈top-k(p(·|x))pη(zi|x)pθ(yi|x,zi,y1:i−1)To\ndecode, we can plug p′\nθ(yi|x,y 1:i−1)into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x)does not break into a conventional per-\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\neach document z, scoring each hypothesis using pθ(yi|x,z,y 1:i−1). This yields a set of hypotheses\nY, some of which may not have appeared in the beams of all documents. To estimate the probability\nof an hypothesis ywe run an additional forward pass for each document zfor whichydoes not\nappear in the beam, multiply generator probability with pη(z|x)and then sum the probabilities across\nbeams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer\noutput sequences,|Y|can become large, requiring many forward passes. For more efﬁcient decoding,\nwe can make a further approximation that pθ(y|x,zi)≈0whereywas not generated during beam\nsearch from x,zi. This avoids the need to run additional forward passes once the candidate set Yhas\nbeen generated. We refer to this decoding procedure as “Fast Decoding.”\n3 Experiments\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\nKarpukhin et al.",
"For all experiments, we use\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\nembedding for each document, and build a single MIPS index using FAISS [ 23] with a Hierarchical\nNavigable Small World approximation for fast retrieval [ 37]. During training, we retrieve the top\nkdocuments for each query. We consider k∈{5,10}for training and set kfor test time using dev\ndata. We now discuss experimental details for each task. 3.1 Open-domain Question Answering\nOpen-domain question answering (QA) is an important real-world application and common testbed\nfor knowledge-intensive tasks [ 20]. We treat questions and answers as input-output text pairs (x,y)\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\nthe popular extractive QA paradigm [ 5,7,31,26], where answers are extracted spans from retrieved\ndocuments, relying primarily on non-parametric knowledge. We also compare to “Closed-Book\nQA” approaches [ 52], which, like RAG, generate answers, but which do not exploit retrieval, instead\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\nQuestions (NQ) [ 29], TriviaQA (TQA) [ 24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As\nCT and WQ are small, we follow DPR [ 26] by initializing CT and WQ models with our NQ RAG\nmodel. We use the same train/dev/test splits as prior work [ 31,26] and report Exact Match (EM)\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\ntext generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,\nwe use the MSMARCO NLG task v2.1 [ 43].",
"To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\nretrieved from a search engine for each question, and a full sentence answer annotated from the\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\n4",
"MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\nanswered in a way that matches the reference answer without access to the gold passages, such as\n“What is the weather in V olcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\nRAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst\ncountry to host this international sports competition twice.” As Jeopardy questions are precise,\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\nchallenging knowledge-intensive generation task. We use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As\nthis is a new task, we train a BART model for comparison. Following [ 67], we evaluate using the\nSQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for\nmatching entities and has higher correlation with human judgment for question generation than\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\none for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external\nsources, and speciﬁcity as high mutual dependence between the input and output [ 33]. We follow\nbest practice and use pairwise comparative evaluation [ 34]. Evaluators are shown an answer and two\ngenerated questions, one from BART and one from RAG.",
"We follow\nbest practice and use pairwise comparative evaluation [ 34]. Evaluators are shown an answer and two\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\noptions—quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriﬁcation\nFEVER [ 56] requires classifying whether a natural language claim is supported or refuted by\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\nwhether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\nexploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\nretrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\ntwo variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy. 4 Results\n4.1 Open-domain Question Answering\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\nthe generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\nwithout expensive, specialized “salient span masking” pre-training [ 20].",
"Unlike REALM and T5+SSM, RAG enjoys strong results\nwithout expensive, specialized “salient span masking” pre-training [ 20]. It is worth noting that RAG’s\nretriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-\nencoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\nre-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu-\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\na correct answer being generated, which is not possible with standard extractive approaches, leading\n5",
"Table 1: Open-Domain QA Test Scores. For TQA,\nleft column uses the standard test set for Open-\nDomain QA, right column uses the TQA-Wiki\ntest set. See Appendix D for further details. Model NQ TQA WQ CT\nClosed\nBookT5-11B [52] 34.5 - /50.1 37.4 -\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\nOpen\nBookREALM [20] 40.4 - / - 40.7 46.8\nDPR [26] 41.5 57.9/ - 41.1 50.6\nRAG-Token 44.1 55.2/66.1 45.5 50.0\nRAG-Seq. 44.5 56.8/ 68.0 45.2 52.2Table 2: Generation and classiﬁcation Test Scores. MS-MARCO SotA is [ 4], FEVER-3 is [ 68] and\nFEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2\nB-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8*49.9*76.8 92.2 *\nBART 15.1 19.7 38.2 41.6 64.0 81.1\nRAG-Tok. 17.3 22.2 40.1 41.572.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\ncases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\nimpressive given that (i) those models access gold passages with speciﬁc information required to\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\nBART generations (see §4.5). 4.3 Jeopardy Question Generation\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\npairs of generations from BART and RAG-Token.",
"4 shows human evaluation results, over 452\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\nBART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\nthe task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\nspeciﬁc by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform\nbest because it can generate responses that combine content from several documents. Figure 2 shows\nan example. When generating “Sun”, the posterior is high for document 2 which mentions “The\nSun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is\ngenerated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens. This observation suggests that the generator can complete the titles without depending on speciﬁc\ndocuments. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\nﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\nSun. BART completes the generation \"The SunAlso Rises\" isanovel bythis author of\"The Sun\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\nBART will complete the partial decoding \"The SunAlso Rises\" isanovel bythis author of\"A\nwith \"The SunAlso Rises\" isanovel bythis author of\"AFarewell toArms\" . This example shows\nhow parametric and non-parametric memories work together —the non-parametric component helps\nto guide the generation, drawing out speciﬁc knowledge stored in the parametric memory. 4.4 Fact Veriﬁcation\nTable 2 shows our results on FEVER.",
"This example shows\nhow parametric and non-parametric memories work together —the non-parametric component helps\nto guide the generation, drawing out speciﬁc knowledge stored in the parametric memory. 4.4 Fact Veriﬁcation\nTable 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of\nstate-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6",
"Document 1 : his works are considered classics of American\nliterature ... His wartime experiences formed the basis for his novel\n”A Farewell to Arms” (1929) ... Document 2 : ... artists of the 1920s ”Lost Generation” expatriate\ncommunity. His debut novel, ”The Sun Also Rises” , was published\nin 1926. BOS”\nTheSunAlsoRises”isa\nnovelbythis\nauthorof”A\nFarewellto\nArms”Doc 1\nDoc 2\nDoc 3\nDoc 4\nDoc 5Figure 2: RAG-Token document posterior p(zi|x,yi,y−i)for each generated token for input “Hem-\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\nwhen generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\". Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\nresponses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation\nMS-\nMARCOdeﬁne middle\nearBART?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. what currency\nneeded in\nscotlandBART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed in Scotland is the pound sterling. Jeopardy\nQuestion\nGener\n-ationWashingtonBART?This state has the largest number of counties in the U.S. RAG-T It’s the only U.S. state named for a U.S. president\nRAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park\nThe Divine\nComedyBART*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\nRAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\nFor 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]\nto classify the claim as true or false given the gold evidence sentence.",
"RAG achieves an accuracy\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold\nevidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved\nby RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases. 4.5 Additional Results\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n[33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\ntotal ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\nmore diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\nany diversity-promoting decoding. Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\nduring training. As shown in Table 6, learned retrieval improves results for all tasks. We compare RAG’s dense retriever to a word overlap-based BM25 retriever [ 53]. Here, we replace\nRAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial. Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\nupdate their behavior as the world changes.",
"Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [ 5]\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\n7",
"Table 4: Human assessments for the Jeopardy\nQuestion Generation Task. Factuality Speciﬁcity\nBART better 7.1% 16.8%\nRAG better 42.7% 37.4%\nBoth good 11.7% 11.8%\nBoth poor 17.7% 6.9%\nNo majority 20.8% 20.1%Table 5: Ratio of distinct to total tri-grams for\ngeneration tasks. MSMARCO Jeopardy QGen\nGold 89.6% 90.0%\nBART 70.7% 32.4%\nRAG-Token 77.8% 46.8%\nRAG-Seq. 83.5% 53.8%\nTable 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent. Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.475.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.472.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.474.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5\nbetween these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory. Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\ndocuments, and we do not observe signiﬁcant differences in performance between them. We have the\nﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.",
"Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence. 10 20 30 40 50\nKR e t r i e v e dD o c s394041424344NQ Exact MatchRAG-Tok\nRAG-Seq\n10 20 30 40 50\nKR e t r i e v e dD o c s4050607080NQ Answer Recall @ KRAG-Tok\nRAG-Seq\nFixed DPR\nBM25\n10 20 30 40 50\nKR e t r i e v e dD o c s4850525456Bleu-1 / Rouge-L scoreRAG-Tok R-L\nRAG-Tok B-1\nRAG-Seq R-L\nRAG-Seq B-1\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved. 5 Related Work\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29],\nfact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article\ngeneration [ 36], dialogue [ 41,65,9,13], translation [ 17], and language modeling [ 19,27]. Our\nwork uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single\nretrieval-based architecture is capable of achieving strong performance across several tasks. 8",
"General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\nhas been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-\nmarks [ 60,61] after ﬁne-tuning [ 49,8]. GPT-2 [ 50] later showed that a single, left-to-right, pre-trained\nlanguage model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained encoder-decoder\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\nand generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed\narchitecture, by learning a retrieval module to augment pre-trained, generative language models. Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information\nretrieval, more recently with pre-trained, neural language models [ 44,26] similar to ours. Some\nwork optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,\nusing search [ 46], reinforcement learning [ 6,63,62], or a latent variable approach [ 31,20] as in our\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\ncan be ﬁne-tuned for strong performance on a variety of tasks. Memory-based Architectures Our document index can be seen as a large external memory for\nneural networks to attend to, analogous to memory networks [ 64,55]. Concurrent work [ 14] learns\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\nwork. Other work improves the ability of dialog models to generate factual text by attending over\nfact embeddings [ 15,13].",
"Concurrent work [ 14] learns\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\nwork. Other work improves the ability of dialog models to generate factual text by attending over\nfact embeddings [ 15,13]. A key feature of our memory is that it is comprised of raw text rather\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\nmemory by editing the document index. This approach has also been used in knowledge-intensive\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\nrather than end-to-end learnt retrieval [9]. Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\nto provide a ﬁnal output. These approaches have proved successful in a number of domains including\nMachine Translation [ 18,22] and Semantic Parsing [ 21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\ncould represent promising future work. 6 Discussion\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\nfound that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual\nand speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\nwithout requiring any retraining.",
"We conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\nanother objective. Our work opens up new research directions on how parametric and non-parametric\nmemories interact and how to most effectively combine them, showing promise in being applied to a\nwide variety of NLP tasks. 9",
"Broader Impact\nThis work offers several positive societal beneﬁts over previous work: the fact that it is more\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\nwith generations that are more factual, and offers more control and interpretability. RAG could be\nemployed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\neffective at their jobs. With these advantages also come potential downsides: Wikipedia, or any potential external knowledge\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\nemployed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\ncontent [ 54]. Advanced language models may also lead to the automation of various jobs in the\ncoming decades [ 16]. In order to mitigate these risks, AI systems could be employed to ﬁght against\nmisleading content and automated spam/phishing. Acknowledgments\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\nprogram. References\n[1]Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. arXiv:1611.09268 [cs] , November 2016. URL http:\n//arxiv.org/abs/1611.09268 . arXiv: 1611.09268.",
"MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. arXiv:1611.09268 [cs] , November 2016. URL http:\n//arxiv.org/abs/1611.09268 . arXiv: 1611.09268. [2]Petr Baudiš and Jan Šediv `y. Modeling of the question answering task in the yodaqa system. In\nInternational Conference of the Cross-Language Evaluation Forum for European Languages ,\npages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\n2F978-3-319-24027-5_20 . [3]Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing , pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/\nD13-1160 . [4]Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\ning&autoregressive language model for context-conditioned generation. ArXiv , abs/2004.07159,\n2020. URL https://arxiv.org/abs/2004.07159 . [5]Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pages 1870–1879, Vancouver, Canada,\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\nhttps://www.aclweb.org/anthology/P17-1171 . [6]Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\nJonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\npages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020 . 10",
"[7]Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\nhension. arXiv:1710.10723 [cs] , October 2017. URL http://arxiv.org/abs/1710.10723 . arXiv: 1710.10723. [8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis,\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423 . [9]Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\nLearning Representations , 2019. URL https://openreview.net/forum?id=r1l73iRqKm . [10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. arXiv:1704.05179 [cs] , April 2017. URL http://arxiv.org/abs/1704.05179 . arXiv:\n1704.05179. [11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers) , pages 889–898, Melbourne, Australia, July 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\nP18-1082 . [12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3558–3567, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\nanthology/P19-1346 . [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\nwith KNN-based composite memory, 2020.",
"Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\nanthology/P19-1346 . [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\nH1gx1CNKPH . [14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. ArXiv , abs/2004.07202,\n2020. URL https://arxiv.org/abs/2004.07202 . [15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\nConference on Artiﬁcial Intelligence , 2018. URL https://www.aaai.org/ocs/index.php/\nAAAI/AAAI18/paper/view/16710 . [16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\nexceed human performance? evidence from AI experts. CoRR , abs/1705.08807, 2017. URL\nhttp://arxiv.org/abs/1705.08807 . [17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\nmachine translation. In AAAI Conference on Artiﬁcial Intelligence , 2018. URL https:\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282 . [18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\nmachine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 , 32nd\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018. 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\nThrough 07-02-2018. [19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\nediting prototypes. Transactions of the Association for Computational Linguistics , 6:437–450,\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031 . 11",
"[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\nRetrieval-augmented language model pre-training. ArXiv , abs/2002.08909, 2020. URL https:\n//arxiv.org/abs/2002.08909 . [21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing Systems 31 , pages 10052–\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs. pdf. [22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 2532–2538, Online, July 2020. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\nanthology/2020.acl-main.228 . [23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734 , 2017. URL https://arxiv.org/abs/1702.08734 . [24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\npages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147 . [25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\naugmented recurrent nets. In Proceedings of the 28th International Conference on\nNeural Information Processing Systems - Volume 1 , NIPS’15, page 190–198, Cam-\nbridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets . [26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih.",
"MIT Press. URL https://papers.nips.cc/paper/\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets . [26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 . [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\ntion through memorization: Nearest neighbor language models. In International Conference on\nLearning Representations , 2020. URL https://openreview.net/forum?id=HklBjCEKvH . [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\nhttp://arxiv.org/abs/1412.6980 . [29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\ntion Answering Research. Transactions of the Association of Computational Lin-\nguistics , 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\nnatural-questions/main-1455-kwiatkowski.pdf . [30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\nformation Processing Systems 32 , pages 8548–8559. Curran Associates, Inc., 2019. URL http:\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf . [31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\n12",
"for Computational Linguistics , pages 6086–6096, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\nanthology/P19-1612 . [32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. arXiv preprint\narXiv:1910.13461 , 2019. URL https://arxiv.org/abs/1910.13461 . [33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies , pages 110–119, San Diego, California, June 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\nN16-1014 . [34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\nwith optimized questions and multi-turn comparisons. ArXiv , abs/1909.03087, 2019. URL\nhttps://arxiv.org/abs/1909.03087 . [35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics , pages 3044–3049, Florence, Italy,\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\nhttps://www.aclweb.org/anthology/P19-1291 . [36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\nConference on Learning Representations , 2018. URL https://openreview.net/forum? id=Hyg0vbWC- . [37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 42:824–836, 2016.",
"id=Hyg0vbWC- . [37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320 . [38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv\npreprint arXiv:2002.06177 , 2020. URL https://arxiv.org/abs/2002.06177 . [39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\nveriﬁability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:\n//arxiv.org/abs/1911.03587 . [40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In ICLR , 2018. URL https://openreview.net/forum?id=r1gs9JgRZ . [41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\ning background knowledge for building conversation systems. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing , pages 2322–2332, Brus-\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255 . [42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing , pages 3950–3959, Brussels, Belgium, October-November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\nanthology/D18-1429 . [43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\nTarek Richard Besold, Antoine Bordes, Artur S.",
"[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\nTarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\n13",
"approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016 , volume 1773 of CEUR Workshop\nProceedings . CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\n2016_paper9.pdf . [44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\narXiv:1901.04085 , 2019. URL https://arxiv.org/abs/1901.04085 . [45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics (Demonstrations) , pages 48–53, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb. org/anthology/N19-4009 . [46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\nCho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\n2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244 . [47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 2463–2473, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250 . [48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models’ factual predictions.",
"doi: 10.18653/v1/\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250 . [48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In\nAutomated Knowledge Base Construction , 2020. URL https://openreview.net/forum? id=025X0zPfn . [49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\nproving Language Understanding by Generative Pre-Training, 2018. URL\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\nlanguage-unsupervised/language_understanding_paper.pdf . [50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask learners, 2019. URL\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\nmodels_are_unsupervised_multitask_learners.pdf . [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. arXiv e-prints , 2019. URL https://arxiv.org/abs/1910.10683 . [52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\nthe parameters of a language model? arXiv e-prints , 2020. URL https://arxiv.org/abs/\n2002.08910 . [53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\nbeyond. Found. Trends Inf. Retr. , 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\n1500000019. URL https://doi.org/10.1561/1500000019 . [54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv , abs/1908.09203, 2019. [55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates, Inc., 2015.",
"End-to-end memory net-\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf . 14",
"[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\nlarge-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) , pages 809–819, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\nhttps://www.aclweb.org/anthology/N18-1074 . [57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\nbiases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv , abs/2004.14366,\n2020. URL https://arxiv.org/abs/2004.14366 . [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 30 , pages 5998–6008. Curran Associates, Inc., 2017. URL\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf . [59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artiﬁcial Intelligence , 2018. URL https://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/17329 . [60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. InProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP , pages 353–355, Brussels, Belgium, November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/\nanthology/W18-5446 . [61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\nPurpose Language Understanding Systems. In H. Wallach, H.",
"[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems 32 , pages 3261–3275. Curran Associates, Inc., 2019. URL https://\narxiv.org/abs/1905.00537 . [62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\nthe Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative\nApplications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\nAdvances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\n2018 , pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/16712 . [63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\nranking in open-domain question answering. In ICLR , 2018. URL https://openreview. net/forum?id=rJl3yM-Ab . [64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\nhttp://arxiv.org/abs/1410.3916 . [65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence\ngeneration models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd\nInternational Workshop on Search-Oriented Conversational AI , pages 87–92, Brussels, Belgium,\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\nhttps://www.aclweb.org/anthology/W18-5713 . 15",
"[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv , abs/1910.03771, 2019. [67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , pages 2495–2509, Hong Kong, China, Novem-\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\nhttps://www.aclweb.org/anthology/D19-1253 . [68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv , abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745 . 16",
"Appendices for Retrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks\nA Implementation Details\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\nwe did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance. B Human Evaluation\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\nand a worked example appear when clicking \"view tool guide\". Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\nwhich model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instruc-\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\ntheir annotations were removed from the results. C Training setup Details\nWe train all RAG models and BART baselines using Fairseq [ 45].2We train with mixed precision\nﬂoating point arithmetic [ 40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\ntraining and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search\nwith FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼100\nGB of CPU memory for all of Wikipedia.",
"We ﬁnd that doing Maximum Inner Product Search\nwith FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼100\nGB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\nTransformers [ 66]3, which achieves equivalent performance to the previous version but is a cleaner\nand easier to use implementation. This version is also open-sourced. We also compress the document\nindex using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to\nrun experiments with RAG can be found at https://github.com/huggingface/transformers/\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\nathttps://huggingface.co/rag/\n2https://github.com/pytorch/fairseq\n3https://github.com/huggingface/transformers\n17",
"D Further Details on Open-Domain QA\nFor open-domain QA, multiple answer annotations are often available for a given question. These\nanswer annotations are exploited by extractive models during training as typically all the answer\nannotations are used to ﬁnd matches within documents when preparing training data. For RAG, we\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\nthe model with each (q,a)pair separately, leading to a small increase in accuracy. For TriviaQA,\nthere are often many valid answers to a given question, some of which are not suitable training targets,\nsuch as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur\nin top 1000 documents for the query. CuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres-\nsions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for\neach query, and use the answer that most frequently matches the regex pattern as the supervision\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace. TriviaQA Evaluation setups The open-domain QA community customarily uses public develop-\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\ncompehension purposes. We report our results using the datasets splits used in DPR [ 26], which are\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\nTriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set\ninstead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\nappendix of [ 14]). We report results on both test sets to enable fair comparison to both approaches.",
"Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set\ninstead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\nappendix of [ 14]). We report results on both test sets to enable fair comparison to both approaches. We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more\nconventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being\nsimpler to answer from Wikipedia. E Further Details on FEVER\nFor FEVER classiﬁcation, we follow the practice from [ 32], and ﬁrst re-generate the claim, and\nthen classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\nﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\nwe explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia\nas evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to\nus, directly tackling this task is not straightforward. We hope to address this in future work. F Null Document Probabilities\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [ 20] in order\nto model cases where no useful information could be retrieved for a given input. Here, if kdocuments\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\ndocument, before marginalizing over k+ 1predictions. We explored modelling this null document\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\n(iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in\nthe interests of simplicity, we omit them.",
"We did not ﬁnd that these improved performance, so in\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\ndocuments for questions that are less likely to beneﬁt from retrieval, suggesting that null document\nmechanisms may not be necessary for RAG. G Parameters\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\n18",
"Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\nTask Train Development Test\nNatural Questions 79169 8758 3611\nTriviaQA 78786 8838 11314\nWebQuestions 3418 362 2033\nCuratedTrec 635 134 635\nJeopardy Question Generation 97392 13714 26849\nMS-MARCO 153726 12468 101093*\nFEVER-3-way 145450 10000 10000\nFEVER-2-way 96966 6666 6666\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [ 52],\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\nparametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consists of 21M\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating\npoint precision to manage memory and disk footprints. H Retrieval Collapse\nIn preliminary experiments, we observed that for some tasks such as story generation [ 11], the\nretrieval component would “collapse” and learn to retrieve the same documents regardless of the\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\nwhen optimizing a retrieval component in order to improve performance on downstream tasks. I Number of instances per dataset\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7. 19"
],
"metadatas": [
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 0,
"heading": "",
"global_index": 0,
"start_char": 0,
"end_char": 167
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 1,
"heading": "ABSTRACT",
"global_index": 1,
"start_char": 169,
"end_char": 1395
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 2,
"heading": "1. INTRODUCTION",
"global_index": 2,
"start_char": 1397,
"end_char": 3401
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 3,
"heading": "1. INTRODUCTION",
"global_index": 3,
"start_char": 3401,
"end_char": 5194
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 4,
"heading": "1. INTRODUCTION",
"global_index": 4,
"start_char": 4985,
"end_char": 6652
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 5,
"heading": "2. PROBLEM STATEMENT",
"global_index": 5,
"start_char": 6654,
"end_char": 8675
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 6,
"heading": "2. PROBLEM STATEMENT",
"global_index": 6,
"start_char": 8675,
"end_char": 10775
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 7,
"heading": "2. PROBLEM STATEMENT",
"global_index": 7,
"start_char": 10775,
"end_char": 11253
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 8,
"heading": "2. PROBLEM STATEMENT",
"global_index": 8,
"start_char": 10753,
"end_char": 11340
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 9,
"heading": "3. GPU: OVERVIEW AND K-SELECTION",
"global_index": 9,
"start_char": 11342,
"end_char": 13399
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 10,
"heading": "3. GPU: OVERVIEW AND K-SELECTION",
"global_index": 10,
"start_char": 13399,
"end_char": 15494
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 11,
"heading": "3. GPU: OVERVIEW AND K-SELECTION",
"global_index": 11,
"start_char": 15494,
"end_char": 17548
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 12,
"heading": "3. GPU: OVERVIEW AND K-SELECTION",
"global_index": 12,
"start_char": 17548,
"end_char": 17978
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 13,
"heading": "3. GPU: OVERVIEW AND K-SELECTION",
"global_index": 13,
"start_char": 17286,
"end_char": 19111
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 14,
"heading": "4. FAST K-SELECTION ON THE GPU",
"global_index": 14,
"start_char": 19113,
"end_char": 20365
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 15,
"heading": "4. FAST K-SELECTION ON THE GPU",
"global_index": 15,
"start_char": 20365,
"end_char": 22198
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 16,
"heading": "4. FAST K-SELECTION ON THE GPU",
"global_index": 16,
"start_char": 22198,
"end_char": 22764
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 17,
"heading": "4. FAST K-SELECTION ON THE GPU",
"global_index": 17,
"start_char": 22343,
"end_char": 24252
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 18,
"heading": "4. FAST K-SELECTION ON THE GPU",
"global_index": 18,
"start_char": 24252,
"end_char": 26106
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 19,
"heading": "4. FAST K-SELECTION ON THE GPU",
"global_index": 19,
"start_char": 26106,
"end_char": 26839
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 20,
"heading": "0. All lanes",
"global_index": 20,
"start_char": 26505,
"end_char": 27480
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 21,
"heading": "0. All lanes",
"global_index": 21,
"start_char": 27482,
"end_char": 29445
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 22,
"heading": "0. All lanes",
"global_index": 22,
"start_char": 29445,
"end_char": 30072
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 23,
"heading": "5. COMPUTATION LAYOUT",
"global_index": 23,
"start_char": 29857,
"end_char": 31927
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 24,
"heading": "5. COMPUTATION LAYOUT",
"global_index": 24,
"start_char": 31927,
"end_char": 32885
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 25,
"heading": "64. The codes are stored as sequential groups of bbytes per",
"global_index": 25,
"start_char": 32722,
"end_char": 32937
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 26,
"heading": "64. The codes are stored as sequential groups of bbytes per",
"global_index": 26,
"start_char": 32939,
"end_char": 34994
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 27,
"heading": "64. The codes are stored as sequential groups of bbytes per",
"global_index": 27,
"start_char": 34994,
"end_char": 36900
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 28,
"heading": "64. The codes are stored as sequential groups of bbytes per",
"global_index": 28,
"start_char": 36900,
"end_char": 38933
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 29,
"heading": "64. The codes are stored as sequential groups of bbytes per",
"global_index": 29,
"start_char": 38429,
"end_char": 38743
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 30,
"heading": "6. EXPERIMENTS & APPLICATIONS",
"global_index": 30,
"start_char": 38745,
"end_char": 40529
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 31,
"heading": "6. EXPERIMENTS & APPLICATIONS",
"global_index": 31,
"start_char": 40529,
"end_char": 42596
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 32,
"heading": "6. EXPERIMENTS & APPLICATIONS",
"global_index": 32,
"start_char": 42596,
"end_char": 44086
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 33,
"heading": "6. EXPERIMENTS & APPLICATIONS",
"global_index": 33,
"start_char": 43719,
"end_char": 45236
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 34,
"heading": "104. We compare the search performance in terms of same",
"global_index": 34,
"start_char": 45238,
"end_char": 47224
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 35,
"heading": "104. We compare the search performance in terms of same",
"global_index": 35,
"start_char": 47224,
"end_char": 48329
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 36,
"heading": "104. We compare the search performance in terms of same",
"global_index": 36,
"start_char": 48074,
"end_char": 50149
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 37,
"heading": "104. We compare the search performance in terms of same",
"global_index": 37,
"start_char": 50149,
"end_char": 51520
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 38,
"heading": "8. REFERENCES",
"global_index": 38,
"start_char": 51466,
"end_char": 52849
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 39,
"heading": "8. REFERENCES",
"global_index": 39,
"start_char": 52851,
"end_char": 55132
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 40,
"heading": "8. REFERENCES",
"global_index": 40,
"start_char": 55132,
"end_char": 57389
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 41,
"heading": "8. REFERENCES",
"global_index": 41,
"start_char": 57389,
"end_char": 59603
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 42,
"heading": "8. REFERENCES",
"global_index": 42,
"start_char": 59603,
"end_char": 60699
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 43,
"heading": "8. REFERENCES",
"global_index": 43,
"start_char": 59907,
"end_char": 62013
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 44,
"heading": "8. REFERENCES",
"global_index": 44,
"start_char": 62013,
"end_char": 64020
},
{
"doc_id": "Billion-scale_similarity_search_with_GPUs",
"chunk_index": 45,
"heading": "8. REFERENCES",
"global_index": 45,
"start_char": 64020,
"end_char": 65421
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 0,
"heading": "",
"global_index": 46,
"start_char": 0,
"end_char": 1993
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 1,
"heading": "",
"global_index": 47,
"start_char": 1993,
"end_char": 4011
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 2,
"heading": "",
"global_index": 48,
"start_char": 4011,
"end_char": 4301
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 3,
"heading": "",
"global_index": 49,
"start_char": 4085,
"end_char": 6143
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 4,
"heading": "",
"global_index": 50,
"start_char": 6143,
"end_char": 8059
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 5,
"heading": "",
"global_index": 51,
"start_char": 8059,
"end_char": 9232
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 6,
"heading": "",
"global_index": 52,
"start_char": 8902,
"end_char": 10939
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 7,
"heading": "",
"global_index": 53,
"start_char": 10939,
"end_char": 12887
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 8,
"heading": "",
"global_index": 54,
"start_char": 12887,
"end_char": 13828
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 9,
"heading": "",
"global_index": 55,
"start_char": 13435,
"end_char": 15493
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 10,
"heading": "",
"global_index": 56,
"start_char": 15493,
"end_char": 17569
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 11,
"heading": "",
"global_index": 57,
"start_char": 17569,
"end_char": 18108
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 12,
"heading": "",
"global_index": 58,
"start_char": 17861,
"end_char": 19810
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 13,
"heading": "",
"global_index": 59,
"start_char": 19810,
"end_char": 20777
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 14,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 60,
"start_char": 20532,
"end_char": 21674
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 15,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 61,
"start_char": 21676,
"end_char": 23703
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 16,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 62,
"start_char": 23703,
"end_char": 25759
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 17,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 63,
"start_char": 25759,
"end_char": 26584
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 18,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 64,
"start_char": 26246,
"end_char": 28283
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 19,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 65,
"start_char": 28283,
"end_char": 30121
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 20,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 66,
"start_char": 30121,
"end_char": 31166
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 21,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 67,
"start_char": 30737,
"end_char": 32573
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 22,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 68,
"start_char": 32573,
"end_char": 34563
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 23,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 69,
"start_char": 34563,
"end_char": 35075
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 24,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 70,
"start_char": 34723,
"end_char": 36758
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 25,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 71,
"start_char": 36758,
"end_char": 38812
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 26,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 72,
"start_char": 38812,
"end_char": 39792
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 27,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 73,
"start_char": 39373,
"end_char": 41418
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 28,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 74,
"start_char": 41418,
"end_char": 43483
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 29,
"heading": "# Train: all (59k)Figure 1: Retriever top- kaccuracy with different num-",
"global_index": 75,
"start_char": 43483,
"end_char": 44027
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 30,
"heading": "2019. Latent retrieval for weakly supervised open",
"global_index": 76,
"start_char": 43505,
"end_char": 44264
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 31,
"heading": "2018. Denoising distantly supervised open-domain",
"global_index": 77,
"start_char": 44320,
"end_char": 46469
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 32,
"heading": "2018. Denoising distantly supervised open-domain",
"global_index": 78,
"start_char": 46469,
"end_char": 48540
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 33,
"heading": "2018. Denoising distantly supervised open-domain",
"global_index": 79,
"start_char": 48540,
"end_char": 49277
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 34,
"heading": "2018. Denoising distantly supervised open-domain",
"global_index": 80,
"start_char": 48740,
"end_char": 50806
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 35,
"heading": "2018. Denoising distantly supervised open-domain",
"global_index": 81,
"start_char": 50806,
"end_char": 52713
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 36,
"heading": "2018. Denoising distantly supervised open-domain",
"global_index": 82,
"start_char": 52713,
"end_char": 53246
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 37,
"heading": "2018. Denoising distantly supervised open-domain",
"global_index": 83,
"start_char": 52938,
"end_char": 54963
},
{
"doc_id": "Dense_Passage_Retrieval_for_Open-Domain_Question_Answering",
"chunk_index": 38,
"heading": "2018. Denoising distantly supervised open-domain",
"global_index": 84,
"start_char": 54963,
"end_char": 55879
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 0,
"heading": "",
"global_index": 85,
"start_char": 0,
"end_char": 194
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 1,
"heading": "ABSTRACT",
"global_index": 86,
"start_char": 196,
"end_char": 2051
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 2,
"heading": "ABSTRACT",
"global_index": 87,
"start_char": 2051,
"end_char": 3460
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 3,
"heading": "ABSTRACT",
"global_index": 88,
"start_char": 3311,
"end_char": 5309
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 4,
"heading": "ABSTRACT",
"global_index": 89,
"start_char": 5309,
"end_char": 7013
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 5,
"heading": "ABSTRACT",
"global_index": 90,
"start_char": 6902,
"end_char": 8970
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 6,
"heading": "ABSTRACT",
"global_index": 91,
"start_char": 8970,
"end_char": 9289
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 7,
"heading": "ABSTRACT",
"global_index": 92,
"start_char": 9100,
"end_char": 9347
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 8,
"heading": "REFERENCES",
"global_index": 93,
"start_char": 9349,
"end_char": 11505
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 9,
"heading": "REFERENCES",
"global_index": 94,
"start_char": 11505,
"end_char": 12437
},
{
"doc_id": "PASSAGE_RE-RANKING_WITH_BERT",
"chunk_index": 10,
"heading": "REFERENCES",
"global_index": 95,
"start_char": 12185,
"end_char": 13441
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 0,
"heading": "",
"global_index": 96,
"start_char": 0,
"end_char": 2047
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 1,
"heading": "",
"global_index": 97,
"start_char": 2047,
"end_char": 4075
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 2,
"heading": "",
"global_index": 98,
"start_char": 4075,
"end_char": 4659
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 3,
"heading": "",
"global_index": 99,
"start_char": 4269,
"end_char": 6295
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 4,
"heading": "",
"global_index": 100,
"start_char": 6295,
"end_char": 8155
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 5,
"heading": "",
"global_index": 101,
"start_char": 8155,
"end_char": 9118
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 6,
"heading": "",
"global_index": 102,
"start_char": 8797,
"end_char": 10842
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 7,
"heading": "",
"global_index": 103,
"start_char": 10842,
"end_char": 12825
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 8,
"heading": "",
"global_index": 104,
"start_char": 12825,
"end_char": 13201
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 9,
"heading": "",
"global_index": 105,
"start_char": 12753,
"end_char": 14798
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 10,
"heading": "",
"global_index": 106,
"start_char": 14798,
"end_char": 16630
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 11,
"heading": "",
"global_index": 107,
"start_char": 16630,
"end_char": 17200
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 12,
"heading": "",
"global_index": 108,
"start_char": 16820,
"end_char": 18854
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 13,
"heading": "",
"global_index": 109,
"start_char": 18854,
"end_char": 19846
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 14,
"heading": "",
"global_index": 110,
"start_char": 19599,
"end_char": 21451
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 15,
"heading": "",
"global_index": 111,
"start_char": 21451,
"end_char": 23538
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 16,
"heading": "",
"global_index": 112,
"start_char": 23538,
"end_char": 23901
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 17,
"heading": "",
"global_index": 113,
"start_char": 23468,
"end_char": 25553
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 18,
"heading": "",
"global_index": 114,
"start_char": 25553,
"end_char": 27505
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 19,
"heading": "",
"global_index": 115,
"start_char": 27505,
"end_char": 28787
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 20,
"heading": "",
"global_index": 116,
"start_char": 28660,
"end_char": 30566
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 21,
"heading": "",
"global_index": 117,
"start_char": 30566,
"end_char": 32464
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 22,
"heading": "2019. Latent retrieval for weakly supervised open",
"global_index": 118,
"start_char": 32203,
"end_char": 33693
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 23,
"heading": "2021. Rethinking search: making domain experts",
"global_index": 119,
"start_char": 33748,
"end_char": 34914
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 24,
"heading": "2021. Rethinking search: making domain experts",
"global_index": 120,
"start_char": 34914,
"end_char": 36981
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 25,
"heading": "2021. Rethinking search: making domain experts",
"global_index": 121,
"start_char": 36981,
"end_char": 39024
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 26,
"heading": "2021. Rethinking search: making domain experts",
"global_index": 122,
"start_char": 39024,
"end_char": 39608
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 27,
"heading": "2021. Rethinking search: making domain experts",
"global_index": 123,
"start_char": 39221,
"end_char": 40201
},
{
"doc_id": "Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels",
"chunk_index": 28,
"heading": "2021. Rethinking search: making domain experts",
"global_index": 124,
"start_char": 40203,
"end_char": 41059
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 0,
"heading": "",
"global_index": 125,
"start_char": 0,
"end_char": 1970
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 1,
"heading": "",
"global_index": 126,
"start_char": 1970,
"end_char": 3062
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 2,
"heading": "",
"global_index": 127,
"start_char": 2901,
"end_char": 4829
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 3,
"heading": "",
"global_index": 128,
"start_char": 4829,
"end_char": 6900
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 4,
"heading": "",
"global_index": 129,
"start_char": 6900,
"end_char": 7845
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 5,
"heading": "",
"global_index": 130,
"start_char": 7467,
"end_char": 9533
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 6,
"heading": "",
"global_index": 131,
"start_char": 9533,
"end_char": 11180
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 7,
"heading": "",
"global_index": 132,
"start_char": 11111,
"end_char": 13192
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 8,
"heading": "",
"global_index": 133,
"start_char": 13192,
"end_char": 15202
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 9,
"heading": "",
"global_index": 134,
"start_char": 15202,
"end_char": 15569
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 10,
"heading": "",
"global_index": 135,
"start_char": 15311,
"end_char": 17338
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 11,
"heading": "",
"global_index": 136,
"start_char": 17338,
"end_char": 19329
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 12,
"heading": "",
"global_index": 137,
"start_char": 19329,
"end_char": 20156
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 13,
"heading": "",
"global_index": 138,
"start_char": 19872,
"end_char": 21912
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 14,
"heading": "",
"global_index": 139,
"start_char": 21912,
"end_char": 23795
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 15,
"heading": "",
"global_index": 140,
"start_char": 23795,
"end_char": 24309
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 16,
"heading": "",
"global_index": 141,
"start_char": 23967,
"end_char": 25978
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 17,
"heading": "",
"global_index": 142,
"start_char": 25978,
"end_char": 28032
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 18,
"heading": "",
"global_index": 143,
"start_char": 28032,
"end_char": 28514
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 19,
"heading": "",
"global_index": 144,
"start_char": 28283,
"end_char": 30355
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 20,
"heading": "",
"global_index": 145,
"start_char": 30355,
"end_char": 31612
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 21,
"heading": "",
"global_index": 146,
"start_char": 31445,
"end_char": 33395
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 22,
"heading": "",
"global_index": 147,
"start_char": 33395,
"end_char": 35432
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 23,
"heading": "",
"global_index": 148,
"start_char": 35432,
"end_char": 36048
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 24,
"heading": "",
"global_index": 149,
"start_char": 35573,
"end_char": 37643
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 25,
"heading": "",
"global_index": 150,
"start_char": 37643,
"end_char": 39523
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 26,
"heading": "",
"global_index": 151,
"start_char": 39362,
"end_char": 41479
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 27,
"heading": "",
"global_index": 152,
"start_char": 41479,
"end_char": 43395
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 28,
"heading": "",
"global_index": 153,
"start_char": 43150,
"end_char": 45294
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 29,
"heading": "",
"global_index": 154,
"start_char": 45294,
"end_char": 47378
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 30,
"heading": "",
"global_index": 155,
"start_char": 47158,
"end_char": 49270
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 31,
"heading": "",
"global_index": 156,
"start_char": 49270,
"end_char": 51303
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 32,
"heading": "",
"global_index": 157,
"start_char": 51303,
"end_char": 51651
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 33,
"heading": "",
"global_index": 158,
"start_char": 51193,
"end_char": 53293
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 34,
"heading": "",
"global_index": 159,
"start_char": 53293,
"end_char": 55386
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 35,
"heading": "",
"global_index": 160,
"start_char": 55386,
"end_char": 55670
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 36,
"heading": "",
"global_index": 161,
"start_char": 55202,
"end_char": 57350
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 37,
"heading": "",
"global_index": 162,
"start_char": 57350,
"end_char": 59455
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 38,
"heading": "",
"global_index": 163,
"start_char": 59229,
"end_char": 60371
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 39,
"heading": "",
"global_index": 164,
"start_char": 60373,
"end_char": 62296
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 40,
"heading": "",
"global_index": 165,
"start_char": 62296,
"end_char": 63116
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 41,
"heading": "",
"global_index": 166,
"start_char": 62936,
"end_char": 64985
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 42,
"heading": "",
"global_index": 167,
"start_char": 64985,
"end_char": 66857
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 43,
"heading": "",
"global_index": 168,
"start_char": 66857,
"end_char": 67545
},
{
"doc_id": "Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks",
"chunk_index": 44,
"heading": "",
"global_index": 169,
"start_char": 67185,
"end_char": 69149
}
]
}